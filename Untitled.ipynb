{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy, sys\n",
    "import torch\n",
    "import argparse\n",
    "from linformer_pytorch import LinformerEncDec\n",
    "\n",
    "from scripts.MoveData import *\n",
    "# from scripts.Transformer import *\n",
    "from scripts.Linformer import *\n",
    "from scripts.TalkTrain import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "singularity exec --nv --overlay /scratch/kp2684/chatbot_transformer.ext3:ro /scratch/work/public/singularity/cuda11.2.2-cudnn8-devel-ubuntu20.04.sif /bin/bash -c \"source /ext3/env.sh; python main.py --epoch 200 --batch 32 --train data2_train_9010 --test data2_test_9010 --weight rtx8000_data2_9010_00004_transformer_false_warmup --modeler transformer --lr 0.0004 --scheduler warmup --shuffle False --verbose False\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = \"data2_train_9010\"\n",
    "test = \"data2_test_9010\"\n",
    "# train = \"smalldata_train_9010\"\n",
    "# test = \"smalldata_test_9010\"\n",
    "# train = \"smalldata2_train_9010\"\n",
    "# test = \"smalldata2_test_9010\"\n",
    "weight = \"testrun2\"\n",
    "batch = 128\n",
    "epoch = 200\n",
    "shuffle = False\n",
    "verbose = True\n",
    "modeler = \"linformer\"\n",
    "lr = 0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Options(batchsize=batch, device=torch.device(device), epochs=epoch, lr=lr, max_len = 128, save_path = f'saved/weights/{weight}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Load Dataset..\n",
      "train input vocab size 31507 train reply vocab size 30876\n"
     ]
    }
   ],
   "source": [
    "print('==> Load Dataset..')\n",
    "train_data_iter, train_infield, train_outfield, train_opt = json2datatools(path = f'saved/data/{train}.json', opt=opt, train=True, shuffle=True)\n",
    "print('train input vocab size', len(train_infield.vocab), 'train reply vocab size', len(train_outfield.vocab))\n",
    "test_data_iter, test_infield, test_outfield, test_opt = json2datatools(path = f'saved/data/{test}.json', opt=opt, train=False, shuffle=shuffle)\n",
    "print('test input vocab size', len(test_infield.vocab), 'test reply vocab size', len(test_outfield.vocab))\n",
    "print(\"==> Number of train steps per epoch\",num_batches(train_data_iter))\n",
    "print(\"==> Number of test steps per epoch\",num_batches(train_data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def t_trainer(model, train_data_iterator, train_options, test_data_iterator, test_options, optimizer, scheduler, scheduler_name):\n",
    "\n",
    "    if torch.cuda.is_available() and train_options.device == torch.device(\"cuda\"):\n",
    "        print(\"==> a GPU was detected, model will be trained on GPU\")\n",
    "        model = model.cuda()\n",
    "    else:\n",
    "        print(\"==> training on cpu\")\n",
    "\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    best_loss = 100\n",
    "    \n",
    "    train_tracker, test_tracker = [], []\n",
    "    iters = len(train_data_iterator)\n",
    "    for epoch in range(train_options.epochs):\n",
    "        train_total_loss = 0\n",
    "        for i, batch in enumerate(train_data_iterator): \n",
    "            src = batch.listen.transpose(0,1)\n",
    "            trg = batch.reply.transpose(0,1)\n",
    "            # print(\"src\", src.shape)\n",
    "            # print(\"trg\", trg.shape)\n",
    "            trg_input = trg[:, :-1]\n",
    "            src_mask, trg_mask = create_masks(src, trg_input, train_options)\n",
    "            preds = model(src, src_mask, trg_input, trg_mask)\n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            optimizer.zero_grad()\n",
    "            train_batch_loss = F.cross_entropy(preds.view(-1, preds.size(-1)), \n",
    "                                         ys, ignore_index = train_options.trg_pad)\n",
    "            train_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_total_loss += train_batch_loss.item()\n",
    "\n",
    "            if scheduler_name == \"cosine\": \n",
    "                scheduler.step(epoch + i / iters)\n",
    "            if scheduler_name == \"warmup\": \n",
    "                scheduler.step()\n",
    "            # print(\"batch loss\", train_batch_loss)\n",
    "        if scheduler_name == \"warmup\":\n",
    "            scheduler.print_lr(epoch+i)\n",
    "        train_epoch_loss = train_total_loss/(num_batches(train_data_iterator)+1)\n",
    "        train_tracker.append(train_epoch_loss)\n",
    "        if scheduler_name == \"plateau\": \n",
    "            scheduler.step(train_epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(test_data_iterator): \n",
    "                src = batch.listen.transpose(0,1)\n",
    "                trg = batch.reply.transpose(0,1)\n",
    "                trg_input = trg[:, :-1]\n",
    "                src_mask, trg_mask = create_masks(src, trg_input, test_options)\n",
    "                preds = model(src, src_mask, trg_input, trg_mask)\n",
    "                ys = trg[:, 1:].contiguous().view(-1)\n",
    "                test_batch_loss = F.cross_entropy(preds.view(-1, preds.size(-1)), \n",
    "                                             ys, ignore_index = test_options.trg_pad)\n",
    "                test_total_loss += test_batch_loss.item()\n",
    "\n",
    "            test_epoch_loss = test_total_loss/(num_batches(test_data_iterator)+1)\n",
    "            test_tracker.append(test_epoch_loss)\n",
    "\n",
    "        # if scheduler_name == \"plateau\": \n",
    "        #     scheduler.step(test_epoch_loss)\n",
    "                    \n",
    "        model.train()\n",
    "        \n",
    "\n",
    "        if train_epoch_loss < best_loss:\n",
    "            best_loss = train_epoch_loss\n",
    "            torch.save(model.state_dict(), train_options.save_path)\n",
    "        # if test_epoch_loss < best_loss:\n",
    "        #     best_loss = test_epoch_loss\n",
    "        #     torch.save(model.state_dict(), train_options.save_path)\n",
    "        print(\"%.3fm: train epoch *%d*, loss = *%.3f*\" %((time.time() - start)//60, epoch+1, train_epoch_loss), end=\", \")\n",
    "        print(\"%.3fm: test epoch *%d*, loss = *%.3f*, best loss = *%.3f*\" %((time.time() - start)//60, epoch+1, test_epoch_loss, best_loss) , flush=True)\n",
    "        train_total_loss = 0\n",
    "        test_total_loss = 0\n",
    "        return train_tracker, test_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim, n_layers, heads, dropout = 512, 6, 8, 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 200\n",
    "shuffle = False\n",
    "verbose = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t_model = Transformer(\n",
    "    len(train_infield.vocab), \n",
    "    len(train_outfield.vocab), \n",
    "    emb_dim, \n",
    "    n_layers, \n",
    "    heads, \n",
    "    dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_model = Linformer(\n",
    "    len(train_infield.vocab), \n",
    "    len(train_outfield.vocab), \n",
    "    emb_dim, \n",
    "    n_layers, \n",
    "    heads, \n",
    "    dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from linformer_pytorch import LinformerEncDec\n",
    "\n",
    "l_model = LinformerEncDec(\n",
    "    enc_num_tokens=len(train_infield.vocab), #vocaburary size\n",
    "    enc_input_size=opt.max_len, # Dimension 1 of the input of (0, 1 ,2) dimens\n",
    "    enc_channels=emb_dim, # decoder channels dimension should be divisible by number of heads \n",
    "    enc_dropout=dropout, # Dropout for attention\n",
    "    enc_dim_k=128, # The second dimension of the P_bar matrix from the paper\n",
    "    enc_dim_ff=2048, # Dimension in the feed forward network\n",
    "    enc_dropout_ff=dropout, # Dropout for feed forward network\n",
    "    enc_nhead=heads, # Number of attention heads\n",
    "    enc_depth=n_layers, # How many times to run the model\n",
    "    \n",
    "    activation=\"gelu\", # What activation to use. Currently, only gelu and relu supported, and only on ff network.\n",
    "    dec_num_tokens=len(train_outfield.vocab), # Dimension 1 of the input\n",
    "    dec_input_size=opt.max_len, # Dimension 1 of the input\n",
    "    dec_channels=emb_dim, # decoder channels dimension should be divisible by number of heads \n",
    "    dec_dropout=dropout, # Dropout for attention\n",
    "    dec_dim_k=128, # The second dimension of the P_bar matrix from the paper\n",
    "    dec_dim_ff=2048, # Dimension in the feed forward network\n",
    "    dec_dropout_ff=dropout, # Dropout for feed forward network\n",
    "    dec_nhead=heads, # Number of attention heads\n",
    "    dec_depth=n_layers, # How many times to run the model\n",
    ")\n",
    "# l_model = Padder(l_model)\n",
    "# x = torch.randint(1,len(train_infield.vocab),(4,batch))\n",
    "# print(x.shape)\n",
    "# y = torch.randint(1,len(train_outfield.vocab),(8,batch))\n",
    "# output = l_model(x,y)# l_model.train()\n",
    "# print(output.shape)\n",
    "# enc_output = encoder(x, input_mask=x_mask)\n",
    "# print(enc_output.shape) # (1, 512, 128)\n",
    "# dec_output = decoder(y, embeddings=enc_output, input_mask=y_mask, embeddings_mask=x_mask)\n",
    "# print(dec_output.shape) # (1, 512, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Trainable Parameters: 91920028\n",
      "==>Other Parameters: 0\n"
     ]
    }
   ],
   "source": [
    "other_params = 0\n",
    "trainable_params = 0\n",
    "for num in t_model.parameters():\n",
    "    if num.requires_grad:\n",
    "        trainable_params += num.numel()\n",
    "    else:\n",
    "        other_params += num.numel()\n",
    "\n",
    "print(f\"==>Trainable Parameters: {trainable_params}\")\n",
    "print(f\"==>Other Parameters: {other_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Trainable Parameters: 92511004\n",
      "==>Other Parameters: 0\n"
     ]
    }
   ],
   "source": [
    "other_params = 0\n",
    "trainable_params = 0\n",
    "for num in l_model.parameters():\n",
    "    if num.requires_grad:\n",
    "        trainable_params += num.numel()\n",
    "    else:\n",
    "        other_params += num.numel()\n",
    "\n",
    "print(f\"==>Trainable Parameters: {trainable_params}\")\n",
    "print(f\"==>Other Parameters: {other_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### from linformer_pytorch import LinformerLM\n",
    "\n",
    "encoder = LinformerLM(\n",
    "    num_tokens=len(train_infield.vocab),\n",
    "    input_size=batch,\n",
    "    channels=emb_dim,\n",
    "    dim_k=128,\n",
    "    dim_ff=2048,\n",
    "    nhead=heads,\n",
    "    depth=n_layers,\n",
    "    activation=\"relu\",\n",
    "    k_reduce_by_layer=1,\n",
    "    return_emb=True,\n",
    "    )\n",
    "decoder = LinformerLM(\n",
    "    num_tokens=len(train_outfield.vocab),\n",
    "    input_size=batch,\n",
    "    channels=emb_dim,\n",
    "    dim_k=128,\n",
    "    dim_ff=2048,\n",
    "    nhead=heads,\n",
    "    depth=n_layers,\n",
    "    activation=\"relu\",\n",
    "    decoder_mode=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = torch.randint(1,10000,(1,batch))\n",
    "y = torch.randint(1,10000,(1,batch))\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "x_mask = torch.ones_like(x).bool()\n",
    "y_mask = torch.ones_like(y).bool()\n",
    "print(x_mask.shape)\n",
    "print(y_mask.shape)\n",
    "enc_output = encoder(x, input_mask=x_mask)\n",
    "print(enc_output.shape) # (1, 512, 128)\n",
    "dec_output = decoder(y, embeddings=enc_output, input_mask=y_mask, embeddings_mask=x_mask)\n",
    "print(dec_output.shape) # (1, 512, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 0\n",
    "lr = 0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_optimizer = torch.optim.Adam(t_model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "l_optimizer = torch.optim.Adam(l_model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_name = \"plateau\"\n",
    "# t_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(t_optimizer, 'min', factor=0.9, patience=3, verbose=verbose)\n",
    "l_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(l_optimizer, 'min', factor=0.9, patience=3, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0: adjusting learning rate of group 0 to 3.0000e-04.\n",
      "Epoch     0: adjusting learning rate of group 0 to 3.0000e-04.\n"
     ]
    }
   ],
   "source": [
    "scheduler_name = \"cosine\"\n",
    "t_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(t_optimizer, num_batches(train_data_iter), T_mult=1, eta_min=0, last_epoch=-1, verbose=True)\n",
    "l_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(l_optimizer, num_batches(train_data_iter), T_mult=1, eta_min=0, last_epoch=-1, verbose=True)\n",
    "# t_scheduler = CosineWithRestarts(t_optimizer, T_max=num_batches(train_data_iter), verbose=verbose)\n",
    "# l_scheduler = CosineWithRestarts(l_optimizer, T_max=num_batches(train_data_iter), verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate to 0.\n",
      "Adjusting learning rate to 0.\n"
     ]
    }
   ],
   "source": [
    "scheduler_name = \"warmup\"\n",
    "t_scheduler = AdamWarmup(model_size = emb_dim, warmup_steps = 4000, optimizer = t_optimizer, verbose=verbose)\n",
    "t_scheduler.print_lr()\n",
    "l_scheduler = AdamWarmup(model_size = emb_dim, warmup_steps = 4000, optimizer = l_optimizer, verbose=verbose)\n",
    "l_scheduler.print_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> a GPU was detected, model will be trained on GPU\n",
      "0.000m: train epoch *1*, loss = *4.017*, 0.000m: test epoch *1*, loss = *4.104*, best loss = *4.017*\n",
      "1.000m: train epoch *2*, loss = *3.492*, 1.000m: test epoch *2*, loss = *3.939*, best loss = *3.492*\n",
      "1.000m: train epoch *3*, loss = *3.379*, 1.000m: test epoch *3*, loss = *4.027*, best loss = *3.379*\n",
      "2.000m: train epoch *4*, loss = *3.310*, 2.000m: test epoch *4*, loss = *4.200*, best loss = *3.310*\n",
      "2.000m: train epoch *5*, loss = *3.281*, 2.000m: test epoch *5*, loss = *4.054*, best loss = *3.281*\n",
      "3.000m: train epoch *6*, loss = *3.243*, 3.000m: test epoch *6*, loss = *4.065*, best loss = *3.243*\n",
      "4.000m: train epoch *7*, loss = *3.237*, 4.000m: test epoch *7*, loss = *4.131*, best loss = *3.237*\n",
      "4.000m: train epoch *8*, loss = *3.247*, 4.000m: test epoch *8*, loss = *4.140*, best loss = *3.237*\n",
      "5.000m: train epoch *9*, loss = *3.220*, 5.000m: test epoch *9*, loss = *4.090*, best loss = *3.220*\n",
      "5.000m: train epoch *10*, loss = *3.224*, 5.000m: test epoch *10*, loss = *4.163*, best loss = *3.220*\n",
      "6.000m: train epoch *11*, loss = *3.179*, 6.000m: test epoch *11*, loss = *4.210*, best loss = *3.179*\n",
      "6.000m: train epoch *12*, loss = *3.161*, 6.000m: test epoch *12*, loss = *4.156*, best loss = *3.161*\n",
      "7.000m: train epoch *13*, loss = *3.153*, 7.000m: test epoch *13*, loss = *4.220*, best loss = *3.153*\n",
      "8.000m: train epoch *14*, loss = *3.122*, 8.000m: test epoch *14*, loss = *4.321*, best loss = *3.122*\n",
      "8.000m: train epoch *15*, loss = *3.098*, 8.000m: test epoch *15*, loss = *4.197*, best loss = *3.098*\n",
      "9.000m: train epoch *16*, loss = *3.081*, 9.000m: test epoch *16*, loss = *4.219*, best loss = *3.081*\n",
      "9.000m: train epoch *17*, loss = *3.079*, 9.000m: test epoch *17*, loss = *4.236*, best loss = *3.079*\n",
      "10.000m: train epoch *18*, loss = *3.039*, 10.000m: test epoch *18*, loss = *4.226*, best loss = *3.039*\n",
      "10.000m: train epoch *19*, loss = *3.027*, 10.000m: test epoch *19*, loss = *4.199*, best loss = *3.027*\n",
      "11.000m: train epoch *20*, loss = *3.021*, 11.000m: test epoch *20*, loss = *4.281*, best loss = *3.021*\n",
      "11.000m: train epoch *21*, loss = *2.996*, 11.000m: test epoch *21*, loss = *4.336*, best loss = *2.996*\n",
      "12.000m: train epoch *22*, loss = *2.986*, 12.000m: test epoch *22*, loss = *4.314*, best loss = *2.986*\n",
      "13.000m: train epoch *23*, loss = *2.999*, 13.000m: test epoch *23*, loss = *4.303*, best loss = *2.986*\n",
      "13.000m: train epoch *24*, loss = *2.983*, 13.000m: test epoch *24*, loss = *4.346*, best loss = *2.983*\n",
      "14.000m: train epoch *25*, loss = *2.960*, 14.000m: test epoch *25*, loss = *4.410*, best loss = *2.960*\n",
      "14.000m: train epoch *26*, loss = *2.933*, 14.000m: test epoch *26*, loss = *4.255*, best loss = *2.933*\n",
      "15.000m: train epoch *27*, loss = *2.932*, 15.000m: test epoch *27*, loss = *4.352*, best loss = *2.932*\n",
      "15.000m: train epoch *28*, loss = *2.924*, 15.000m: test epoch *28*, loss = *4.400*, best loss = *2.924*\n",
      "16.000m: train epoch *29*, loss = *2.917*, 16.000m: test epoch *29*, loss = *4.315*, best loss = *2.917*\n",
      "17.000m: train epoch *30*, loss = *2.920*, 17.000m: test epoch *30*, loss = *4.466*, best loss = *2.917*\n",
      "17.000m: train epoch *31*, loss = *2.926*, 17.000m: test epoch *31*, loss = *4.306*, best loss = *2.917*\n",
      "18.000m: train epoch *32*, loss = *2.959*, 18.000m: test epoch *32*, loss = *4.281*, best loss = *2.917*\n",
      "Epoch    33: reducing learning rate of group 0 to 9.0000e-03.\n",
      "18.000m: train epoch *33*, loss = *2.924*, 18.000m: test epoch *33*, loss = *4.331*, best loss = *2.917*\n",
      "19.000m: train epoch *34*, loss = *2.870*, 19.000m: test epoch *34*, loss = *4.476*, best loss = *2.870*\n",
      "19.000m: train epoch *35*, loss = *2.853*, 19.000m: test epoch *35*, loss = *4.414*, best loss = *2.853*\n",
      "20.000m: train epoch *36*, loss = *2.848*, 20.000m: test epoch *36*, loss = *4.313*, best loss = *2.848*\n",
      "21.000m: train epoch *37*, loss = *2.824*, 21.000m: test epoch *37*, loss = *4.472*, best loss = *2.824*\n",
      "21.000m: train epoch *38*, loss = *2.837*, 21.000m: test epoch *38*, loss = *4.406*, best loss = *2.824*\n",
      "22.000m: train epoch *39*, loss = *2.817*, 22.000m: test epoch *39*, loss = *4.501*, best loss = *2.817*\n",
      "22.000m: train epoch *40*, loss = *2.811*, 22.000m: test epoch *40*, loss = *4.519*, best loss = *2.811*\n",
      "23.000m: train epoch *41*, loss = *2.809*, 23.000m: test epoch *41*, loss = *4.397*, best loss = *2.809*\n",
      "23.000m: train epoch *42*, loss = *2.796*, 23.000m: test epoch *42*, loss = *4.576*, best loss = *2.796*\n",
      "24.000m: train epoch *43*, loss = *2.786*, 24.000m: test epoch *43*, loss = *4.483*, best loss = *2.786*\n",
      "25.000m: train epoch *44*, loss = *2.760*, 25.000m: test epoch *44*, loss = *4.536*, best loss = *2.760*\n",
      "25.000m: train epoch *45*, loss = *2.760*, 25.000m: test epoch *45*, loss = *4.536*, best loss = *2.760*\n",
      "26.000m: train epoch *46*, loss = *2.743*, 26.000m: test epoch *46*, loss = *4.531*, best loss = *2.743*\n",
      "26.000m: train epoch *47*, loss = *2.744*, 26.000m: test epoch *47*, loss = *4.531*, best loss = *2.743*\n",
      "27.000m: train epoch *48*, loss = *2.747*, 27.000m: test epoch *48*, loss = *4.512*, best loss = *2.743*\n",
      "27.000m: train epoch *49*, loss = *2.732*, 27.000m: test epoch *49*, loss = *4.510*, best loss = *2.732*\n",
      "28.000m: train epoch *50*, loss = *2.723*, 28.000m: test epoch *50*, loss = *4.576*, best loss = *2.723*\n",
      "29.000m: train epoch *51*, loss = *2.722*, 29.000m: test epoch *51*, loss = *4.556*, best loss = *2.722*\n",
      "29.000m: train epoch *52*, loss = *2.721*, 29.000m: test epoch *52*, loss = *4.608*, best loss = *2.721*\n",
      "30.000m: train epoch *53*, loss = *2.702*, 30.000m: test epoch *53*, loss = *4.559*, best loss = *2.702*\n",
      "30.000m: train epoch *54*, loss = *2.691*, 30.000m: test epoch *54*, loss = *4.646*, best loss = *2.691*\n",
      "31.000m: train epoch *55*, loss = *2.689*, 31.000m: test epoch *55*, loss = *4.605*, best loss = *2.689*\n",
      "31.000m: train epoch *56*, loss = *2.703*, 31.000m: test epoch *56*, loss = *4.520*, best loss = *2.689*\n",
      "32.000m: train epoch *57*, loss = *2.703*, 32.000m: test epoch *57*, loss = *4.518*, best loss = *2.689*\n",
      "33.000m: train epoch *58*, loss = *2.682*, 33.000m: test epoch *58*, loss = *4.533*, best loss = *2.682*\n",
      "33.000m: train epoch *59*, loss = *2.671*, 33.000m: test epoch *59*, loss = *4.637*, best loss = *2.671*\n",
      "34.000m: train epoch *60*, loss = *2.664*, 34.000m: test epoch *60*, loss = *4.619*, best loss = *2.664*\n",
      "34.000m: train epoch *61*, loss = *2.636*, 34.000m: test epoch *61*, loss = *4.657*, best loss = *2.636*\n",
      "35.000m: train epoch *62*, loss = *2.629*, 35.000m: test epoch *62*, loss = *4.666*, best loss = *2.629*\n",
      "35.000m: train epoch *63*, loss = *2.639*, 35.000m: test epoch *63*, loss = *4.696*, best loss = *2.629*\n",
      "36.000m: train epoch *64*, loss = *2.635*, 36.000m: test epoch *64*, loss = *4.522*, best loss = *2.629*\n",
      "37.000m: train epoch *65*, loss = *2.641*, 37.000m: test epoch *65*, loss = *4.634*, best loss = *2.629*\n",
      "Epoch    66: reducing learning rate of group 0 to 8.1000e-03.\n",
      "37.000m: train epoch *66*, loss = *2.631*, 37.000m: test epoch *66*, loss = *4.616*, best loss = *2.629*\n",
      "38.000m: train epoch *67*, loss = *2.593*, 38.000m: test epoch *67*, loss = *4.696*, best loss = *2.593*\n",
      "38.000m: train epoch *68*, loss = *2.592*, 38.000m: test epoch *68*, loss = *4.691*, best loss = *2.592*\n",
      "39.000m: train epoch *69*, loss = *2.590*, 39.000m: test epoch *69*, loss = *4.670*, best loss = *2.590*\n",
      "39.000m: train epoch *70*, loss = *2.585*, 39.000m: test epoch *70*, loss = *4.630*, best loss = *2.585*\n",
      "40.000m: train epoch *71*, loss = *2.573*, 40.000m: test epoch *71*, loss = *4.693*, best loss = *2.573*\n",
      "40.000m: train epoch *72*, loss = *2.568*, 40.000m: test epoch *72*, loss = *4.751*, best loss = *2.568*\n",
      "41.000m: train epoch *73*, loss = *2.559*, 41.000m: test epoch *73*, loss = *4.739*, best loss = *2.559*\n",
      "42.000m: train epoch *74*, loss = *2.539*, 42.000m: test epoch *74*, loss = *4.651*, best loss = *2.539*\n",
      "42.000m: train epoch *75*, loss = *2.534*, 42.000m: test epoch *75*, loss = *4.728*, best loss = *2.534*\n",
      "43.000m: train epoch *76*, loss = *2.518*, 43.000m: test epoch *76*, loss = *4.728*, best loss = *2.518*\n",
      "43.000m: train epoch *77*, loss = *2.513*, 43.000m: test epoch *77*, loss = *4.914*, best loss = *2.513*\n",
      "44.000m: train epoch *78*, loss = *2.502*, 44.000m: test epoch *78*, loss = *4.812*, best loss = *2.502*\n",
      "44.000m: train epoch *79*, loss = *2.496*, 44.000m: test epoch *79*, loss = *4.734*, best loss = *2.496*\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-59b903284d43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plateau 0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtransformer_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/deeplearning/project2/scripts/TalkTrain.py\u001b[0m in \u001b[0;36mtransformer_trainer\u001b[0;34m(model, train_data_iterator, train_options, test_data_iterator, test_options, optimizer, scheduler, scheduler_name)\u001b[0m\n\u001b[1;32m    234\u001b[0m                                          ys, ignore_index = train_options.trg_pad)\n\u001b[1;32m    235\u001b[0m             \u001b[0mtrain_batch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0mtrain_total_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_batch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#plateau 0.01\n",
    "transformer_trainer(t_model, train_data_iter, train_opt, test_data_iter, test_opt, t_optimizer, t_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> a GPU was detected, model will be trained on GPU\n",
      "0.000m: train epoch *1*, loss = *3.511*, 0.000m: test epoch *1*, loss = *4.106*, best loss = *3.511*\n",
      "0.000m: train epoch *2*, loss = *2.911*, 0.000m: test epoch *2*, loss = *4.157*, best loss = *2.911*\n",
      "0.000m: train epoch *3*, loss = *2.638*, 0.000m: test epoch *3*, loss = *4.352*, best loss = *2.638*\n",
      "1.000m: train epoch *4*, loss = *2.404*, 1.000m: test epoch *4*, loss = *4.457*, best loss = *2.404*\n",
      "1.000m: train epoch *5*, loss = *2.179*, 1.000m: test epoch *5*, loss = *4.656*, best loss = *2.179*\n",
      "1.000m: train epoch *6*, loss = *1.963*, 1.000m: test epoch *6*, loss = *4.859*, best loss = *1.963*\n",
      "2.000m: train epoch *7*, loss = *1.770*, 2.000m: test epoch *7*, loss = *5.036*, best loss = *1.770*\n",
      "2.000m: train epoch *8*, loss = *1.584*, 2.000m: test epoch *8*, loss = *5.204*, best loss = *1.584*\n",
      "2.000m: train epoch *9*, loss = *1.409*, 2.000m: test epoch *9*, loss = *5.453*, best loss = *1.409*\n",
      "2.000m: train epoch *10*, loss = *1.269*, 2.000m: test epoch *10*, loss = *5.667*, best loss = *1.269*\n",
      "3.000m: train epoch *11*, loss = *1.152*, 3.000m: test epoch *11*, loss = *5.778*, best loss = *1.152*\n",
      "3.000m: train epoch *12*, loss = *1.066*, 3.000m: test epoch *12*, loss = *5.989*, best loss = *1.066*\n",
      "3.000m: train epoch *13*, loss = *0.982*, 3.000m: test epoch *13*, loss = *6.077*, best loss = *0.982*\n",
      "4.000m: train epoch *14*, loss = *0.923*, 4.000m: test epoch *14*, loss = *6.222*, best loss = *0.923*\n",
      "4.000m: train epoch *15*, loss = *0.870*, 4.000m: test epoch *15*, loss = *6.427*, best loss = *0.870*\n",
      "4.000m: train epoch *16*, loss = *0.829*, 4.000m: test epoch *16*, loss = *6.410*, best loss = *0.829*\n",
      "4.000m: train epoch *17*, loss = *0.799*, 4.000m: test epoch *17*, loss = *6.540*, best loss = *0.799*\n",
      "5.000m: train epoch *18*, loss = *0.771*, 5.000m: test epoch *18*, loss = *6.616*, best loss = *0.771*\n",
      "5.000m: train epoch *19*, loss = *0.759*, 5.000m: test epoch *19*, loss = *6.754*, best loss = *0.759*\n",
      "5.000m: train epoch *20*, loss = *0.733*, 5.000m: test epoch *20*, loss = *6.738*, best loss = *0.733*\n",
      "6.000m: train epoch *21*, loss = *0.720*, 6.000m: test epoch *21*, loss = *6.868*, best loss = *0.720*\n",
      "6.000m: train epoch *22*, loss = *0.694*, 6.000m: test epoch *22*, loss = *6.812*, best loss = *0.694*\n",
      "6.000m: train epoch *23*, loss = *0.694*, 6.000m: test epoch *23*, loss = *6.839*, best loss = *0.694*\n",
      "7.000m: train epoch *24*, loss = *0.682*, 7.000m: test epoch *24*, loss = *6.960*, best loss = *0.682*\n",
      "7.000m: train epoch *25*, loss = *0.664*, 7.000m: test epoch *25*, loss = *7.030*, best loss = *0.664*\n",
      "7.000m: train epoch *26*, loss = *0.665*, 7.000m: test epoch *26*, loss = *6.988*, best loss = *0.664*\n",
      "7.000m: train epoch *27*, loss = *0.652*, 7.000m: test epoch *27*, loss = *7.032*, best loss = *0.652*\n",
      "8.000m: train epoch *28*, loss = *0.650*, 8.000m: test epoch *28*, loss = *7.034*, best loss = *0.650*\n",
      "8.000m: train epoch *29*, loss = *0.650*, 8.000m: test epoch *29*, loss = *7.141*, best loss = *0.650*\n",
      "8.000m: train epoch *30*, loss = *0.639*, 8.000m: test epoch *30*, loss = *7.032*, best loss = *0.639*\n",
      "9.000m: train epoch *31*, loss = *0.649*, 9.000m: test epoch *31*, loss = *6.988*, best loss = *0.639*\n",
      "9.000m: train epoch *32*, loss = *0.638*, 9.000m: test epoch *32*, loss = *7.151*, best loss = *0.638*\n",
      "9.000m: train epoch *33*, loss = *0.629*, 9.000m: test epoch *33*, loss = *7.197*, best loss = *0.629*\n",
      "9.000m: train epoch *34*, loss = *0.655*, 9.000m: test epoch *34*, loss = *7.117*, best loss = *0.629*\n",
      "10.000m: train epoch *35*, loss = *0.641*, 10.000m: test epoch *35*, loss = *7.136*, best loss = *0.629*\n",
      "10.000m: train epoch *36*, loss = *0.636*, 10.000m: test epoch *36*, loss = *7.162*, best loss = *0.629*\n",
      "10.000m: train epoch *37*, loss = *0.626*, 10.000m: test epoch *37*, loss = *7.166*, best loss = *0.626*\n",
      "11.000m: train epoch *38*, loss = *0.642*, 11.000m: test epoch *38*, loss = *7.134*, best loss = *0.626*\n",
      "11.000m: train epoch *39*, loss = *0.644*, 11.000m: test epoch *39*, loss = *7.106*, best loss = *0.626*\n",
      "11.000m: train epoch *40*, loss = *0.661*, 11.000m: test epoch *40*, loss = *7.115*, best loss = *0.626*\n",
      "Epoch    41: reducing learning rate of group 0 to 9.0000e-04.\n",
      "11.000m: train epoch *41*, loss = *0.660*, 11.000m: test epoch *41*, loss = *7.221*, best loss = *0.626*\n",
      "12.000m: train epoch *42*, loss = *0.595*, 12.000m: test epoch *42*, loss = *7.221*, best loss = *0.595*\n",
      "12.000m: train epoch *43*, loss = *0.593*, 12.000m: test epoch *43*, loss = *7.160*, best loss = *0.593*\n",
      "12.000m: train epoch *44*, loss = *0.581*, 12.000m: test epoch *44*, loss = *7.221*, best loss = *0.581*\n",
      "13.000m: train epoch *45*, loss = *0.602*, 13.000m: test epoch *45*, loss = *7.229*, best loss = *0.581*\n",
      "13.000m: train epoch *46*, loss = *0.579*, 13.000m: test epoch *46*, loss = *7.349*, best loss = *0.579*\n",
      "13.000m: train epoch *47*, loss = *0.578*, 13.000m: test epoch *47*, loss = *7.307*, best loss = *0.578*\n",
      "14.000m: train epoch *48*, loss = *0.575*, 14.000m: test epoch *48*, loss = *7.346*, best loss = *0.575*\n",
      "14.000m: train epoch *49*, loss = *0.577*, 14.000m: test epoch *49*, loss = *7.304*, best loss = *0.575*\n",
      "14.000m: train epoch *50*, loss = *0.568*, 14.000m: test epoch *50*, loss = *7.429*, best loss = *0.568*\n",
      "14.000m: train epoch *51*, loss = *0.563*, 14.000m: test epoch *51*, loss = *7.359*, best loss = *0.563*\n",
      "15.000m: train epoch *52*, loss = *0.563*, 15.000m: test epoch *52*, loss = *7.337*, best loss = *0.563*\n",
      "15.000m: train epoch *53*, loss = *0.541*, 15.000m: test epoch *53*, loss = *7.414*, best loss = *0.541*\n",
      "15.000m: train epoch *54*, loss = *0.569*, 15.000m: test epoch *54*, loss = *7.494*, best loss = *0.541*\n",
      "16.000m: train epoch *55*, loss = *0.571*, 16.000m: test epoch *55*, loss = *7.480*, best loss = *0.541*\n",
      "16.000m: train epoch *56*, loss = *0.556*, 16.000m: test epoch *56*, loss = *7.406*, best loss = *0.541*\n",
      "Epoch    57: reducing learning rate of group 0 to 8.1000e-04.\n",
      "16.000m: train epoch *57*, loss = *0.576*, 16.000m: test epoch *57*, loss = *7.383*, best loss = *0.541*\n",
      "16.000m: train epoch *58*, loss = *0.512*, 16.000m: test epoch *58*, loss = *7.419*, best loss = *0.512*\n",
      "17.000m: train epoch *59*, loss = *0.491*, 17.000m: test epoch *59*, loss = *7.595*, best loss = *0.491*\n",
      "17.000m: train epoch *60*, loss = *0.518*, 17.000m: test epoch *60*, loss = *7.530*, best loss = *0.491*\n",
      "17.000m: train epoch *61*, loss = *0.499*, 17.000m: test epoch *61*, loss = *7.544*, best loss = *0.491*\n",
      "18.000m: train epoch *62*, loss = *0.473*, 18.000m: test epoch *62*, loss = *7.477*, best loss = *0.473*\n",
      "18.000m: train epoch *63*, loss = *0.488*, 18.000m: test epoch *63*, loss = *7.472*, best loss = *0.473*\n",
      "18.000m: train epoch *64*, loss = *0.483*, 18.000m: test epoch *64*, loss = *7.417*, best loss = *0.473*\n",
      "18.000m: train epoch *65*, loss = *0.455*, 18.000m: test epoch *65*, loss = *7.602*, best loss = *0.455*\n",
      "19.000m: train epoch *66*, loss = *0.469*, 19.000m: test epoch *66*, loss = *7.575*, best loss = *0.455*\n",
      "19.000m: train epoch *67*, loss = *0.445*, 19.000m: test epoch *67*, loss = *7.675*, best loss = *0.445*\n",
      "19.000m: train epoch *68*, loss = *0.440*, 19.000m: test epoch *68*, loss = *7.642*, best loss = *0.440*\n",
      "20.000m: train epoch *69*, loss = *0.454*, 20.000m: test epoch *69*, loss = *7.722*, best loss = *0.440*\n",
      "20.000m: train epoch *70*, loss = *0.429*, 20.000m: test epoch *70*, loss = *7.695*, best loss = *0.429*\n",
      "20.000m: train epoch *71*, loss = *0.416*, 20.000m: test epoch *71*, loss = *7.863*, best loss = *0.416*\n",
      "21.000m: train epoch *72*, loss = *0.408*, 21.000m: test epoch *72*, loss = *7.772*, best loss = *0.408*\n",
      "21.000m: train epoch *73*, loss = *0.404*, 21.000m: test epoch *73*, loss = *7.763*, best loss = *0.404*\n",
      "21.000m: train epoch *74*, loss = *0.412*, 21.000m: test epoch *74*, loss = *7.868*, best loss = *0.404*\n",
      "21.000m: train epoch *75*, loss = *0.416*, 21.000m: test epoch *75*, loss = *8.011*, best loss = *0.404*\n",
      "22.000m: train epoch *76*, loss = *0.400*, 22.000m: test epoch *76*, loss = *7.968*, best loss = *0.400*\n",
      "22.000m: train epoch *77*, loss = *0.395*, 22.000m: test epoch *77*, loss = *7.858*, best loss = *0.395*\n",
      "22.000m: train epoch *78*, loss = *0.384*, 22.000m: test epoch *78*, loss = *7.858*, best loss = *0.384*\n",
      "23.000m: train epoch *79*, loss = *0.369*, 23.000m: test epoch *79*, loss = *7.839*, best loss = *0.369*\n",
      "23.000m: train epoch *80*, loss = *0.355*, 23.000m: test epoch *80*, loss = *8.028*, best loss = *0.355*\n",
      "23.000m: train epoch *81*, loss = *0.365*, 23.000m: test epoch *81*, loss = *8.094*, best loss = *0.355*\n",
      "23.000m: train epoch *82*, loss = *0.360*, 23.000m: test epoch *82*, loss = *8.089*, best loss = *0.355*\n",
      "24.000m: train epoch *83*, loss = *0.353*, 24.000m: test epoch *83*, loss = *7.986*, best loss = *0.353*\n",
      "24.000m: train epoch *84*, loss = *0.362*, 24.000m: test epoch *84*, loss = *7.953*, best loss = *0.353*\n",
      "24.000m: train epoch *85*, loss = *0.352*, 24.000m: test epoch *85*, loss = *8.125*, best loss = *0.352*\n",
      "25.000m: train epoch *86*, loss = *0.341*, 25.000m: test epoch *86*, loss = *8.046*, best loss = *0.341*\n",
      "25.000m: train epoch *87*, loss = *0.337*, 25.000m: test epoch *87*, loss = *8.052*, best loss = *0.337*\n",
      "25.000m: train epoch *88*, loss = *0.334*, 25.000m: test epoch *88*, loss = *8.121*, best loss = *0.334*\n",
      "25.000m: train epoch *89*, loss = *0.338*, 25.000m: test epoch *89*, loss = *8.066*, best loss = *0.334*\n",
      "26.000m: train epoch *90*, loss = *0.331*, 26.000m: test epoch *90*, loss = *7.927*, best loss = *0.331*\n",
      "26.000m: train epoch *91*, loss = *0.336*, 26.000m: test epoch *91*, loss = *8.036*, best loss = *0.331*\n",
      "26.000m: train epoch *92*, loss = *0.327*, 26.000m: test epoch *92*, loss = *8.151*, best loss = *0.327*\n",
      "27.000m: train epoch *93*, loss = *0.316*, 27.000m: test epoch *93*, loss = *8.174*, best loss = *0.316*\n",
      "27.000m: train epoch *94*, loss = *0.318*, 27.000m: test epoch *94*, loss = *8.113*, best loss = *0.316*\n",
      "27.000m: train epoch *95*, loss = *0.321*, 27.000m: test epoch *95*, loss = *8.189*, best loss = *0.316*\n",
      "28.000m: train epoch *96*, loss = *0.313*, 28.000m: test epoch *96*, loss = *8.321*, best loss = *0.313*\n",
      "28.000m: train epoch *97*, loss = *0.311*, 28.000m: test epoch *97*, loss = *8.243*, best loss = *0.311*\n",
      "28.000m: train epoch *98*, loss = *0.308*, 28.000m: test epoch *98*, loss = *8.183*, best loss = *0.308*\n",
      "28.000m: train epoch *99*, loss = *0.305*, 28.000m: test epoch *99*, loss = *8.183*, best loss = *0.305*\n",
      "29.000m: train epoch *100*, loss = *0.300*, 29.000m: test epoch *100*, loss = *8.197*, best loss = *0.300*\n",
      "29.000m: train epoch *101*, loss = *0.298*, 29.000m: test epoch *101*, loss = *8.198*, best loss = *0.298*\n",
      "29.000m: train epoch *102*, loss = *0.298*, 29.000m: test epoch *102*, loss = *8.305*, best loss = *0.298*\n",
      "30.000m: train epoch *103*, loss = *0.297*, 30.000m: test epoch *103*, loss = *8.214*, best loss = *0.297*\n",
      "30.000m: train epoch *104*, loss = *0.297*, 30.000m: test epoch *104*, loss = *8.205*, best loss = *0.297*\n",
      "30.000m: train epoch *105*, loss = *0.292*, 30.000m: test epoch *105*, loss = *8.262*, best loss = *0.292*\n",
      "30.000m: train epoch *106*, loss = *0.297*, 30.000m: test epoch *106*, loss = *8.229*, best loss = *0.292*\n",
      "31.000m: train epoch *107*, loss = *0.288*, 31.000m: test epoch *107*, loss = *8.347*, best loss = *0.288*\n",
      "31.000m: train epoch *108*, loss = *0.286*, 31.000m: test epoch *108*, loss = *8.370*, best loss = *0.286*\n",
      "31.000m: train epoch *109*, loss = *0.287*, 31.000m: test epoch *109*, loss = *8.420*, best loss = *0.286*\n",
      "32.000m: train epoch *110*, loss = *0.284*, 32.000m: test epoch *110*, loss = *8.362*, best loss = *0.284*\n",
      "32.000m: train epoch *111*, loss = *0.278*, 32.000m: test epoch *111*, loss = *8.197*, best loss = *0.278*\n",
      "32.000m: train epoch *112*, loss = *0.281*, 32.000m: test epoch *112*, loss = *8.294*, best loss = *0.278*\n",
      "32.000m: train epoch *113*, loss = *0.271*, 32.000m: test epoch *113*, loss = *8.354*, best loss = *0.271*\n",
      "33.000m: train epoch *114*, loss = *0.269*, 33.000m: test epoch *114*, loss = *8.369*, best loss = *0.269*\n",
      "33.000m: train epoch *115*, loss = *0.274*, 33.000m: test epoch *115*, loss = *8.226*, best loss = *0.269*\n",
      "33.000m: train epoch *116*, loss = *0.275*, 33.000m: test epoch *116*, loss = *8.234*, best loss = *0.269*\n",
      "34.000m: train epoch *117*, loss = *0.271*, 34.000m: test epoch *117*, loss = *8.206*, best loss = *0.269*\n",
      "34.000m: train epoch *118*, loss = *0.263*, 34.000m: test epoch *118*, loss = *8.379*, best loss = *0.263*\n",
      "34.000m: train epoch *119*, loss = *0.269*, 34.000m: test epoch *119*, loss = *8.176*, best loss = *0.263*\n",
      "34.000m: train epoch *120*, loss = *0.270*, 34.000m: test epoch *120*, loss = *8.383*, best loss = *0.263*\n",
      "35.000m: train epoch *121*, loss = *0.259*, 35.000m: test epoch *121*, loss = *8.398*, best loss = *0.259*\n",
      "35.000m: train epoch *122*, loss = *0.261*, 35.000m: test epoch *122*, loss = *8.295*, best loss = *0.259*\n",
      "35.000m: train epoch *123*, loss = *0.261*, 35.000m: test epoch *123*, loss = *8.256*, best loss = *0.259*\n",
      "36.000m: train epoch *124*, loss = *0.262*, 36.000m: test epoch *124*, loss = *8.565*, best loss = *0.259*\n",
      "36.000m: train epoch *125*, loss = *0.257*, 36.000m: test epoch *125*, loss = *8.416*, best loss = *0.257*\n",
      "36.000m: train epoch *126*, loss = *0.260*, 36.000m: test epoch *126*, loss = *8.468*, best loss = *0.257*\n",
      "37.000m: train epoch *127*, loss = *0.252*, 37.000m: test epoch *127*, loss = *8.411*, best loss = *0.252*\n",
      "37.000m: train epoch *128*, loss = *0.253*, 37.000m: test epoch *128*, loss = *8.608*, best loss = *0.252*\n",
      "37.000m: train epoch *129*, loss = *0.252*, 37.000m: test epoch *129*, loss = *8.278*, best loss = *0.252*\n",
      "37.000m: train epoch *130*, loss = *0.251*, 37.000m: test epoch *130*, loss = *8.356*, best loss = *0.251*\n",
      "38.000m: train epoch *131*, loss = *0.247*, 38.000m: test epoch *131*, loss = *8.415*, best loss = *0.247*\n",
      "38.000m: train epoch *132*, loss = *0.245*, 38.000m: test epoch *132*, loss = *8.526*, best loss = *0.245*\n",
      "38.000m: train epoch *133*, loss = *0.250*, 38.000m: test epoch *133*, loss = *8.593*, best loss = *0.245*\n",
      "39.000m: train epoch *134*, loss = *0.243*, 39.000m: test epoch *134*, loss = *8.519*, best loss = *0.243*\n",
      "39.000m: train epoch *135*, loss = *0.246*, 39.000m: test epoch *135*, loss = *8.572*, best loss = *0.243*\n",
      "39.000m: train epoch *136*, loss = *0.242*, 39.000m: test epoch *136*, loss = *8.534*, best loss = *0.242*\n",
      "39.000m: train epoch *137*, loss = *0.241*, 39.000m: test epoch *137*, loss = *8.565*, best loss = *0.241*\n",
      "40.000m: train epoch *138*, loss = *0.243*, 40.000m: test epoch *138*, loss = *8.459*, best loss = *0.241*\n",
      "40.000m: train epoch *139*, loss = *0.239*, 40.000m: test epoch *139*, loss = *8.473*, best loss = *0.239*\n",
      "40.000m: train epoch *140*, loss = *0.244*, 40.000m: test epoch *140*, loss = *8.561*, best loss = *0.239*\n",
      "41.000m: train epoch *141*, loss = *0.236*, 41.000m: test epoch *141*, loss = *8.630*, best loss = *0.236*\n",
      "41.000m: train epoch *142*, loss = *0.236*, 41.000m: test epoch *142*, loss = *8.479*, best loss = *0.236*\n",
      "41.000m: train epoch *143*, loss = *0.233*, 41.000m: test epoch *143*, loss = *8.589*, best loss = *0.233*\n",
      "42.000m: train epoch *144*, loss = *0.235*, 42.000m: test epoch *144*, loss = *8.665*, best loss = *0.233*\n",
      "42.000m: train epoch *145*, loss = *0.237*, 42.000m: test epoch *145*, loss = *8.710*, best loss = *0.233*\n",
      "42.000m: train epoch *146*, loss = *0.231*, 42.000m: test epoch *146*, loss = *8.714*, best loss = *0.231*\n",
      "42.000m: train epoch *147*, loss = *0.231*, 42.000m: test epoch *147*, loss = *8.695*, best loss = *0.231*\n",
      "43.000m: train epoch *148*, loss = *0.233*, 43.000m: test epoch *148*, loss = *8.629*, best loss = *0.231*\n",
      "43.000m: train epoch *149*, loss = *0.228*, 43.000m: test epoch *149*, loss = *8.664*, best loss = *0.228*\n",
      "43.000m: train epoch *150*, loss = *0.231*, 43.000m: test epoch *150*, loss = *8.605*, best loss = *0.228*\n",
      "44.000m: train epoch *151*, loss = *0.225*, 44.000m: test epoch *151*, loss = *8.856*, best loss = *0.225*\n",
      "44.000m: train epoch *152*, loss = *0.225*, 44.000m: test epoch *152*, loss = *8.844*, best loss = *0.225*\n",
      "44.000m: train epoch *153*, loss = *0.230*, 44.000m: test epoch *153*, loss = *8.838*, best loss = *0.225*\n",
      "44.000m: train epoch *154*, loss = *0.227*, 44.000m: test epoch *154*, loss = *8.759*, best loss = *0.225*\n",
      "45.000m: train epoch *155*, loss = *0.222*, 45.000m: test epoch *155*, loss = *8.873*, best loss = *0.222*\n",
      "45.000m: train epoch *156*, loss = *0.224*, 45.000m: test epoch *156*, loss = *8.771*, best loss = *0.222*\n",
      "45.000m: train epoch *157*, loss = *0.224*, 45.000m: test epoch *157*, loss = *8.861*, best loss = *0.222*\n",
      "46.000m: train epoch *158*, loss = *0.222*, 46.000m: test epoch *158*, loss = *9.009*, best loss = *0.222*\n",
      "Epoch   159: reducing learning rate of group 0 to 7.2900e-04.\n",
      "46.000m: train epoch *159*, loss = *0.222*, 46.000m: test epoch *159*, loss = *8.724*, best loss = *0.222*\n",
      "46.000m: train epoch *160*, loss = *0.213*, 46.000m: test epoch *160*, loss = *8.894*, best loss = *0.213*\n",
      "46.000m: train epoch *161*, loss = *0.206*, 46.000m: test epoch *161*, loss = *8.864*, best loss = *0.206*\n",
      "47.000m: train epoch *162*, loss = *0.208*, 47.000m: test epoch *162*, loss = *8.907*, best loss = *0.206*\n",
      "47.000m: train epoch *163*, loss = *0.206*, 47.000m: test epoch *163*, loss = *8.971*, best loss = *0.206*\n",
      "47.000m: train epoch *164*, loss = *0.205*, 47.000m: test epoch *164*, loss = *8.908*, best loss = *0.205*\n",
      "48.000m: train epoch *165*, loss = *0.206*, 48.000m: test epoch *165*, loss = *8.778*, best loss = *0.205*\n",
      "48.000m: train epoch *166*, loss = *0.203*, 48.000m: test epoch *166*, loss = *8.938*, best loss = *0.203*\n",
      "48.000m: train epoch *167*, loss = *0.203*, 48.000m: test epoch *167*, loss = *8.773*, best loss = *0.203*\n",
      "48.000m: train epoch *168*, loss = *0.200*, 48.000m: test epoch *168*, loss = *8.963*, best loss = *0.200*\n",
      "49.000m: train epoch *169*, loss = *0.201*, 49.000m: test epoch *169*, loss = *9.052*, best loss = *0.200*\n",
      "49.000m: train epoch *170*, loss = *0.200*, 49.000m: test epoch *170*, loss = *8.999*, best loss = *0.200*\n",
      "49.000m: train epoch *171*, loss = *0.197*, 49.000m: test epoch *171*, loss = *8.931*, best loss = *0.197*\n",
      "50.000m: train epoch *172*, loss = *0.200*, 50.000m: test epoch *172*, loss = *9.144*, best loss = *0.197*\n",
      "50.000m: train epoch *173*, loss = *0.200*, 50.000m: test epoch *173*, loss = *8.994*, best loss = *0.197*\n",
      "50.000m: train epoch *174*, loss = *0.196*, 50.000m: test epoch *174*, loss = *8.988*, best loss = *0.196*\n",
      "50.000m: train epoch *175*, loss = *0.201*, 50.000m: test epoch *175*, loss = *8.913*, best loss = *0.196*\n",
      "51.000m: train epoch *176*, loss = *0.198*, 51.000m: test epoch *176*, loss = *9.034*, best loss = *0.196*\n",
      "51.000m: train epoch *177*, loss = *0.197*, 51.000m: test epoch *177*, loss = *9.153*, best loss = *0.196*\n",
      "Epoch   178: reducing learning rate of group 0 to 6.5610e-04.\n",
      "51.000m: train epoch *178*, loss = *0.198*, 51.000m: test epoch *178*, loss = *8.973*, best loss = *0.196*\n",
      "52.000m: train epoch *179*, loss = *0.188*, 52.000m: test epoch *179*, loss = *9.267*, best loss = *0.188*\n",
      "52.000m: train epoch *180*, loss = *0.184*, 52.000m: test epoch *180*, loss = *9.294*, best loss = *0.184*\n",
      "52.000m: train epoch *181*, loss = *0.184*, 52.000m: test epoch *181*, loss = *9.212*, best loss = *0.184*\n",
      "52.000m: train epoch *182*, loss = *0.181*, 52.000m: test epoch *182*, loss = *9.130*, best loss = *0.181*\n",
      "53.000m: train epoch *183*, loss = *0.183*, 53.000m: test epoch *183*, loss = *9.440*, best loss = *0.181*\n",
      "53.000m: train epoch *184*, loss = *0.183*, 53.000m: test epoch *184*, loss = *9.341*, best loss = *0.181*\n",
      "53.000m: train epoch *185*, loss = *0.182*, 53.000m: test epoch *185*, loss = *9.349*, best loss = *0.181*\n",
      "54.000m: train epoch *186*, loss = *0.181*, 54.000m: test epoch *186*, loss = *9.136*, best loss = *0.181*\n",
      "54.000m: train epoch *187*, loss = *0.183*, 54.000m: test epoch *187*, loss = *9.299*, best loss = *0.181*\n",
      "54.000m: train epoch *188*, loss = *0.184*, 54.000m: test epoch *188*, loss = *9.306*, best loss = *0.181*\n",
      "54.000m: train epoch *189*, loss = *0.185*, 54.000m: test epoch *189*, loss = *8.988*, best loss = *0.181*\n",
      "Epoch   190: reducing learning rate of group 0 to 5.9049e-04.\n",
      "55.000m: train epoch *190*, loss = *0.182*, 55.000m: test epoch *190*, loss = *9.127*, best loss = *0.181*\n",
      "55.000m: train epoch *191*, loss = *0.173*, 55.000m: test epoch *191*, loss = *9.264*, best loss = *0.173*\n",
      "55.000m: train epoch *192*, loss = *0.175*, 55.000m: test epoch *192*, loss = *9.316*, best loss = *0.173*\n",
      "56.000m: train epoch *193*, loss = *0.173*, 56.000m: test epoch *193*, loss = *9.150*, best loss = *0.173*\n",
      "56.000m: train epoch *194*, loss = *0.173*, 56.000m: test epoch *194*, loss = *9.139*, best loss = *0.173*\n",
      "56.000m: train epoch *195*, loss = *0.173*, 56.000m: test epoch *195*, loss = *9.189*, best loss = *0.173*\n",
      "56.000m: train epoch *196*, loss = *0.172*, 56.000m: test epoch *196*, loss = *9.216*, best loss = *0.172*\n",
      "57.000m: train epoch *197*, loss = *0.172*, 57.000m: test epoch *197*, loss = *9.103*, best loss = *0.172*\n",
      "57.000m: train epoch *198*, loss = *0.170*, 57.000m: test epoch *198*, loss = *9.104*, best loss = *0.170*\n",
      "57.000m: train epoch *199*, loss = *0.173*, 57.000m: test epoch *199*, loss = *9.047*, best loss = *0.170*\n",
      "58.000m: train epoch *200*, loss = *0.169*, 58.000m: test epoch *200*, loss = *9.265*, best loss = *0.169*\n"
     ]
    }
   ],
   "source": [
    "#plateau 0.001\n",
    "transformer_trainer(t_model, train_data_iter, train_opt, test_data_iter, test_opt, t_optimizer, t_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> a GPU was detected, model will be trained on GPU\n",
      "0.000m: train epoch *1*, loss = *0.255*, 0.000m: test epoch *1*, loss = *8.522*, best loss = *0.255*\n",
      "0.000m: train epoch *2*, loss = *0.226*, 0.000m: test epoch *2*, loss = *8.750*, best loss = *0.226*\n",
      "0.000m: train epoch *3*, loss = *0.225*, 0.000m: test epoch *3*, loss = *8.898*, best loss = *0.225*\n",
      "1.000m: train epoch *4*, loss = *0.219*, 1.000m: test epoch *4*, loss = *9.049*, best loss = *0.219*\n",
      "1.000m: train epoch *5*, loss = *0.225*, 1.000m: test epoch *5*, loss = *9.135*, best loss = *0.219*\n",
      "1.000m: train epoch *6*, loss = *0.225*, 1.000m: test epoch *6*, loss = *9.247*, best loss = *0.219*\n",
      "2.000m: train epoch *7*, loss = *0.225*, 2.000m: test epoch *7*, loss = *9.259*, best loss = *0.219*\n",
      "2.000m: train epoch *8*, loss = *0.225*, 2.000m: test epoch *8*, loss = *9.234*, best loss = *0.219*\n",
      "2.000m: train epoch *9*, loss = *0.224*, 2.000m: test epoch *9*, loss = *9.505*, best loss = *0.219*\n",
      "2.000m: train epoch *10*, loss = *0.220*, 2.000m: test epoch *10*, loss = *9.292*, best loss = *0.219*\n",
      "3.000m: train epoch *11*, loss = *0.223*, 3.000m: test epoch *11*, loss = *9.339*, best loss = *0.219*\n",
      "3.000m: train epoch *12*, loss = *0.224*, 3.000m: test epoch *12*, loss = *9.205*, best loss = *0.219*\n",
      "3.000m: train epoch *13*, loss = *0.218*, 3.000m: test epoch *13*, loss = *9.366*, best loss = *0.218*\n",
      "4.000m: train epoch *14*, loss = *0.216*, 4.000m: test epoch *14*, loss = *9.432*, best loss = *0.216*\n",
      "4.000m: train epoch *15*, loss = *0.223*, 4.000m: test epoch *15*, loss = *9.481*, best loss = *0.216*\n",
      "4.000m: train epoch *16*, loss = *0.216*, 4.000m: test epoch *16*, loss = *9.476*, best loss = *0.216*\n",
      "4.000m: train epoch *17*, loss = *0.213*, 4.000m: test epoch *17*, loss = *9.537*, best loss = *0.213*\n",
      "5.000m: train epoch *18*, loss = *0.217*, 5.000m: test epoch *18*, loss = *9.589*, best loss = *0.213*\n",
      "5.000m: train epoch *19*, loss = *0.215*, 5.000m: test epoch *19*, loss = *9.455*, best loss = *0.213*\n",
      "5.000m: train epoch *20*, loss = *0.219*, 5.000m: test epoch *20*, loss = *9.799*, best loss = *0.213*\n",
      "6.000m: train epoch *21*, loss = *0.215*, 6.000m: test epoch *21*, loss = *9.633*, best loss = *0.213*\n",
      "6.000m: train epoch *22*, loss = *0.214*, 6.000m: test epoch *22*, loss = *9.673*, best loss = *0.213*\n",
      "6.000m: train epoch *23*, loss = *0.213*, 6.000m: test epoch *23*, loss = *9.390*, best loss = *0.213*\n",
      "6.000m: train epoch *24*, loss = *0.214*, 6.000m: test epoch *24*, loss = *9.752*, best loss = *0.213*\n",
      "7.000m: train epoch *25*, loss = *0.213*, 7.000m: test epoch *25*, loss = *9.787*, best loss = *0.213*\n",
      "7.000m: train epoch *26*, loss = *0.212*, 7.000m: test epoch *26*, loss = *9.687*, best loss = *0.212*\n",
      "7.000m: train epoch *27*, loss = *0.215*, 7.000m: test epoch *27*, loss = *9.733*, best loss = *0.212*\n",
      "8.000m: train epoch *28*, loss = *0.211*, 8.000m: test epoch *28*, loss = *9.753*, best loss = *0.211*\n",
      "8.000m: train epoch *29*, loss = *0.209*, 8.000m: test epoch *29*, loss = *9.671*, best loss = *0.209*\n",
      "8.000m: train epoch *30*, loss = *0.205*, 8.000m: test epoch *30*, loss = *9.821*, best loss = *0.205*\n",
      "8.000m: train epoch *31*, loss = *0.204*, 8.000m: test epoch *31*, loss = *9.707*, best loss = *0.204*\n",
      "9.000m: train epoch *32*, loss = *0.207*, 9.000m: test epoch *32*, loss = *9.696*, best loss = *0.204*\n",
      "9.000m: train epoch *33*, loss = *0.205*, 9.000m: test epoch *33*, loss = *9.755*, best loss = *0.204*\n",
      "9.000m: train epoch *34*, loss = *0.203*, 9.000m: test epoch *34*, loss = *9.908*, best loss = *0.203*\n",
      "10.000m: train epoch *35*, loss = *0.202*, 10.000m: test epoch *35*, loss = *9.745*, best loss = *0.202*\n",
      "10.000m: train epoch *36*, loss = *0.203*, 10.000m: test epoch *36*, loss = *9.691*, best loss = *0.202*\n",
      "10.000m: train epoch *37*, loss = *0.199*, 10.000m: test epoch *37*, loss = *9.706*, best loss = *0.199*\n",
      "10.000m: train epoch *38*, loss = *0.199*, 10.000m: test epoch *38*, loss = *9.798*, best loss = *0.199*\n",
      "11.000m: train epoch *39*, loss = *0.202*, 11.000m: test epoch *39*, loss = *9.915*, best loss = *0.199*\n",
      "11.000m: train epoch *40*, loss = *0.201*, 11.000m: test epoch *40*, loss = *9.804*, best loss = *0.199*\n",
      "11.000m: train epoch *41*, loss = *0.198*, 11.000m: test epoch *41*, loss = *9.680*, best loss = *0.198*\n",
      "12.000m: train epoch *42*, loss = *0.196*, 12.000m: test epoch *42*, loss = *9.937*, best loss = *0.196*\n",
      "12.000m: train epoch *43*, loss = *0.199*, 12.000m: test epoch *43*, loss = *9.816*, best loss = *0.196*\n",
      "12.000m: train epoch *44*, loss = *0.197*, 12.000m: test epoch *44*, loss = *9.797*, best loss = *0.196*\n",
      "12.000m: train epoch *45*, loss = *0.195*, 12.000m: test epoch *45*, loss = *9.974*, best loss = *0.195*\n",
      "13.000m: train epoch *46*, loss = *0.195*, 13.000m: test epoch *46*, loss = *9.955*, best loss = *0.195*\n",
      "13.000m: train epoch *47*, loss = *0.194*, 13.000m: test epoch *47*, loss = *9.800*, best loss = *0.194*\n",
      "13.000m: train epoch *48*, loss = *0.194*, 13.000m: test epoch *48*, loss = *10.237*, best loss = *0.194*\n",
      "14.000m: train epoch *49*, loss = *0.189*, 14.000m: test epoch *49*, loss = *9.816*, best loss = *0.189*\n",
      "14.000m: train epoch *50*, loss = *0.193*, 14.000m: test epoch *50*, loss = *9.935*, best loss = *0.189*\n",
      "14.000m: train epoch *51*, loss = *0.192*, 14.000m: test epoch *51*, loss = *9.945*, best loss = *0.189*\n",
      "14.000m: train epoch *52*, loss = *0.188*, 14.000m: test epoch *52*, loss = *10.268*, best loss = *0.188*\n",
      "15.000m: train epoch *53*, loss = *0.187*, 15.000m: test epoch *53*, loss = *9.958*, best loss = *0.187*\n",
      "15.000m: train epoch *54*, loss = *0.190*, 15.000m: test epoch *54*, loss = *10.083*, best loss = *0.187*\n",
      "15.000m: train epoch *55*, loss = *0.185*, 15.000m: test epoch *55*, loss = *10.084*, best loss = *0.185*\n",
      "16.000m: train epoch *56*, loss = *0.184*, 16.000m: test epoch *56*, loss = *10.147*, best loss = *0.184*\n",
      "16.000m: train epoch *57*, loss = *0.186*, 16.000m: test epoch *57*, loss = *10.184*, best loss = *0.184*\n",
      "16.000m: train epoch *58*, loss = *0.184*, 16.000m: test epoch *58*, loss = *10.109*, best loss = *0.184*\n",
      "17.000m: train epoch *59*, loss = *0.181*, 17.000m: test epoch *59*, loss = *9.991*, best loss = *0.181*\n",
      "17.000m: train epoch *60*, loss = *0.182*, 17.000m: test epoch *60*, loss = *10.024*, best loss = *0.181*\n",
      "17.000m: train epoch *61*, loss = *0.180*, 17.000m: test epoch *61*, loss = *10.082*, best loss = *0.180*\n",
      "17.000m: train epoch *62*, loss = *0.181*, 17.000m: test epoch *62*, loss = *10.108*, best loss = *0.180*\n",
      "18.000m: train epoch *63*, loss = *0.180*, 18.000m: test epoch *63*, loss = *10.179*, best loss = *0.180*\n",
      "18.000m: train epoch *64*, loss = *0.179*, 18.000m: test epoch *64*, loss = *10.235*, best loss = *0.179*\n",
      "18.000m: train epoch *65*, loss = *0.178*, 18.000m: test epoch *65*, loss = *9.945*, best loss = *0.178*\n",
      "19.000m: train epoch *66*, loss = *0.175*, 19.000m: test epoch *66*, loss = *10.140*, best loss = *0.175*\n",
      "19.000m: train epoch *67*, loss = *0.178*, 19.000m: test epoch *67*, loss = *10.157*, best loss = *0.175*\n",
      "19.000m: train epoch *68*, loss = *0.175*, 19.000m: test epoch *68*, loss = *10.094*, best loss = *0.175*\n",
      "19.000m: train epoch *69*, loss = *0.174*, 19.000m: test epoch *69*, loss = *10.200*, best loss = *0.174*\n",
      "20.000m: train epoch *70*, loss = *0.175*, 20.000m: test epoch *70*, loss = *10.115*, best loss = *0.174*\n",
      "20.000m: train epoch *71*, loss = *0.173*, 20.000m: test epoch *71*, loss = *10.207*, best loss = *0.173*\n",
      "20.000m: train epoch *72*, loss = *0.174*, 20.000m: test epoch *72*, loss = *9.985*, best loss = *0.173*\n",
      "21.000m: train epoch *73*, loss = *0.171*, 21.000m: test epoch *73*, loss = *10.146*, best loss = *0.171*\n",
      "21.000m: train epoch *74*, loss = *0.170*, 21.000m: test epoch *74*, loss = *10.291*, best loss = *0.170*\n",
      "21.000m: train epoch *75*, loss = *0.172*, 21.000m: test epoch *75*, loss = *10.169*, best loss = *0.170*\n",
      "21.000m: train epoch *76*, loss = *0.168*, 21.000m: test epoch *76*, loss = *10.444*, best loss = *0.168*\n",
      "22.000m: train epoch *77*, loss = *0.170*, 22.000m: test epoch *77*, loss = *10.088*, best loss = *0.168*\n",
      "22.000m: train epoch *78*, loss = *0.169*, 22.000m: test epoch *78*, loss = *10.262*, best loss = *0.168*\n",
      "22.000m: train epoch *79*, loss = *0.168*, 22.000m: test epoch *79*, loss = *10.199*, best loss = *0.168*\n",
      "23.000m: train epoch *80*, loss = *0.168*, 23.000m: test epoch *80*, loss = *10.254*, best loss = *0.168*\n",
      "23.000m: train epoch *81*, loss = *0.166*, 23.000m: test epoch *81*, loss = *10.115*, best loss = *0.166*\n",
      "23.000m: train epoch *82*, loss = *0.166*, 23.000m: test epoch *82*, loss = *10.171*, best loss = *0.166*\n",
      "23.000m: train epoch *83*, loss = *0.166*, 23.000m: test epoch *83*, loss = *10.188*, best loss = *0.166*\n",
      "24.000m: train epoch *84*, loss = *0.165*, 24.000m: test epoch *84*, loss = *10.071*, best loss = *0.165*\n",
      "24.000m: train epoch *85*, loss = *0.164*, 24.000m: test epoch *85*, loss = *10.187*, best loss = *0.164*\n",
      "24.000m: train epoch *86*, loss = *0.162*, 24.000m: test epoch *86*, loss = *10.045*, best loss = *0.162*\n",
      "25.000m: train epoch *87*, loss = *0.164*, 25.000m: test epoch *87*, loss = *10.227*, best loss = *0.162*\n",
      "25.000m: train epoch *88*, loss = *0.162*, 25.000m: test epoch *88*, loss = *10.484*, best loss = *0.162*\n",
      "25.000m: train epoch *89*, loss = *0.160*, 25.000m: test epoch *89*, loss = *10.025*, best loss = *0.160*\n",
      "25.000m: train epoch *90*, loss = *0.162*, 25.000m: test epoch *90*, loss = *10.091*, best loss = *0.160*\n",
      "26.000m: train epoch *91*, loss = *0.161*, 26.000m: test epoch *91*, loss = *10.080*, best loss = *0.160*\n",
      "26.000m: train epoch *92*, loss = *0.159*, 26.000m: test epoch *92*, loss = *10.109*, best loss = *0.159*\n",
      "26.000m: train epoch *93*, loss = *0.160*, 26.000m: test epoch *93*, loss = *10.205*, best loss = *0.159*\n",
      "27.000m: train epoch *94*, loss = *0.157*, 27.000m: test epoch *94*, loss = *10.003*, best loss = *0.157*\n",
      "27.000m: train epoch *95*, loss = *0.158*, 27.000m: test epoch *95*, loss = *10.123*, best loss = *0.157*\n",
      "27.000m: train epoch *96*, loss = *0.157*, 27.000m: test epoch *96*, loss = *10.302*, best loss = *0.157*\n",
      "27.000m: train epoch *97*, loss = *0.156*, 27.000m: test epoch *97*, loss = *10.155*, best loss = *0.156*\n",
      "28.000m: train epoch *98*, loss = *0.154*, 28.000m: test epoch *98*, loss = *10.280*, best loss = *0.154*\n",
      "28.000m: train epoch *99*, loss = *0.156*, 28.000m: test epoch *99*, loss = *10.211*, best loss = *0.154*\n",
      "28.000m: train epoch *100*, loss = *0.156*, 28.000m: test epoch *100*, loss = *10.393*, best loss = *0.154*\n",
      "29.000m: train epoch *101*, loss = *0.155*, 29.000m: test epoch *101*, loss = *10.254*, best loss = *0.154*\n",
      "29.000m: train epoch *102*, loss = *0.154*, 29.000m: test epoch *102*, loss = *10.312*, best loss = *0.154*\n",
      "29.000m: train epoch *103*, loss = *0.152*, 29.000m: test epoch *103*, loss = *10.385*, best loss = *0.152*\n",
      "29.000m: train epoch *104*, loss = *0.153*, 29.000m: test epoch *104*, loss = *10.295*, best loss = *0.152*\n",
      "30.000m: train epoch *105*, loss = *0.153*, 30.000m: test epoch *105*, loss = *10.097*, best loss = *0.152*\n",
      "30.000m: train epoch *106*, loss = *0.151*, 30.000m: test epoch *106*, loss = *10.356*, best loss = *0.151*\n",
      "30.000m: train epoch *107*, loss = *0.152*, 30.000m: test epoch *107*, loss = *10.112*, best loss = *0.151*\n",
      "31.000m: train epoch *108*, loss = *0.150*, 31.000m: test epoch *108*, loss = *10.158*, best loss = *0.150*\n",
      "31.000m: train epoch *109*, loss = *0.150*, 31.000m: test epoch *109*, loss = *10.164*, best loss = *0.150*\n",
      "31.000m: train epoch *110*, loss = *0.149*, 31.000m: test epoch *110*, loss = *10.387*, best loss = *0.149*\n",
      "31.000m: train epoch *111*, loss = *0.150*, 31.000m: test epoch *111*, loss = *10.335*, best loss = *0.149*\n",
      "32.000m: train epoch *112*, loss = *0.148*, 32.000m: test epoch *112*, loss = *10.274*, best loss = *0.148*\n",
      "32.000m: train epoch *113*, loss = *0.149*, 32.000m: test epoch *113*, loss = *10.101*, best loss = *0.148*\n",
      "32.000m: train epoch *114*, loss = *0.147*, 32.000m: test epoch *114*, loss = *10.216*, best loss = *0.147*\n",
      "33.000m: train epoch *115*, loss = *0.147*, 33.000m: test epoch *115*, loss = *10.293*, best loss = *0.147*\n",
      "33.000m: train epoch *116*, loss = *0.149*, 33.000m: test epoch *116*, loss = *9.978*, best loss = *0.147*\n",
      "33.000m: train epoch *117*, loss = *0.145*, 33.000m: test epoch *117*, loss = *10.231*, best loss = *0.145*\n",
      "33.000m: train epoch *118*, loss = *0.146*, 33.000m: test epoch *118*, loss = *10.220*, best loss = *0.145*\n",
      "34.000m: train epoch *119*, loss = *0.145*, 34.000m: test epoch *119*, loss = *10.171*, best loss = *0.145*\n",
      "34.000m: train epoch *120*, loss = *0.143*, 34.000m: test epoch *120*, loss = *10.152*, best loss = *0.143*\n",
      "34.000m: train epoch *121*, loss = *0.145*, 34.000m: test epoch *121*, loss = *10.044*, best loss = *0.143*\n",
      "35.000m: train epoch *122*, loss = *0.143*, 35.000m: test epoch *122*, loss = *10.195*, best loss = *0.143*\n",
      "35.000m: train epoch *123*, loss = *0.142*, 35.000m: test epoch *123*, loss = *10.117*, best loss = *0.142*\n",
      "35.000m: train epoch *124*, loss = *0.142*, 35.000m: test epoch *124*, loss = *10.021*, best loss = *0.142*\n",
      "35.000m: train epoch *125*, loss = *0.143*, 35.000m: test epoch *125*, loss = *10.176*, best loss = *0.142*\n",
      "36.000m: train epoch *126*, loss = *0.144*, 36.000m: test epoch *126*, loss = *10.226*, best loss = *0.142*\n",
      "36.000m: train epoch *127*, loss = *0.141*, 36.000m: test epoch *127*, loss = *10.192*, best loss = *0.141*\n",
      "36.000m: train epoch *128*, loss = *0.141*, 36.000m: test epoch *128*, loss = *10.082*, best loss = *0.141*\n",
      "37.000m: train epoch *129*, loss = *0.141*, 37.000m: test epoch *129*, loss = *10.134*, best loss = *0.141*\n",
      "37.000m: train epoch *130*, loss = *0.140*, 37.000m: test epoch *130*, loss = *10.186*, best loss = *0.140*\n",
      "37.000m: train epoch *131*, loss = *0.139*, 37.000m: test epoch *131*, loss = *10.132*, best loss = *0.139*\n",
      "37.000m: train epoch *132*, loss = *0.138*, 37.000m: test epoch *132*, loss = *10.136*, best loss = *0.138*\n",
      "38.000m: train epoch *133*, loss = *0.138*, 38.000m: test epoch *133*, loss = *9.959*, best loss = *0.138*\n",
      "38.000m: train epoch *134*, loss = *0.137*, 38.000m: test epoch *134*, loss = *10.033*, best loss = *0.137*\n",
      "38.000m: train epoch *135*, loss = *0.137*, 38.000m: test epoch *135*, loss = *10.066*, best loss = *0.137*\n",
      "39.000m: train epoch *136*, loss = *0.137*, 39.000m: test epoch *136*, loss = *10.072*, best loss = *0.137*\n",
      "39.000m: train epoch *137*, loss = *0.138*, 39.000m: test epoch *137*, loss = *9.910*, best loss = *0.137*\n",
      "39.000m: train epoch *138*, loss = *0.137*, 39.000m: test epoch *138*, loss = *10.012*, best loss = *0.137*\n",
      "39.000m: train epoch *139*, loss = *0.136*, 39.000m: test epoch *139*, loss = *10.029*, best loss = *0.136*\n",
      "40.000m: train epoch *140*, loss = *0.135*, 40.000m: test epoch *140*, loss = *10.102*, best loss = *0.135*\n",
      "40.000m: train epoch *141*, loss = *0.135*, 40.000m: test epoch *141*, loss = *10.137*, best loss = *0.135*\n",
      "40.000m: train epoch *142*, loss = *0.135*, 40.000m: test epoch *142*, loss = *9.979*, best loss = *0.135*\n",
      "41.000m: train epoch *143*, loss = *0.133*, 41.000m: test epoch *143*, loss = *9.949*, best loss = *0.133*\n",
      "41.000m: train epoch *144*, loss = *0.135*, 41.000m: test epoch *144*, loss = *9.998*, best loss = *0.133*\n",
      "41.000m: train epoch *145*, loss = *0.134*, 41.000m: test epoch *145*, loss = *10.013*, best loss = *0.133*\n",
      "41.000m: train epoch *146*, loss = *0.134*, 41.000m: test epoch *146*, loss = *9.964*, best loss = *0.133*\n",
      "42.000m: train epoch *147*, loss = *0.133*, 42.000m: test epoch *147*, loss = *9.981*, best loss = *0.133*\n",
      "42.000m: train epoch *148*, loss = *0.131*, 42.000m: test epoch *148*, loss = *9.966*, best loss = *0.131*\n",
      "42.000m: train epoch *149*, loss = *0.133*, 42.000m: test epoch *149*, loss = *9.976*, best loss = *0.131*\n",
      "43.000m: train epoch *150*, loss = *0.131*, 43.000m: test epoch *150*, loss = *10.006*, best loss = *0.131*\n",
      "43.000m: train epoch *151*, loss = *0.132*, 43.000m: test epoch *151*, loss = *9.897*, best loss = *0.131*\n",
      "43.000m: train epoch *152*, loss = *0.132*, 43.000m: test epoch *152*, loss = *10.017*, best loss = *0.131*\n",
      "43.000m: train epoch *153*, loss = *0.130*, 43.000m: test epoch *153*, loss = *9.949*, best loss = *0.130*\n",
      "44.000m: train epoch *154*, loss = *0.131*, 44.000m: test epoch *154*, loss = *9.932*, best loss = *0.130*\n",
      "44.000m: train epoch *155*, loss = *0.129*, 44.000m: test epoch *155*, loss = *9.861*, best loss = *0.129*\n",
      "44.000m: train epoch *156*, loss = *0.130*, 44.000m: test epoch *156*, loss = *9.818*, best loss = *0.129*\n",
      "45.000m: train epoch *157*, loss = *0.129*, 45.000m: test epoch *157*, loss = *9.820*, best loss = *0.129*\n",
      "45.000m: train epoch *158*, loss = *0.130*, 45.000m: test epoch *158*, loss = *9.777*, best loss = *0.129*\n",
      "45.000m: train epoch *159*, loss = *0.129*, 45.000m: test epoch *159*, loss = *9.783*, best loss = *0.129*\n",
      "46.000m: train epoch *160*, loss = *0.127*, 46.000m: test epoch *160*, loss = *9.734*, best loss = *0.127*\n",
      "46.000m: train epoch *161*, loss = *0.128*, 46.000m: test epoch *161*, loss = *9.768*, best loss = *0.127*\n",
      "46.000m: train epoch *162*, loss = *0.129*, 46.000m: test epoch *162*, loss = *9.718*, best loss = *0.127*\n",
      "46.000m: train epoch *163*, loss = *0.127*, 46.000m: test epoch *163*, loss = *9.671*, best loss = *0.127*\n",
      "47.000m: train epoch *164*, loss = *0.127*, 47.000m: test epoch *164*, loss = *9.713*, best loss = *0.127*\n",
      "47.000m: train epoch *165*, loss = *0.126*, 47.000m: test epoch *165*, loss = *9.683*, best loss = *0.126*\n",
      "47.000m: train epoch *166*, loss = *0.127*, 47.000m: test epoch *166*, loss = *9.670*, best loss = *0.126*\n",
      "48.000m: train epoch *167*, loss = *0.127*, 48.000m: test epoch *167*, loss = *9.704*, best loss = *0.126*\n",
      "48.000m: train epoch *168*, loss = *0.126*, 48.000m: test epoch *168*, loss = *9.731*, best loss = *0.126*\n",
      "48.000m: train epoch *169*, loss = *0.125*, 48.000m: test epoch *169*, loss = *9.712*, best loss = *0.125*\n",
      "48.000m: train epoch *170*, loss = *0.124*, 48.000m: test epoch *170*, loss = *9.768*, best loss = *0.124*\n",
      "49.000m: train epoch *171*, loss = *0.125*, 49.000m: test epoch *171*, loss = *9.662*, best loss = *0.124*\n",
      "49.000m: train epoch *172*, loss = *0.123*, 49.000m: test epoch *172*, loss = *9.627*, best loss = *0.123*\n",
      "49.000m: train epoch *173*, loss = *0.124*, 49.000m: test epoch *173*, loss = *9.624*, best loss = *0.123*\n",
      "50.000m: train epoch *174*, loss = *0.123*, 50.000m: test epoch *174*, loss = *9.716*, best loss = *0.123*\n",
      "50.000m: train epoch *175*, loss = *0.123*, 50.000m: test epoch *175*, loss = *9.629*, best loss = *0.123*\n",
      "50.000m: train epoch *176*, loss = *0.122*, 50.000m: test epoch *176*, loss = *9.628*, best loss = *0.122*\n",
      "50.000m: train epoch *177*, loss = *0.123*, 50.000m: test epoch *177*, loss = *9.734*, best loss = *0.122*\n",
      "51.000m: train epoch *178*, loss = *0.122*, 51.000m: test epoch *178*, loss = *9.674*, best loss = *0.122*\n",
      "51.000m: train epoch *179*, loss = *0.122*, 51.000m: test epoch *179*, loss = *9.688*, best loss = *0.122*\n",
      "51.000m: train epoch *180*, loss = *0.120*, 51.000m: test epoch *180*, loss = *9.727*, best loss = *0.120*\n",
      "51.000m: train epoch *181*, loss = *0.121*, 51.000m: test epoch *181*, loss = *9.701*, best loss = *0.120*\n",
      "52.000m: train epoch *182*, loss = *0.121*, 52.000m: test epoch *182*, loss = *9.672*, best loss = *0.120*\n",
      "52.000m: train epoch *183*, loss = *0.121*, 52.000m: test epoch *183*, loss = *9.627*, best loss = *0.120*\n",
      "52.000m: train epoch *184*, loss = *0.120*, 52.000m: test epoch *184*, loss = *9.573*, best loss = *0.120*\n",
      "53.000m: train epoch *185*, loss = *0.119*, 53.000m: test epoch *185*, loss = *9.578*, best loss = *0.119*\n",
      "53.000m: train epoch *186*, loss = *0.121*, 53.000m: test epoch *186*, loss = *9.544*, best loss = *0.119*\n",
      "53.000m: train epoch *187*, loss = *0.120*, 53.000m: test epoch *187*, loss = *9.547*, best loss = *0.119*\n",
      "53.000m: train epoch *188*, loss = *0.119*, 53.000m: test epoch *188*, loss = *9.588*, best loss = *0.119*\n",
      "54.000m: train epoch *189*, loss = *0.119*, 54.000m: test epoch *189*, loss = *9.559*, best loss = *0.119*\n",
      "54.000m: train epoch *190*, loss = *0.118*, 54.000m: test epoch *190*, loss = *9.540*, best loss = *0.118*\n",
      "54.000m: train epoch *191*, loss = *0.117*, 54.000m: test epoch *191*, loss = *9.540*, best loss = *0.117*\n",
      "55.000m: train epoch *192*, loss = *0.118*, 55.000m: test epoch *192*, loss = *9.531*, best loss = *0.117*\n",
      "55.000m: train epoch *193*, loss = *0.119*, 55.000m: test epoch *193*, loss = *9.529*, best loss = *0.117*\n",
      "55.000m: train epoch *194*, loss = *0.117*, 55.000m: test epoch *194*, loss = *9.586*, best loss = *0.117*\n",
      "55.000m: train epoch *195*, loss = *0.117*, 55.000m: test epoch *195*, loss = *9.568*, best loss = *0.117*\n",
      "56.000m: train epoch *196*, loss = *0.116*, 56.000m: test epoch *196*, loss = *9.577*, best loss = *0.116*\n",
      "56.000m: train epoch *197*, loss = *0.117*, 56.000m: test epoch *197*, loss = *9.519*, best loss = *0.116*\n",
      "56.000m: train epoch *198*, loss = *0.116*, 56.000m: test epoch *198*, loss = *9.567*, best loss = *0.116*\n",
      "57.000m: train epoch *199*, loss = *0.116*, 57.000m: test epoch *199*, loss = *9.557*, best loss = *0.116*\n",
      "57.000m: train epoch *200*, loss = *0.116*, 57.000m: test epoch *200*, loss = *9.564*, best loss = *0.116*\n"
     ]
    }
   ],
   "source": [
    "#cosine\n",
    "transformer_trainer(t_model, train_data_iter, train_opt, test_data_iter, test_opt, t_optimizer, t_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> a GPU was detected, model will be trained on GPU\n",
      "Epoch 00247: adjusting learning rate to 5.520292819452606e-05.\n",
      "0.000m: train epoch *1*, loss = *0.113*, 0.000m: test epoch *1*, loss = *9.549*, best loss = *0.113*\n",
      "Epoch 00248: adjusting learning rate to 9.85267452585845e-05.\n",
      "0.000m: train epoch *2*, loss = *0.122*, 0.000m: test epoch *2*, loss = *9.400*, best loss = *0.113*\n",
      "Epoch 00249: adjusting learning rate to 0.00014185056232264294.\n",
      "1.000m: train epoch *3*, loss = *0.131*, 1.000m: test epoch *3*, loss = *9.366*, best loss = *0.113*\n",
      "Epoch 00250: adjusting learning rate to 0.00018517437938670136.\n",
      "1.000m: train epoch *4*, loss = *0.137*, 1.000m: test epoch *4*, loss = *9.278*, best loss = *0.113*\n",
      "Epoch 00251: adjusting learning rate to 0.0002284981964507598.\n",
      "1.000m: train epoch *5*, loss = *0.145*, 1.000m: test epoch *5*, loss = *9.284*, best loss = *0.113*\n",
      "Epoch 00252: adjusting learning rate to 0.00027182201351481824.\n",
      "2.000m: train epoch *6*, loss = *0.149*, 2.000m: test epoch *6*, loss = *9.060*, best loss = *0.113*\n",
      "Epoch 00253: adjusting learning rate to 0.00031514583057887664.\n",
      "2.000m: train epoch *7*, loss = *0.154*, 2.000m: test epoch *7*, loss = *9.119*, best loss = *0.113*\n",
      "Epoch 00254: adjusting learning rate to 0.0003584696476429351.\n",
      "2.000m: train epoch *8*, loss = *0.159*, 2.000m: test epoch *8*, loss = *9.130*, best loss = *0.113*\n",
      "Epoch 00255: adjusting learning rate to 0.00040179346470699354.\n",
      "3.000m: train epoch *9*, loss = *0.164*, 3.000m: test epoch *9*, loss = *9.253*, best loss = *0.113*\n",
      "Epoch 00256: adjusting learning rate to 0.000445117281771052.\n",
      "3.000m: train epoch *10*, loss = *0.171*, 3.000m: test epoch *10*, loss = *9.196*, best loss = *0.113*\n",
      "Epoch 00257: adjusting learning rate to 0.0004884410988351104.\n",
      "4.000m: train epoch *11*, loss = *0.175*, 4.000m: test epoch *11*, loss = *9.089*, best loss = *0.113*\n",
      "Epoch 00258: adjusting learning rate to 0.0005317649158991688.\n",
      "4.000m: train epoch *12*, loss = *0.180*, 4.000m: test epoch *12*, loss = *9.200*, best loss = *0.113*\n",
      "Epoch 00259: adjusting learning rate to 0.0005750887329632272.\n",
      "4.000m: train epoch *13*, loss = *0.186*, 4.000m: test epoch *13*, loss = *9.092*, best loss = *0.113*\n",
      "Epoch 00260: adjusting learning rate to 0.0006184125500272857.\n",
      "5.000m: train epoch *14*, loss = *0.194*, 5.000m: test epoch *14*, loss = *9.067*, best loss = *0.113*\n",
      "Epoch 00261: adjusting learning rate to 0.0006617363670913441.\n",
      "5.000m: train epoch *15*, loss = *0.205*, 5.000m: test epoch *15*, loss = *9.268*, best loss = *0.113*\n",
      "Epoch 00262: adjusting learning rate to 0.0006956478396065475.\n",
      "5.000m: train epoch *16*, loss = *0.217*, 5.000m: test epoch *16*, loss = *9.376*, best loss = *0.113*\n",
      "Epoch 00263: adjusting learning rate to 0.0006752122120335046.\n",
      "6.000m: train epoch *17*, loss = *0.220*, 6.000m: test epoch *17*, loss = *9.557*, best loss = *0.113*\n",
      "Epoch 00264: adjusting learning rate to 0.0006564778373461996.\n",
      "6.000m: train epoch *18*, loss = *0.216*, 6.000m: test epoch *18*, loss = *9.522*, best loss = *0.113*\n",
      "Epoch 00265: adjusting learning rate to 0.000639221054475958.\n",
      "6.000m: train epoch *19*, loss = *0.208*, 6.000m: test epoch *19*, loss = *9.620*, best loss = *0.113*\n",
      "Epoch 00266: adjusting learning rate to 0.0006232573158672273.\n",
      "7.000m: train epoch *20*, loss = *0.214*, 7.000m: test epoch *20*, loss = *9.665*, best loss = *0.113*\n",
      "Epoch 00267: adjusting learning rate to 0.0006084328107655543.\n",
      "7.000m: train epoch *21*, loss = *0.207*, 7.000m: test epoch *21*, loss = *9.659*, best loss = *0.113*\n",
      "Epoch 00268: adjusting learning rate to 0.0005946181817284085.\n",
      "8.000m: train epoch *22*, loss = *0.206*, 8.000m: test epoch *22*, loss = *9.583*, best loss = *0.113*\n",
      "Epoch 00269: adjusting learning rate to 0.0005817037430077657.\n",
      "8.000m: train epoch *23*, loss = *0.203*, 8.000m: test epoch *23*, loss = *10.005*, best loss = *0.113*\n",
      "Epoch 00270: adjusting learning rate to 0.0005695957942003822.\n",
      "8.000m: train epoch *24*, loss = *0.200*, 8.000m: test epoch *24*, loss = *9.565*, best loss = *0.113*\n",
      "Epoch 00271: adjusting learning rate to 0.0005582137445069562.\n",
      "9.000m: train epoch *25*, loss = *0.195*, 9.000m: test epoch *25*, loss = *9.726*, best loss = *0.113*\n",
      "Epoch 00272: adjusting learning rate to 0.0005474878450287883.\n",
      "9.000m: train epoch *26*, loss = *0.195*, 9.000m: test epoch *26*, loss = *9.937*, best loss = *0.113*\n",
      "Epoch 00273: adjusting learning rate to 0.0005373573827819601.\n",
      "9.000m: train epoch *27*, loss = *0.191*, 9.000m: test epoch *27*, loss = *9.596*, best loss = *0.113*\n",
      "Epoch 00274: adjusting learning rate to 0.0005277692292886289.\n",
      "10.000m: train epoch *28*, loss = *0.190*, 10.000m: test epoch *28*, loss = *9.785*, best loss = *0.113*\n",
      "Epoch 00275: adjusting learning rate to 0.0005186766643041346.\n",
      "10.000m: train epoch *29*, loss = *0.188*, 10.000m: test epoch *29*, loss = *9.584*, best loss = *0.113*\n",
      "Epoch 00276: adjusting learning rate to 0.000510038415091915.\n",
      "10.000m: train epoch *30*, loss = *0.187*, 10.000m: test epoch *30*, loss = *9.845*, best loss = *0.113*\n",
      "Epoch 00277: adjusting learning rate to 0.0005018178660695032.\n",
      "11.000m: train epoch *31*, loss = *0.183*, 11.000m: test epoch *31*, loss = *9.904*, best loss = *0.113*\n",
      "Epoch 00278: adjusting learning rate to 0.0004939824042333431.\n",
      "11.000m: train epoch *32*, loss = *0.182*, 11.000m: test epoch *32*, loss = *9.638*, best loss = *0.113*\n",
      "Epoch 00279: adjusting learning rate to 0.0004865028736291725.\n",
      "12.000m: train epoch *33*, loss = *0.182*, 12.000m: test epoch *33*, loss = *9.739*, best loss = *0.113*\n",
      "Epoch 00280: adjusting learning rate to 0.00047935311802960656.\n",
      "12.000m: train epoch *34*, loss = *0.181*, 12.000m: test epoch *34*, loss = *9.841*, best loss = *0.113*\n",
      "Epoch 00281: adjusting learning rate to 0.0004725095954442859.\n",
      "12.000m: train epoch *35*, loss = *0.178*, 12.000m: test epoch *35*, loss = *9.771*, best loss = *0.113*\n",
      "Epoch 00282: adjusting learning rate to 0.00046595105149808855.\n",
      "13.000m: train epoch *36*, loss = *0.178*, 13.000m: test epoch *36*, loss = *9.889*, best loss = *0.113*\n",
      "Epoch 00283: adjusting learning rate to 0.00045965824133988357.\n",
      "13.000m: train epoch *37*, loss = *0.176*, 13.000m: test epoch *37*, loss = *9.684*, best loss = *0.113*\n",
      "Epoch 00284: adjusting learning rate to 0.00045361369178386637.\n",
      "13.000m: train epoch *38*, loss = *0.175*, 13.000m: test epoch *38*, loss = *9.636*, best loss = *0.113*\n",
      "Epoch 00285: adjusting learning rate to 0.0004478014969806957.\n",
      "14.000m: train epoch *39*, loss = *0.176*, 14.000m: test epoch *39*, loss = *9.728*, best loss = *0.113*\n",
      "Epoch 00286: adjusting learning rate to 0.0004422071421719752.\n",
      "14.000m: train epoch *40*, loss = *0.173*, 14.000m: test epoch *40*, loss = *9.989*, best loss = *0.113*\n",
      "Epoch 00287: adjusting learning rate to 0.00043681735107756745.\n",
      "14.000m: train epoch *41*, loss = *0.172*, 14.000m: test epoch *41*, loss = *9.668*, best loss = *0.113*\n",
      "Epoch 00288: adjusting learning rate to 0.00043161995325968034.\n",
      "15.000m: train epoch *42*, loss = *0.172*, 15.000m: test epoch *42*, loss = *9.495*, best loss = *0.113*\n",
      "Epoch 00289: adjusting learning rate to 0.00042660376844512763.\n",
      "15.000m: train epoch *43*, loss = *0.171*, 15.000m: test epoch *43*, loss = *9.916*, best loss = *0.113*\n",
      "Epoch 00290: adjusting learning rate to 0.0004217585053015188.\n",
      "16.000m: train epoch *44*, loss = *0.168*, 16.000m: test epoch *44*, loss = *9.733*, best loss = *0.113*\n",
      "Epoch 00291: adjusting learning rate to 0.00041707467258036144.\n",
      "16.000m: train epoch *45*, loss = *0.169*, 16.000m: test epoch *45*, loss = *9.950*, best loss = *0.113*\n",
      "Epoch 00292: adjusting learning rate to 0.0004125435008802202.\n",
      "16.000m: train epoch *46*, loss = *0.170*, 16.000m: test epoch *46*, loss = *9.705*, best loss = *0.113*\n",
      "Epoch 00293: adjusting learning rate to 0.00040815687356173175.\n",
      "17.000m: train epoch *47*, loss = *0.166*, 17.000m: test epoch *47*, loss = *9.869*, best loss = *0.113*\n",
      "Epoch 00294: adjusting learning rate to 0.0004039072655756077.\n",
      "17.000m: train epoch *48*, loss = *0.169*, 17.000m: test epoch *48*, loss = *9.566*, best loss = *0.113*\n",
      "Epoch 00295: adjusting learning rate to 0.0003997876891543127.\n",
      "17.000m: train epoch *49*, loss = *0.166*, 17.000m: test epoch *49*, loss = *9.726*, best loss = *0.113*\n",
      "Epoch 00296: adjusting learning rate to 0.00039579164547545487.\n",
      "18.000m: train epoch *50*, loss = *0.167*, 18.000m: test epoch *50*, loss = *9.736*, best loss = *0.113*\n",
      "Epoch 00297: adjusting learning rate to 0.00039191308153604866.\n",
      "18.000m: train epoch *51*, loss = *0.164*, 18.000m: test epoch *51*, loss = *9.841*, best loss = *0.113*\n",
      "Epoch 00298: adjusting learning rate to 0.00038814635158651546.\n",
      "18.000m: train epoch *52*, loss = *0.164*, 18.000m: test epoch *52*, loss = *9.754*, best loss = *0.113*\n",
      "Epoch 00299: adjusting learning rate to 0.00038448618256539706.\n",
      "19.000m: train epoch *53*, loss = *0.164*, 19.000m: test epoch *53*, loss = *9.794*, best loss = *0.113*\n",
      "Epoch 00300: adjusting learning rate to 0.0003809276430533745.\n",
      "19.000m: train epoch *54*, loss = *0.165*, 19.000m: test epoch *54*, loss = *9.753*, best loss = *0.113*\n",
      "Epoch 00301: adjusting learning rate to 0.0003774661153308142.\n",
      "20.000m: train epoch *55*, loss = *0.162*, 20.000m: test epoch *55*, loss = *9.869*, best loss = *0.113*\n",
      "Epoch 00302: adjusting learning rate to 0.0003740972701787243.\n",
      "20.000m: train epoch *56*, loss = *0.163*, 20.000m: test epoch *56*, loss = *9.904*, best loss = *0.113*\n",
      "Epoch 00303: adjusting learning rate to 0.0003708170441103804.\n",
      "20.000m: train epoch *57*, loss = *0.161*, 20.000m: test epoch *57*, loss = *9.830*, best loss = *0.113*\n",
      "Epoch 00304: adjusting learning rate to 0.00036762161876130315.\n",
      "21.000m: train epoch *58*, loss = *0.162*, 21.000m: test epoch *58*, loss = *9.699*, best loss = *0.113*\n",
      "Epoch 00305: adjusting learning rate to 0.0003645074021998777.\n",
      "21.000m: train epoch *59*, loss = *0.161*, 21.000m: test epoch *59*, loss = *9.822*, best loss = *0.113*\n",
      "Epoch 00306: adjusting learning rate to 0.000361471011950612.\n",
      "21.000m: train epoch *60*, loss = *0.163*, 21.000m: test epoch *60*, loss = *9.765*, best loss = *0.113*\n",
      "Epoch 00307: adjusting learning rate to 0.00035850925954759324.\n",
      "22.000m: train epoch *61*, loss = *0.162*, 22.000m: test epoch *61*, loss = *9.670*, best loss = *0.113*\n",
      "Epoch 00308: adjusting learning rate to 0.0003556191364577703.\n",
      "22.000m: train epoch *62*, loss = *0.160*, 22.000m: test epoch *62*, loss = *9.775*, best loss = *0.113*\n",
      "Epoch 00309: adjusting learning rate to 0.0003527978012327707.\n",
      "22.000m: train epoch *63*, loss = *0.159*, 22.000m: test epoch *63*, loss = *9.867*, best loss = *0.113*\n",
      "Epoch 00310: adjusting learning rate to 0.00035004256776451765.\n",
      "23.000m: train epoch *64*, loss = *0.159*, 23.000m: test epoch *64*, loss = *9.699*, best loss = *0.113*\n",
      "Epoch 00311: adjusting learning rate to 0.0003473508945342996.\n",
      "23.000m: train epoch *65*, loss = *0.160*, 23.000m: test epoch *65*, loss = *9.642*, best loss = *0.113*\n",
      "Epoch 00312: adjusting learning rate to 0.00034472037475748.\n",
      "24.000m: train epoch *66*, loss = *0.160*, 24.000m: test epoch *66*, loss = *9.514*, best loss = *0.113*\n",
      "Epoch 00313: adjusting learning rate to 0.00034214872733698276.\n",
      "24.000m: train epoch *67*, loss = *0.160*, 24.000m: test epoch *67*, loss = *9.760*, best loss = *0.113*\n",
      "Epoch 00314: adjusting learning rate to 0.0003396337885482649.\n",
      "24.000m: train epoch *68*, loss = *0.157*, 24.000m: test epoch *68*, loss = *9.688*, best loss = *0.113*\n",
      "Epoch 00315: adjusting learning rate to 0.0003371735043868891.\n",
      "25.000m: train epoch *69*, loss = *0.160*, 25.000m: test epoch *69*, loss = *9.537*, best loss = *0.113*\n",
      "Epoch 00316: adjusting learning rate to 0.000334765923517184.\n",
      "25.000m: train epoch *70*, loss = *0.158*, 25.000m: test epoch *70*, loss = *9.662*, best loss = *0.113*\n",
      "Epoch 00317: adjusting learning rate to 0.00033240919076698064.\n",
      "25.000m: train epoch *71*, loss = *0.157*, 25.000m: test epoch *71*, loss = *9.570*, best loss = *0.113*\n",
      "Epoch 00318: adjusting learning rate to 0.00033010154111913823.\n",
      "26.000m: train epoch *72*, loss = *0.159*, 26.000m: test epoch *72*, loss = *9.606*, best loss = *0.113*\n",
      "Epoch 00319: adjusting learning rate to 0.0003278412941556371.\n",
      "26.000m: train epoch *73*, loss = *0.158*, 26.000m: test epoch *73*, loss = *9.471*, best loss = *0.113*\n",
      "Epoch 00320: adjusting learning rate to 0.0003256268489144972.\n",
      "26.000m: train epoch *74*, loss = *0.157*, 26.000m: test epoch *74*, loss = *9.559*, best loss = *0.113*\n",
      "Epoch 00321: adjusting learning rate to 0.0003234566791237529.\n",
      "27.000m: train epoch *75*, loss = *0.156*, 27.000m: test epoch *75*, loss = *9.539*, best loss = *0.113*\n",
      "Epoch 00322: adjusting learning rate to 0.0003213293287802445.\n",
      "27.000m: train epoch *76*, loss = *0.155*, 27.000m: test epoch *76*, loss = *9.500*, best loss = *0.113*\n",
      "Epoch 00323: adjusting learning rate to 0.00031924340804412624.\n",
      "28.000m: train epoch *77*, loss = *0.155*, 28.000m: test epoch *77*, loss = *9.496*, best loss = *0.113*\n",
      "Epoch 00324: adjusting learning rate to 0.00031719758942278783.\n",
      "28.000m: train epoch *78*, loss = *0.155*, 28.000m: test epoch *78*, loss = *9.398*, best loss = *0.113*\n",
      "Epoch 00325: adjusting learning rate to 0.00031519060422038246.\n",
      "28.000m: train epoch *79*, loss = *0.155*, 28.000m: test epoch *79*, loss = *9.573*, best loss = *0.113*\n",
      "Epoch 00326: adjusting learning rate to 0.0003132212392313874.\n",
      "29.000m: train epoch *80*, loss = *0.154*, 29.000m: test epoch *80*, loss = *9.492*, best loss = *0.113*\n",
      "Epoch 00327: adjusting learning rate to 0.00031128833365861745.\n",
      "29.000m: train epoch *81*, loss = *0.154*, 29.000m: test epoch *81*, loss = *9.559*, best loss = *0.113*\n",
      "Epoch 00328: adjusting learning rate to 0.0003093907762379058.\n",
      "29.000m: train epoch *82*, loss = *0.154*, 29.000m: test epoch *82*, loss = *9.699*, best loss = *0.113*\n",
      "Epoch 00329: adjusting learning rate to 0.0003075275025532695.\n",
      "30.000m: train epoch *83*, loss = *0.155*, 30.000m: test epoch *83*, loss = *9.440*, best loss = *0.113*\n",
      "Epoch 00330: adjusting learning rate to 0.0003056974925278241.\n",
      "30.000m: train epoch *84*, loss = *0.154*, 30.000m: test epoch *84*, loss = *9.350*, best loss = *0.113*\n",
      "Epoch 00331: adjusting learning rate to 0.0003038997680770112.\n",
      "30.000m: train epoch *85*, loss = *0.154*, 30.000m: test epoch *85*, loss = *9.411*, best loss = *0.113*\n",
      "Epoch 00332: adjusting learning rate to 0.00030213339091187405.\n",
      "31.000m: train epoch *86*, loss = *0.154*, 31.000m: test epoch *86*, loss = *9.388*, best loss = *0.113*\n",
      "Epoch 00333: adjusting learning rate to 0.0003003974604811719.\n",
      "31.000m: train epoch *87*, loss = *0.153*, 31.000m: test epoch *87*, loss = *9.334*, best loss = *0.113*\n",
      "Epoch 00334: adjusting learning rate to 0.0002986911120420831.\n",
      "32.000m: train epoch *88*, loss = *0.153*, 32.000m: test epoch *88*, loss = *9.456*, best loss = *0.113*\n",
      "Epoch 00335: adjusting learning rate to 0.00029701351485010477.\n",
      "32.000m: train epoch *89*, loss = *0.154*, 32.000m: test epoch *89*, loss = *9.321*, best loss = *0.113*\n",
      "Epoch 00336: adjusting learning rate to 0.00029536387045954454.\n",
      "32.000m: train epoch *90*, loss = *0.153*, 32.000m: test epoch *90*, loss = *9.375*, best loss = *0.113*\n",
      "Epoch 00337: adjusting learning rate to 0.00029374141112670617.\n",
      "33.000m: train epoch *91*, loss = *0.152*, 33.000m: test epoch *91*, loss = *9.460*, best loss = *0.113*\n",
      "Epoch 00338: adjusting learning rate to 0.0002921453983085142.\n",
      "33.000m: train epoch *92*, loss = *0.151*, 33.000m: test epoch *92*, loss = *9.402*, best loss = *0.113*\n",
      "Epoch 00339: adjusting learning rate to 0.0002905751212499101.\n",
      "33.000m: train epoch *93*, loss = *0.152*, 33.000m: test epoch *93*, loss = *9.408*, best loss = *0.113*\n",
      "Epoch 00340: adjusting learning rate to 0.00028902989565388174.\n",
      "34.000m: train epoch *94*, loss = *0.152*, 34.000m: test epoch *94*, loss = *9.478*, best loss = *0.113*\n",
      "Epoch 00341: adjusting learning rate to 0.0002875090624284739.\n",
      "34.000m: train epoch *95*, loss = *0.153*, 34.000m: test epoch *95*, loss = *9.334*, best loss = *0.113*\n",
      "Epoch 00342: adjusting learning rate to 0.000286011986505569.\n",
      "35.000m: train epoch *96*, loss = *0.152*, 35.000m: test epoch *96*, loss = *9.397*, best loss = *0.113*\n",
      "Epoch 00343: adjusting learning rate to 0.0002845380557266294.\n",
      "35.000m: train epoch *97*, loss = *0.150*, 35.000m: test epoch *97*, loss = *9.359*, best loss = *0.113*\n",
      "Epoch 00344: adjusting learning rate to 0.00028308667979096044.\n",
      "35.000m: train epoch *98*, loss = *0.150*, 35.000m: test epoch *98*, loss = *9.327*, best loss = *0.113*\n",
      "Epoch 00345: adjusting learning rate to 0.0002816572892623916.\n",
      "36.000m: train epoch *99*, loss = *0.150*, 36.000m: test epoch *99*, loss = *9.489*, best loss = *0.113*\n",
      "Epoch 00346: adjusting learning rate to 0.0002802493346305776.\n",
      "36.000m: train epoch *100*, loss = *0.151*, 36.000m: test epoch *100*, loss = *9.440*, best loss = *0.113*\n",
      "Epoch 00347: adjusting learning rate to 0.0002788622854234067.\n",
      "36.000m: train epoch *101*, loss = *0.150*, 36.000m: test epoch *101*, loss = *9.465*, best loss = *0.113*\n",
      "Epoch 00348: adjusting learning rate to 0.00027749562936725856.\n",
      "37.000m: train epoch *102*, loss = *0.150*, 37.000m: test epoch *102*, loss = *9.326*, best loss = *0.113*\n",
      "Epoch 00349: adjusting learning rate to 0.0002761488715920939.\n",
      "37.000m: train epoch *103*, loss = *0.152*, 37.000m: test epoch *103*, loss = *9.312*, best loss = *0.113*\n",
      "Epoch 00350: adjusting learning rate to 0.00027482153387857364.\n",
      "37.000m: train epoch *104*, loss = *0.151*, 37.000m: test epoch *104*, loss = *9.319*, best loss = *0.113*\n",
      "Epoch 00351: adjusting learning rate to 0.00027351315394460687.\n",
      "38.000m: train epoch *105*, loss = *0.150*, 38.000m: test epoch *105*, loss = *9.363*, best loss = *0.113*\n",
      "Epoch 00352: adjusting learning rate to 0.00027222328476890966.\n",
      "38.000m: train epoch *106*, loss = *0.150*, 38.000m: test epoch *106*, loss = *9.430*, best loss = *0.113*\n",
      "Epoch 00353: adjusting learning rate to 0.0002709514939493275.\n",
      "39.000m: train epoch *107*, loss = *0.149*, 39.000m: test epoch *107*, loss = *9.155*, best loss = *0.113*\n",
      "Epoch 00354: adjusting learning rate to 0.00026969736309382904.\n",
      "39.000m: train epoch *108*, loss = *0.148*, 39.000m: test epoch *108*, loss = *9.278*, best loss = *0.113*\n",
      "Epoch 00355: adjusting learning rate to 0.0002684604872422236.\n",
      "39.000m: train epoch *109*, loss = *0.149*, 39.000m: test epoch *109*, loss = *9.379*, best loss = *0.113*\n",
      "Epoch 00356: adjusting learning rate to 0.000267240474316787.\n",
      "40.000m: train epoch *110*, loss = *0.148*, 40.000m: test epoch *110*, loss = *9.352*, best loss = *0.113*\n",
      "Epoch 00357: adjusting learning rate to 0.00026603694460010304.\n",
      "40.000m: train epoch *111*, loss = *0.149*, 40.000m: test epoch *111*, loss = *9.318*, best loss = *0.113*\n",
      "Epoch 00358: adjusting learning rate to 0.00026484953023854225.\n",
      "40.000m: train epoch *112*, loss = *0.149*, 40.000m: test epoch *112*, loss = *9.292*, best loss = *0.113*\n",
      "Epoch 00359: adjusting learning rate to 0.0002636778747699026.\n",
      "41.000m: train epoch *113*, loss = *0.147*, 41.000m: test epoch *113*, loss = *9.319*, best loss = *0.113*\n",
      "Epoch 00360: adjusting learning rate to 0.00026252163267383524.\n",
      "41.000m: train epoch *114*, loss = *0.149*, 41.000m: test epoch *114*, loss = *9.369*, best loss = *0.113*\n",
      "Epoch 00361: adjusting learning rate to 0.0002613804689437674.\n",
      "41.000m: train epoch *115*, loss = *0.148*, 41.000m: test epoch *115*, loss = *9.251*, best loss = *0.113*\n",
      "Epoch 00362: adjusting learning rate to 0.00026025405867911855.\n",
      "42.000m: train epoch *116*, loss = *0.148*, 42.000m: test epoch *116*, loss = *9.216*, best loss = *0.113*\n",
      "Epoch 00363: adjusting learning rate to 0.000259142086696682.\n",
      "42.000m: train epoch *117*, loss = *0.148*, 42.000m: test epoch *117*, loss = *9.216*, best loss = *0.113*\n",
      "Epoch 00364: adjusting learning rate to 0.0002580442471601169.\n",
      "43.000m: train epoch *118*, loss = *0.147*, 43.000m: test epoch *118*, loss = *9.281*, best loss = *0.113*\n",
      "Epoch 00365: adjusting learning rate to 0.0002569602432265611.\n",
      "43.000m: train epoch *119*, loss = *0.149*, 43.000m: test epoch *119*, loss = *9.256*, best loss = *0.113*\n",
      "Epoch 00366: adjusting learning rate to 0.00025588978670943856.\n",
      "43.000m: train epoch *120*, loss = *0.148*, 43.000m: test epoch *120*, loss = *9.377*, best loss = *0.113*\n",
      "Epoch 00367: adjusting learning rate to 0.0002548325977565905.\n",
      "44.000m: train epoch *121*, loss = *0.147*, 44.000m: test epoch *121*, loss = *9.206*, best loss = *0.113*\n",
      "Epoch 00368: adjusting learning rate to 0.0002537884045429143.\n",
      "44.000m: train epoch *122*, loss = *0.146*, 44.000m: test epoch *122*, loss = *9.178*, best loss = *0.113*\n",
      "Epoch 00369: adjusting learning rate to 0.0002527569429767442.\n",
      "44.000m: train epoch *123*, loss = *0.147*, 44.000m: test epoch *123*, loss = *9.343*, best loss = *0.113*\n",
      "Epoch 00370: adjusting learning rate to 0.0002517379564192525.\n",
      "45.000m: train epoch *124*, loss = *0.148*, 45.000m: test epoch *124*, loss = *9.303*, best loss = *0.113*\n",
      "Epoch 00371: adjusting learning rate to 0.0002507311954161958.\n",
      "45.000m: train epoch *125*, loss = *0.148*, 45.000m: test epoch *125*, loss = *9.107*, best loss = *0.113*\n",
      "Epoch 00372: adjusting learning rate to 0.0002497364174413678.\n",
      "46.000m: train epoch *126*, loss = *0.146*, 46.000m: test epoch *126*, loss = *9.321*, best loss = *0.113*\n",
      "Epoch 00373: adjusting learning rate to 0.000248753386651161.\n",
      "46.000m: train epoch *127*, loss = *0.148*, 46.000m: test epoch *127*, loss = *9.197*, best loss = *0.113*\n",
      "Epoch 00374: adjusting learning rate to 0.00024778187364967277.\n",
      "46.000m: train epoch *128*, loss = *0.147*, 46.000m: test epoch *128*, loss = *9.322*, best loss = *0.113*\n",
      "Epoch 00375: adjusting learning rate to 0.0002468216552638242.\n",
      "47.000m: train epoch *129*, loss = *0.147*, 47.000m: test epoch *129*, loss = *9.254*, best loss = *0.113*\n",
      "Epoch 00376: adjusting learning rate to 0.0002458725143279919.\n",
      "47.000m: train epoch *130*, loss = *0.146*, 47.000m: test epoch *130*, loss = *9.208*, best loss = *0.113*\n",
      "Epoch 00377: adjusting learning rate to 0.00024493423947768034.\n",
      "47.000m: train epoch *131*, loss = *0.146*, 47.000m: test epoch *131*, loss = *9.195*, best loss = *0.113*\n",
      "Epoch 00378: adjusting learning rate to 0.00024400662495178974.\n",
      "48.000m: train epoch *132*, loss = *0.146*, 48.000m: test epoch *132*, loss = *9.167*, best loss = *0.113*\n",
      "Epoch 00379: adjusting learning rate to 0.00024308947040305943.\n",
      "48.000m: train epoch *133*, loss = *0.147*, 48.000m: test epoch *133*, loss = *9.329*, best loss = *0.113*\n",
      "Epoch 00380: adjusting learning rate to 0.00024218258071629018.\n",
      "48.000m: train epoch *134*, loss = *0.146*, 48.000m: test epoch *134*, loss = *9.131*, best loss = *0.113*\n",
      "Epoch 00381: adjusting learning rate to 0.00024128576583397143.\n",
      "49.000m: train epoch *135*, loss = *0.145*, 49.000m: test epoch *135*, loss = *9.161*, best loss = *0.113*\n",
      "Epoch 00382: adjusting learning rate to 0.0002403988405889587.\n",
      "49.000m: train epoch *136*, loss = *0.146*, 49.000m: test epoch *136*, loss = *9.290*, best loss = *0.113*\n",
      "Epoch 00383: adjusting learning rate to 0.0002395216245438676.\n",
      "50.000m: train epoch *137*, loss = *0.145*, 50.000m: test epoch *137*, loss = *9.149*, best loss = *0.113*\n",
      "Epoch 00384: adjusting learning rate to 0.00023865394183686777.\n",
      "50.000m: train epoch *138*, loss = *0.143*, 50.000m: test epoch *138*, loss = *9.269*, best loss = *0.113*\n",
      "Epoch 00385: adjusting learning rate to 0.0002377956210335772.\n",
      "50.000m: train epoch *139*, loss = *0.145*, 50.000m: test epoch *139*, loss = *9.202*, best loss = *0.113*\n",
      "Epoch 00386: adjusting learning rate to 0.00023694649498477445.\n",
      "51.000m: train epoch *140*, loss = *0.145*, 51.000m: test epoch *140*, loss = *9.102*, best loss = *0.113*\n",
      "Epoch 00387: adjusting learning rate to 0.0002361064006896601.\n",
      "51.000m: train epoch *141*, loss = *0.145*, 51.000m: test epoch *141*, loss = *9.079*, best loss = *0.113*\n",
      "Epoch 00388: adjusting learning rate to 0.00023527517916441346.\n",
      "51.000m: train epoch *142*, loss = *0.144*, 51.000m: test epoch *142*, loss = *9.325*, best loss = *0.113*\n",
      "Epoch 00389: adjusting learning rate to 0.00023445267531580402.\n",
      "52.000m: train epoch *143*, loss = *0.145*, 52.000m: test epoch *143*, loss = *9.153*, best loss = *0.113*\n",
      "Epoch 00390: adjusting learning rate to 0.00023363873781962976.\n",
      "52.000m: train epoch *144*, loss = *0.144*, 52.000m: test epoch *144*, loss = *9.168*, best loss = *0.113*\n",
      "Epoch 00391: adjusting learning rate to 0.0002328332190037654.\n",
      "53.000m: train epoch *145*, loss = *0.144*, 53.000m: test epoch *145*, loss = *9.124*, best loss = *0.113*\n",
      "Epoch 00392: adjusting learning rate to 0.00023203597473561583.\n",
      "53.000m: train epoch *146*, loss = *0.146*, 53.000m: test epoch *146*, loss = *9.153*, best loss = *0.113*\n",
      "Epoch 00393: adjusting learning rate to 0.00023124686431377973.\n",
      "53.000m: train epoch *147*, loss = *0.144*, 53.000m: test epoch *147*, loss = *9.053*, best loss = *0.113*\n",
      "Epoch 00394: adjusting learning rate to 0.00023046575036373887.\n",
      "54.000m: train epoch *148*, loss = *0.143*, 54.000m: test epoch *148*, loss = *9.149*, best loss = *0.113*\n",
      "Epoch 00395: adjusting learning rate to 0.0002296924987373969.\n",
      "54.000m: train epoch *149*, loss = *0.145*, 54.000m: test epoch *149*, loss = *9.160*, best loss = *0.113*\n",
      "Epoch 00396: adjusting learning rate to 0.00022892697841630125.\n",
      "54.000m: train epoch *150*, loss = *0.143*, 54.000m: test epoch *150*, loss = *9.089*, best loss = *0.113*\n",
      "Epoch 00397: adjusting learning rate to 0.0002281690614183897.\n",
      "55.000m: train epoch *151*, loss = *0.143*, 55.000m: test epoch *151*, loss = *9.127*, best loss = *0.113*\n",
      "Epoch 00398: adjusting learning rate to 0.00022741862270811064.\n",
      "55.000m: train epoch *152*, loss = *0.145*, 55.000m: test epoch *152*, loss = *9.094*, best loss = *0.113*\n",
      "Epoch 00399: adjusting learning rate to 0.00022667554010977382.\n",
      "55.000m: train epoch *153*, loss = *0.144*, 55.000m: test epoch *153*, loss = *9.118*, best loss = *0.113*\n",
      "Epoch 00400: adjusting learning rate to 0.0002259396942239951.\n",
      "56.000m: train epoch *154*, loss = *0.144*, 56.000m: test epoch *154*, loss = *9.136*, best loss = *0.113*\n",
      "Epoch 00401: adjusting learning rate to 0.00022521096834710622.\n",
      "56.000m: train epoch *155*, loss = *0.143*, 56.000m: test epoch *155*, loss = *9.136*, best loss = *0.113*\n",
      "Epoch 00402: adjusting learning rate to 0.00022448924839340464.\n",
      "57.000m: train epoch *156*, loss = *0.142*, 57.000m: test epoch *156*, loss = *9.214*, best loss = *0.113*\n",
      "Epoch 00403: adjusting learning rate to 0.00022377442282012748.\n",
      "57.000m: train epoch *157*, loss = *0.144*, 57.000m: test epoch *157*, loss = *9.122*, best loss = *0.113*\n",
      "Epoch 00404: adjusting learning rate to 0.0002230663825550362.\n",
      "57.000m: train epoch *158*, loss = *0.144*, 57.000m: test epoch *158*, loss = *9.125*, best loss = *0.113*\n",
      "Epoch 00405: adjusting learning rate to 0.00022236502092650627.\n",
      "58.000m: train epoch *159*, loss = *0.142*, 58.000m: test epoch *159*, loss = *9.228*, best loss = *0.113*\n",
      "Epoch 00406: adjusting learning rate to 0.0002216702335960195.\n",
      "58.000m: train epoch *160*, loss = *0.143*, 58.000m: test epoch *160*, loss = *9.172*, best loss = *0.113*\n",
      "Epoch 00407: adjusting learning rate to 0.00022098191849296195.\n",
      "58.000m: train epoch *161*, loss = *0.142*, 58.000m: test epoch *161*, loss = *9.108*, best loss = *0.113*\n",
      "Epoch 00408: adjusting learning rate to 0.00022029997575163587.\n",
      "59.000m: train epoch *162*, loss = *0.143*, 59.000m: test epoch *162*, loss = *9.284*, best loss = *0.113*\n",
      "Epoch 00409: adjusting learning rate to 0.00021962430765039588.\n",
      "59.000m: train epoch *163*, loss = *0.143*, 59.000m: test epoch *163*, loss = *9.112*, best loss = *0.113*\n",
      "Epoch 00410: adjusting learning rate to 0.00021895481855282664.\n",
      "60.000m: train epoch *164*, loss = *0.143*, 60.000m: test epoch *164*, loss = *9.122*, best loss = *0.113*\n",
      "Epoch 00411: adjusting learning rate to 0.0002182914148508807.\n",
      "60.000m: train epoch *165*, loss = *0.142*, 60.000m: test epoch *165*, loss = *8.997*, best loss = *0.113*\n",
      "Epoch 00412: adjusting learning rate to 0.00021763400490990034.\n",
      "60.000m: train epoch *166*, loss = *0.143*, 60.000m: test epoch *166*, loss = *9.113*, best loss = *0.113*\n",
      "Epoch 00413: adjusting learning rate to 0.0002169824990154494.\n",
      "61.000m: train epoch *167*, loss = *0.142*, 61.000m: test epoch *167*, loss = *9.253*, best loss = *0.113*\n",
      "Epoch 00414: adjusting learning rate to 0.00021633680932188598.\n",
      "61.000m: train epoch *168*, loss = *0.141*, 61.000m: test epoch *168*, loss = *9.144*, best loss = *0.113*\n",
      "Epoch 00415: adjusting learning rate to 0.00021569684980260825.\n",
      "61.000m: train epoch *169*, loss = *0.142*, 61.000m: test epoch *169*, loss = *9.108*, best loss = *0.113*\n",
      "Epoch 00416: adjusting learning rate to 0.0002150625362019099.\n",
      "62.000m: train epoch *170*, loss = *0.142*, 62.000m: test epoch *170*, loss = *9.134*, best loss = *0.113*\n",
      "Epoch 00417: adjusting learning rate to 0.00021443378598838382.\n",
      "62.000m: train epoch *171*, loss = *0.144*, 62.000m: test epoch *171*, loss = *9.072*, best loss = *0.113*\n",
      "Epoch 00418: adjusting learning rate to 0.00021381051830981563.\n",
      "62.000m: train epoch *172*, loss = *0.142*, 62.000m: test epoch *172*, loss = *9.131*, best loss = *0.113*\n",
      "Epoch 00419: adjusting learning rate to 0.00021319265394951099.\n",
      "63.000m: train epoch *173*, loss = *0.142*, 63.000m: test epoch *173*, loss = *9.121*, best loss = *0.113*\n",
      "Epoch 00420: adjusting learning rate to 0.00021258011528400326.\n",
      "63.000m: train epoch *174*, loss = *0.141*, 63.000m: test epoch *174*, loss = *9.198*, best loss = *0.113*\n",
      "Epoch 00421: adjusting learning rate to 0.0002119728262420902.\n",
      "64.000m: train epoch *175*, loss = *0.142*, 64.000m: test epoch *175*, loss = *9.047*, best loss = *0.113*\n",
      "Epoch 00422: adjusting learning rate to 0.0002113707122651504.\n",
      "64.000m: train epoch *176*, loss = *0.141*, 64.000m: test epoch *176*, loss = *9.176*, best loss = *0.113*\n",
      "Epoch 00423: adjusting learning rate to 0.00021077370026869287.\n",
      "64.000m: train epoch *177*, loss = *0.142*, 64.000m: test epoch *177*, loss = *9.103*, best loss = *0.113*\n",
      "Epoch 00424: adjusting learning rate to 0.00021018171860509466.\n",
      "65.000m: train epoch *178*, loss = *0.141*, 65.000m: test epoch *178*, loss = *9.282*, best loss = *0.113*\n",
      "Epoch 00425: adjusting learning rate to 0.00020959469702748278.\n",
      "65.000m: train epoch *179*, loss = *0.142*, 65.000m: test epoch *179*, loss = *9.068*, best loss = *0.113*\n",
      "Epoch 00426: adjusting learning rate to 0.0002090125666547201.\n",
      "65.000m: train epoch *180*, loss = *0.140*, 65.000m: test epoch *180*, loss = *9.197*, best loss = *0.113*\n",
      "Epoch 00427: adjusting learning rate to 0.00020843525993745498.\n",
      "66.000m: train epoch *181*, loss = *0.141*, 66.000m: test epoch *181*, loss = *9.007*, best loss = *0.113*\n",
      "Epoch 00428: adjusting learning rate to 0.00020786271062519637.\n",
      "66.000m: train epoch *182*, loss = *0.142*, 66.000m: test epoch *182*, loss = *9.083*, best loss = *0.113*\n",
      "Epoch 00429: adjusting learning rate to 0.00020729485373437894.\n",
      "67.000m: train epoch *183*, loss = *0.140*, 67.000m: test epoch *183*, loss = *9.116*, best loss = *0.113*\n",
      "Epoch 00430: adjusting learning rate to 0.00020673162551738204.\n",
      "67.000m: train epoch *184*, loss = *0.141*, 67.000m: test epoch *184*, loss = *9.168*, best loss = *0.113*\n",
      "Epoch 00431: adjusting learning rate to 0.0002061729634324702.\n",
      "67.000m: train epoch *185*, loss = *0.139*, 67.000m: test epoch *185*, loss = *9.094*, best loss = *0.113*\n",
      "Epoch 00432: adjusting learning rate to 0.0002056188061146218.\n",
      "68.000m: train epoch *186*, loss = *0.142*, 68.000m: test epoch *186*, loss = *9.079*, best loss = *0.113*\n",
      "Epoch 00433: adjusting learning rate to 0.00020506909334721613.\n",
      "68.000m: train epoch *187*, loss = *0.140*, 68.000m: test epoch *187*, loss = *9.069*, best loss = *0.113*\n",
      "Epoch 00434: adjusting learning rate to 0.0002045237660345484.\n",
      "68.000m: train epoch *188*, loss = *0.141*, 68.000m: test epoch *188*, loss = *9.017*, best loss = *0.113*\n",
      "Epoch 00435: adjusting learning rate to 0.00020398276617514426.\n",
      "69.000m: train epoch *189*, loss = *0.142*, 69.000m: test epoch *189*, loss = *9.012*, best loss = *0.113*\n",
      "Epoch 00436: adjusting learning rate to 0.00020344603683584734.\n",
      "69.000m: train epoch *190*, loss = *0.140*, 69.000m: test epoch *190*, loss = *9.121*, best loss = *0.113*\n",
      "Epoch 00437: adjusting learning rate to 0.0002029135221266521.\n",
      "69.000m: train epoch *191*, loss = *0.141*, 69.000m: test epoch *191*, loss = *9.057*, best loss = *0.113*\n",
      "Epoch 00438: adjusting learning rate to 0.00020238516717625826.\n",
      "70.000m: train epoch *192*, loss = *0.140*, 70.000m: test epoch *192*, loss = *8.960*, best loss = *0.113*\n",
      "Epoch 00439: adjusting learning rate to 0.00020186091810832116.\n",
      "70.000m: train epoch *193*, loss = *0.140*, 70.000m: test epoch *193*, loss = *9.122*, best loss = *0.113*\n",
      "Epoch 00440: adjusting learning rate to 0.0002013407220183758.\n",
      "71.000m: train epoch *194*, loss = *0.140*, 71.000m: test epoch *194*, loss = *8.998*, best loss = *0.113*\n",
      "Epoch 00441: adjusting learning rate to 0.00020082452695141127.\n",
      "71.000m: train epoch *195*, loss = *0.140*, 71.000m: test epoch *195*, loss = *9.058*, best loss = *0.113*\n",
      "Epoch 00442: adjusting learning rate to 0.00020031228188007504.\n",
      "71.000m: train epoch *196*, loss = *0.140*, 71.000m: test epoch *196*, loss = *9.211*, best loss = *0.113*\n",
      "Epoch 00443: adjusting learning rate to 0.00019980393668348473.\n",
      "72.000m: train epoch *197*, loss = *0.141*, 72.000m: test epoch *197*, loss = *9.103*, best loss = *0.113*\n",
      "Epoch 00444: adjusting learning rate to 0.00019929944212662937.\n",
      "72.000m: train epoch *198*, loss = *0.140*, 72.000m: test epoch *198*, loss = *9.137*, best loss = *0.113*\n",
      "Epoch 00445: adjusting learning rate to 0.00019879874984033918.\n",
      "72.000m: train epoch *199*, loss = *0.141*, 72.000m: test epoch *199*, loss = *9.057*, best loss = *0.113*\n",
      "Epoch 00446: adjusting learning rate to 0.00019830181230180645.\n",
      "73.000m: train epoch *200*, loss = *0.139*, 73.000m: test epoch *200*, loss = *9.090*, best loss = *0.113*\n"
     ]
    }
   ],
   "source": [
    "#warmup 0.001\n",
    "transformer_trainer(t_model, train_data_iter, train_opt, test_data_iter, test_opt, t_optimizer, t_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test\n",
    "transformer_trainer(t_model, train_data_iter, train_opt, test_data_iter, test_opt, t_optimizer, t_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> a GPU was detected, model will be trained on GPU\n",
      "0.000m: train epoch *1*, loss = *2.784*, 0.000m: test epoch *1*, loss = *4.599*, best loss = *2.784*\n",
      "0.000m: train epoch *2*, loss = *2.786*, 0.000m: test epoch *2*, loss = *4.585*, best loss = *2.784*\n",
      "0.000m: train epoch *3*, loss = *2.770*, 0.000m: test epoch *3*, loss = *4.618*, best loss = *2.770*\n",
      "0.000m: train epoch *4*, loss = *2.798*, 0.000m: test epoch *4*, loss = *4.620*, best loss = *2.770*\n",
      "1.000m: train epoch *5*, loss = *2.779*, 1.000m: test epoch *5*, loss = *4.604*, best loss = *2.770*\n",
      "1.000m: train epoch *6*, loss = *2.779*, 1.000m: test epoch *6*, loss = *4.571*, best loss = *2.770*\n",
      "Epoch     7: reducing learning rate of group 0 to 3.0902e-04.\n",
      "1.000m: train epoch *7*, loss = *2.780*, 1.000m: test epoch *7*, loss = *4.596*, best loss = *2.770*\n",
      "1.000m: train epoch *8*, loss = *2.769*, 1.000m: test epoch *8*, loss = *4.633*, best loss = *2.769*\n",
      "1.000m: train epoch *9*, loss = *2.777*, 1.000m: test epoch *9*, loss = *4.619*, best loss = *2.769*\n",
      "2.000m: train epoch *10*, loss = *2.781*, 2.000m: test epoch *10*, loss = *4.631*, best loss = *2.769*\n",
      "2.000m: train epoch *11*, loss = *2.775*, 2.000m: test epoch *11*, loss = *4.629*, best loss = *2.769*\n",
      "Epoch    12: reducing learning rate of group 0 to 2.7812e-04.\n",
      "2.000m: train epoch *12*, loss = *2.784*, 2.000m: test epoch *12*, loss = *4.624*, best loss = *2.769*\n",
      "2.000m: train epoch *13*, loss = *2.744*, 2.000m: test epoch *13*, loss = *4.689*, best loss = *2.744*\n",
      "2.000m: train epoch *14*, loss = *2.784*, 2.000m: test epoch *14*, loss = *4.682*, best loss = *2.744*\n",
      "3.000m: train epoch *15*, loss = *2.773*, 3.000m: test epoch *15*, loss = *4.676*, best loss = *2.744*\n",
      "3.000m: train epoch *16*, loss = *2.786*, 3.000m: test epoch *16*, loss = *4.615*, best loss = *2.744*\n",
      "Epoch    17: reducing learning rate of group 0 to 2.5031e-04.\n",
      "3.000m: train epoch *17*, loss = *2.762*, 3.000m: test epoch *17*, loss = *4.630*, best loss = *2.744*\n",
      "3.000m: train epoch *18*, loss = *2.778*, 3.000m: test epoch *18*, loss = *4.614*, best loss = *2.744*\n",
      "3.000m: train epoch *19*, loss = *2.767*, 3.000m: test epoch *19*, loss = *4.645*, best loss = *2.744*\n",
      "4.000m: train epoch *20*, loss = *2.776*, 4.000m: test epoch *20*, loss = *4.684*, best loss = *2.744*\n",
      "Epoch    21: reducing learning rate of group 0 to 2.2528e-04.\n",
      "4.000m: train epoch *21*, loss = *2.779*, 4.000m: test epoch *21*, loss = *4.666*, best loss = *2.744*\n",
      "4.000m: train epoch *22*, loss = *2.786*, 4.000m: test epoch *22*, loss = *4.685*, best loss = *2.744*\n",
      "4.000m: train epoch *23*, loss = *2.781*, 4.000m: test epoch *23*, loss = *4.696*, best loss = *2.744*\n",
      "4.000m: train epoch *24*, loss = *2.776*, 4.000m: test epoch *24*, loss = *4.651*, best loss = *2.744*\n",
      "Epoch    25: reducing learning rate of group 0 to 2.0275e-04.\n",
      "5.000m: train epoch *25*, loss = *2.776*, 5.000m: test epoch *25*, loss = *4.659*, best loss = *2.744*\n",
      "5.000m: train epoch *26*, loss = *2.782*, 5.000m: test epoch *26*, loss = *4.637*, best loss = *2.744*\n",
      "5.000m: train epoch *27*, loss = *2.769*, 5.000m: test epoch *27*, loss = *4.659*, best loss = *2.744*\n",
      "5.000m: train epoch *28*, loss = *2.769*, 5.000m: test epoch *28*, loss = *4.652*, best loss = *2.744*\n",
      "Epoch    29: reducing learning rate of group 0 to 1.8248e-04.\n",
      "5.000m: train epoch *29*, loss = *2.781*, 5.000m: test epoch *29*, loss = *4.610*, best loss = *2.744*\n",
      "6.000m: train epoch *30*, loss = *2.780*, 6.000m: test epoch *30*, loss = *4.633*, best loss = *2.744*\n",
      "6.000m: train epoch *31*, loss = *2.760*, 6.000m: test epoch *31*, loss = *4.636*, best loss = *2.744*\n",
      "6.000m: train epoch *32*, loss = *2.762*, 6.000m: test epoch *32*, loss = *4.637*, best loss = *2.744*\n",
      "Epoch    33: reducing learning rate of group 0 to 1.6423e-04.\n",
      "6.000m: train epoch *33*, loss = *2.766*, 6.000m: test epoch *33*, loss = *4.633*, best loss = *2.744*\n",
      "6.000m: train epoch *34*, loss = *2.752*, 6.000m: test epoch *34*, loss = *4.657*, best loss = *2.744*\n",
      "7.000m: train epoch *35*, loss = *2.763*, 7.000m: test epoch *35*, loss = *4.651*, best loss = *2.744*\n",
      "7.000m: train epoch *36*, loss = *2.763*, 7.000m: test epoch *36*, loss = *4.655*, best loss = *2.744*\n",
      "Epoch    37: reducing learning rate of group 0 to 1.4781e-04.\n",
      "7.000m: train epoch *37*, loss = *2.775*, 7.000m: test epoch *37*, loss = *4.637*, best loss = *2.744*\n",
      "7.000m: train epoch *38*, loss = *2.775*, 7.000m: test epoch *38*, loss = *4.649*, best loss = *2.744*\n",
      "7.000m: train epoch *39*, loss = *2.768*, 7.000m: test epoch *39*, loss = *4.663*, best loss = *2.744*\n",
      "8.000m: train epoch *40*, loss = *2.770*, 8.000m: test epoch *40*, loss = *4.655*, best loss = *2.744*\n",
      "Epoch    41: reducing learning rate of group 0 to 1.3302e-04.\n",
      "8.000m: train epoch *41*, loss = *2.755*, 8.000m: test epoch *41*, loss = *4.664*, best loss = *2.744*\n",
      "8.000m: train epoch *42*, loss = *2.764*, 8.000m: test epoch *42*, loss = *4.656*, best loss = *2.744*\n",
      "8.000m: train epoch *43*, loss = *2.775*, 8.000m: test epoch *43*, loss = *4.681*, best loss = *2.744*\n",
      "8.000m: train epoch *44*, loss = *2.758*, 8.000m: test epoch *44*, loss = *4.682*, best loss = *2.744*\n",
      "Epoch    45: reducing learning rate of group 0 to 1.1972e-04.\n",
      "9.000m: train epoch *45*, loss = *2.776*, 9.000m: test epoch *45*, loss = *4.678*, best loss = *2.744*\n",
      "9.000m: train epoch *46*, loss = *2.785*, 9.000m: test epoch *46*, loss = *4.661*, best loss = *2.744*\n",
      "9.000m: train epoch *47*, loss = *2.767*, 9.000m: test epoch *47*, loss = *4.672*, best loss = *2.744*\n",
      "9.000m: train epoch *48*, loss = *2.757*, 9.000m: test epoch *48*, loss = *4.673*, best loss = *2.744*\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0775e-04.\n",
      "9.000m: train epoch *49*, loss = *2.768*, 9.000m: test epoch *49*, loss = *4.698*, best loss = *2.744*\n",
      "10.000m: train epoch *50*, loss = *2.771*, 10.000m: test epoch *50*, loss = *4.698*, best loss = *2.744*\n",
      "10.000m: train epoch *51*, loss = *2.753*, 10.000m: test epoch *51*, loss = *4.700*, best loss = *2.744*\n",
      "10.000m: train epoch *52*, loss = *2.748*, 10.000m: test epoch *52*, loss = *4.706*, best loss = *2.744*\n",
      "Epoch    53: reducing learning rate of group 0 to 9.6975e-05.\n",
      "10.000m: train epoch *53*, loss = *2.766*, 10.000m: test epoch *53*, loss = *4.702*, best loss = *2.744*\n",
      "10.000m: train epoch *54*, loss = *2.763*, 10.000m: test epoch *54*, loss = *4.704*, best loss = *2.744*\n",
      "11.000m: train epoch *55*, loss = *2.758*, 11.000m: test epoch *55*, loss = *4.708*, best loss = *2.744*\n",
      "11.000m: train epoch *56*, loss = *2.757*, 11.000m: test epoch *56*, loss = *4.713*, best loss = *2.744*\n",
      "Epoch    57: reducing learning rate of group 0 to 8.7278e-05.\n",
      "11.000m: train epoch *57*, loss = *2.769*, 11.000m: test epoch *57*, loss = *4.711*, best loss = *2.744*\n",
      "11.000m: train epoch *58*, loss = *2.773*, 11.000m: test epoch *58*, loss = *4.692*, best loss = *2.744*\n",
      "12.000m: train epoch *59*, loss = *2.753*, 12.000m: test epoch *59*, loss = *4.697*, best loss = *2.744*\n",
      "12.000m: train epoch *60*, loss = *2.755*, 12.000m: test epoch *60*, loss = *4.708*, best loss = *2.744*\n",
      "Epoch    61: reducing learning rate of group 0 to 7.8550e-05.\n",
      "12.000m: train epoch *61*, loss = *2.755*, 12.000m: test epoch *61*, loss = *4.707*, best loss = *2.744*\n",
      "12.000m: train epoch *62*, loss = *2.760*, 12.000m: test epoch *62*, loss = *4.694*, best loss = *2.744*\n",
      "12.000m: train epoch *63*, loss = *2.751*, 12.000m: test epoch *63*, loss = *4.683*, best loss = *2.744*\n",
      "13.000m: train epoch *64*, loss = *2.771*, 13.000m: test epoch *64*, loss = *4.668*, best loss = *2.744*\n",
      "Epoch    65: reducing learning rate of group 0 to 7.0695e-05.\n",
      "13.000m: train epoch *65*, loss = *2.765*, 13.000m: test epoch *65*, loss = *4.669*, best loss = *2.744*\n",
      "13.000m: train epoch *66*, loss = *2.769*, 13.000m: test epoch *66*, loss = *4.660*, best loss = *2.744*\n",
      "13.000m: train epoch *67*, loss = *2.747*, 13.000m: test epoch *67*, loss = *4.667*, best loss = *2.744*\n",
      "13.000m: train epoch *68*, loss = *2.766*, 13.000m: test epoch *68*, loss = *4.657*, best loss = *2.744*\n",
      "Epoch    69: reducing learning rate of group 0 to 6.3625e-05.\n",
      "14.000m: train epoch *69*, loss = *2.758*, 14.000m: test epoch *69*, loss = *4.648*, best loss = *2.744*\n",
      "14.000m: train epoch *70*, loss = *2.761*, 14.000m: test epoch *70*, loss = *4.654*, best loss = *2.744*\n",
      "14.000m: train epoch *71*, loss = *2.752*, 14.000m: test epoch *71*, loss = *4.655*, best loss = *2.744*\n",
      "14.000m: train epoch *72*, loss = *2.753*, 14.000m: test epoch *72*, loss = *4.668*, best loss = *2.744*\n",
      "Epoch    73: reducing learning rate of group 0 to 5.7263e-05.\n",
      "14.000m: train epoch *73*, loss = *2.766*, 14.000m: test epoch *73*, loss = *4.666*, best loss = *2.744*\n",
      "15.000m: train epoch *74*, loss = *2.753*, 15.000m: test epoch *74*, loss = *4.667*, best loss = *2.744*\n",
      "15.000m: train epoch *75*, loss = *2.763*, 15.000m: test epoch *75*, loss = *4.676*, best loss = *2.744*\n",
      "15.000m: train epoch *76*, loss = *2.762*, 15.000m: test epoch *76*, loss = *4.682*, best loss = *2.744*\n",
      "Epoch    77: reducing learning rate of group 0 to 5.1537e-05.\n",
      "15.000m: train epoch *77*, loss = *2.763*, 15.000m: test epoch *77*, loss = *4.683*, best loss = *2.744*\n",
      "15.000m: train epoch *78*, loss = *2.761*, 15.000m: test epoch *78*, loss = *4.681*, best loss = *2.744*\n",
      "16.000m: train epoch *79*, loss = *2.760*, 16.000m: test epoch *79*, loss = *4.679*, best loss = *2.744*\n",
      "16.000m: train epoch *80*, loss = *2.794*, 16.000m: test epoch *80*, loss = *4.671*, best loss = *2.744*\n",
      "Epoch    81: reducing learning rate of group 0 to 4.6383e-05.\n",
      "16.000m: train epoch *81*, loss = *2.764*, 16.000m: test epoch *81*, loss = *4.667*, best loss = *2.744*\n",
      "16.000m: train epoch *82*, loss = *2.732*, 16.000m: test epoch *82*, loss = *4.672*, best loss = *2.732*\n",
      "16.000m: train epoch *83*, loss = *2.754*, 16.000m: test epoch *83*, loss = *4.671*, best loss = *2.732*\n",
      "17.000m: train epoch *84*, loss = *2.759*, 17.000m: test epoch *84*, loss = *4.672*, best loss = *2.732*\n",
      "17.000m: train epoch *85*, loss = *2.772*, 17.000m: test epoch *85*, loss = *4.673*, best loss = *2.732*\n",
      "Epoch    86: reducing learning rate of group 0 to 4.1745e-05.\n",
      "17.000m: train epoch *86*, loss = *2.773*, 17.000m: test epoch *86*, loss = *4.670*, best loss = *2.732*\n",
      "17.000m: train epoch *87*, loss = *2.759*, 17.000m: test epoch *87*, loss = *4.672*, best loss = *2.732*\n",
      "17.000m: train epoch *88*, loss = *2.770*, 17.000m: test epoch *88*, loss = *4.661*, best loss = *2.732*\n",
      "18.000m: train epoch *89*, loss = *2.766*, 18.000m: test epoch *89*, loss = *4.657*, best loss = *2.732*\n",
      "Epoch    90: reducing learning rate of group 0 to 3.7570e-05.\n",
      "18.000m: train epoch *90*, loss = *2.763*, 18.000m: test epoch *90*, loss = *4.657*, best loss = *2.732*\n",
      "18.000m: train epoch *91*, loss = *2.765*, 18.000m: test epoch *91*, loss = *4.660*, best loss = *2.732*\n",
      "18.000m: train epoch *92*, loss = *2.754*, 18.000m: test epoch *92*, loss = *4.669*, best loss = *2.732*\n",
      "18.000m: train epoch *93*, loss = *2.770*, 18.000m: test epoch *93*, loss = *4.668*, best loss = *2.732*\n",
      "Epoch    94: reducing learning rate of group 0 to 3.3813e-05.\n",
      "19.000m: train epoch *94*, loss = *2.775*, 19.000m: test epoch *94*, loss = *4.673*, best loss = *2.732*\n",
      "19.000m: train epoch *95*, loss = *2.754*, 19.000m: test epoch *95*, loss = *4.678*, best loss = *2.732*\n",
      "19.000m: train epoch *96*, loss = *2.759*, 19.000m: test epoch *96*, loss = *4.681*, best loss = *2.732*\n",
      "19.000m: train epoch *97*, loss = *2.754*, 19.000m: test epoch *97*, loss = *4.679*, best loss = *2.732*\n",
      "Epoch    98: reducing learning rate of group 0 to 3.0432e-05.\n",
      "19.000m: train epoch *98*, loss = *2.750*, 19.000m: test epoch *98*, loss = *4.688*, best loss = *2.732*\n",
      "20.000m: train epoch *99*, loss = *2.748*, 20.000m: test epoch *99*, loss = *4.690*, best loss = *2.732*\n",
      "20.000m: train epoch *100*, loss = *2.769*, 20.000m: test epoch *100*, loss = *4.684*, best loss = *2.732*\n",
      "20.000m: train epoch *101*, loss = *2.756*, 20.000m: test epoch *101*, loss = *4.681*, best loss = *2.732*\n",
      "Epoch   102: reducing learning rate of group 0 to 2.7389e-05.\n",
      "20.000m: train epoch *102*, loss = *2.754*, 20.000m: test epoch *102*, loss = *4.681*, best loss = *2.732*\n",
      "20.000m: train epoch *103*, loss = *2.769*, 20.000m: test epoch *103*, loss = *4.680*, best loss = *2.732*\n",
      "21.000m: train epoch *104*, loss = *2.762*, 21.000m: test epoch *104*, loss = *4.682*, best loss = *2.732*\n",
      "21.000m: train epoch *105*, loss = *2.767*, 21.000m: test epoch *105*, loss = *4.674*, best loss = *2.732*\n",
      "Epoch   106: reducing learning rate of group 0 to 2.4650e-05.\n",
      "21.000m: train epoch *106*, loss = *2.746*, 21.000m: test epoch *106*, loss = *4.679*, best loss = *2.732*\n",
      "21.000m: train epoch *107*, loss = *2.770*, 21.000m: test epoch *107*, loss = *4.677*, best loss = *2.732*\n",
      "21.000m: train epoch *108*, loss = *2.747*, 21.000m: test epoch *108*, loss = *4.680*, best loss = *2.732*\n",
      "22.000m: train epoch *109*, loss = *2.766*, 22.000m: test epoch *109*, loss = *4.681*, best loss = *2.732*\n",
      "Epoch   110: reducing learning rate of group 0 to 2.2185e-05.\n",
      "22.000m: train epoch *110*, loss = *2.768*, 22.000m: test epoch *110*, loss = *4.680*, best loss = *2.732*\n",
      "22.000m: train epoch *111*, loss = *2.771*, 22.000m: test epoch *111*, loss = *4.674*, best loss = *2.732*\n",
      "22.000m: train epoch *112*, loss = *2.755*, 22.000m: test epoch *112*, loss = *4.676*, best loss = *2.732*\n",
      "22.000m: train epoch *113*, loss = *2.749*, 22.000m: test epoch *113*, loss = *4.678*, best loss = *2.732*\n",
      "Epoch   114: reducing learning rate of group 0 to 1.9966e-05.\n",
      "23.000m: train epoch *114*, loss = *2.764*, 23.000m: test epoch *114*, loss = *4.675*, best loss = *2.732*\n",
      "23.000m: train epoch *115*, loss = *2.756*, 23.000m: test epoch *115*, loss = *4.678*, best loss = *2.732*\n",
      "23.000m: train epoch *116*, loss = *2.744*, 23.000m: test epoch *116*, loss = *4.680*, best loss = *2.732*\n",
      "23.000m: train epoch *117*, loss = *2.752*, 23.000m: test epoch *117*, loss = *4.680*, best loss = *2.732*\n",
      "Epoch   118: reducing learning rate of group 0 to 1.7970e-05.\n",
      "23.000m: train epoch *118*, loss = *2.752*, 23.000m: test epoch *118*, loss = *4.684*, best loss = *2.732*\n",
      "24.000m: train epoch *119*, loss = *2.763*, 24.000m: test epoch *119*, loss = *4.683*, best loss = *2.732*\n",
      "24.000m: train epoch *120*, loss = *2.771*, 24.000m: test epoch *120*, loss = *4.681*, best loss = *2.732*\n",
      "24.000m: train epoch *121*, loss = *2.771*, 24.000m: test epoch *121*, loss = *4.675*, best loss = *2.732*\n",
      "Epoch   122: reducing learning rate of group 0 to 1.6173e-05.\n",
      "24.000m: train epoch *122*, loss = *2.768*, 24.000m: test epoch *122*, loss = *4.674*, best loss = *2.732*\n",
      "24.000m: train epoch *123*, loss = *2.776*, 24.000m: test epoch *123*, loss = *4.674*, best loss = *2.732*\n",
      "25.000m: train epoch *124*, loss = *2.767*, 25.000m: test epoch *124*, loss = *4.672*, best loss = *2.732*\n",
      "25.000m: train epoch *125*, loss = *2.752*, 25.000m: test epoch *125*, loss = *4.674*, best loss = *2.732*\n",
      "Epoch   126: reducing learning rate of group 0 to 1.4555e-05.\n",
      "25.000m: train epoch *126*, loss = *2.737*, 25.000m: test epoch *126*, loss = *4.675*, best loss = *2.732*\n",
      "25.000m: train epoch *127*, loss = *2.753*, 25.000m: test epoch *127*, loss = *4.676*, best loss = *2.732*\n",
      "26.000m: train epoch *128*, loss = *2.757*, 26.000m: test epoch *128*, loss = *4.682*, best loss = *2.732*\n",
      "26.000m: train epoch *129*, loss = *2.774*, 26.000m: test epoch *129*, loss = *4.680*, best loss = *2.732*\n",
      "Epoch   130: reducing learning rate of group 0 to 1.3100e-05.\n",
      "26.000m: train epoch *130*, loss = *2.754*, 26.000m: test epoch *130*, loss = *4.678*, best loss = *2.732*\n",
      "26.000m: train epoch *131*, loss = *2.754*, 26.000m: test epoch *131*, loss = *4.678*, best loss = *2.732*\n",
      "26.000m: train epoch *132*, loss = *2.760*, 26.000m: test epoch *132*, loss = *4.676*, best loss = *2.732*\n",
      "27.000m: train epoch *133*, loss = *2.744*, 27.000m: test epoch *133*, loss = *4.680*, best loss = *2.732*\n",
      "Epoch   134: reducing learning rate of group 0 to 1.1790e-05.\n",
      "27.000m: train epoch *134*, loss = *2.763*, 27.000m: test epoch *134*, loss = *4.679*, best loss = *2.732*\n",
      "27.000m: train epoch *135*, loss = *2.752*, 27.000m: test epoch *135*, loss = *4.681*, best loss = *2.732*\n",
      "27.000m: train epoch *136*, loss = *2.745*, 27.000m: test epoch *136*, loss = *4.679*, best loss = *2.732*\n",
      "27.000m: train epoch *137*, loss = *2.755*, 27.000m: test epoch *137*, loss = *4.677*, best loss = *2.732*\n",
      "Epoch   138: reducing learning rate of group 0 to 1.0611e-05.\n",
      "28.000m: train epoch *138*, loss = *2.757*, 28.000m: test epoch *138*, loss = *4.677*, best loss = *2.732*\n",
      "28.000m: train epoch *139*, loss = *2.768*, 28.000m: test epoch *139*, loss = *4.677*, best loss = *2.732*\n",
      "28.000m: train epoch *140*, loss = *2.736*, 28.000m: test epoch *140*, loss = *4.677*, best loss = *2.732*\n",
      "28.000m: train epoch *141*, loss = *2.755*, 28.000m: test epoch *141*, loss = *4.676*, best loss = *2.732*\n",
      "Epoch   142: reducing learning rate of group 0 to 9.5498e-06.\n",
      "28.000m: train epoch *142*, loss = *2.746*, 28.000m: test epoch *142*, loss = *4.679*, best loss = *2.732*\n",
      "29.000m: train epoch *143*, loss = *2.742*, 29.000m: test epoch *143*, loss = *4.680*, best loss = *2.732*\n",
      "29.000m: train epoch *144*, loss = *2.748*, 29.000m: test epoch *144*, loss = *4.680*, best loss = *2.732*\n",
      "29.000m: train epoch *145*, loss = *2.751*, 29.000m: test epoch *145*, loss = *4.679*, best loss = *2.732*\n",
      "Epoch   146: reducing learning rate of group 0 to 8.5948e-06.\n",
      "29.000m: train epoch *146*, loss = *2.762*, 29.000m: test epoch *146*, loss = *4.681*, best loss = *2.732*\n",
      "29.000m: train epoch *147*, loss = *2.762*, 29.000m: test epoch *147*, loss = *4.678*, best loss = *2.732*\n",
      "30.000m: train epoch *148*, loss = *2.752*, 30.000m: test epoch *148*, loss = *4.679*, best loss = *2.732*\n",
      "30.000m: train epoch *149*, loss = *2.753*, 30.000m: test epoch *149*, loss = *4.680*, best loss = *2.732*\n",
      "Epoch   150: reducing learning rate of group 0 to 7.7354e-06.\n",
      "30.000m: train epoch *150*, loss = *2.753*, 30.000m: test epoch *150*, loss = *4.680*, best loss = *2.732*\n",
      "30.000m: train epoch *151*, loss = *2.746*, 30.000m: test epoch *151*, loss = *4.681*, best loss = *2.732*\n",
      "30.000m: train epoch *152*, loss = *2.766*, 30.000m: test epoch *152*, loss = *4.681*, best loss = *2.732*\n",
      "31.000m: train epoch *153*, loss = *2.737*, 31.000m: test epoch *153*, loss = *4.682*, best loss = *2.732*\n",
      "Epoch   154: reducing learning rate of group 0 to 6.9618e-06.\n",
      "31.000m: train epoch *154*, loss = *2.749*, 31.000m: test epoch *154*, loss = *4.682*, best loss = *2.732*\n",
      "31.000m: train epoch *155*, loss = *2.752*, 31.000m: test epoch *155*, loss = *4.682*, best loss = *2.732*\n",
      "31.000m: train epoch *156*, loss = *2.749*, 31.000m: test epoch *156*, loss = *4.683*, best loss = *2.732*\n",
      "31.000m: train epoch *157*, loss = *2.740*, 31.000m: test epoch *157*, loss = *4.684*, best loss = *2.732*\n",
      "Epoch   158: reducing learning rate of group 0 to 6.2656e-06.\n",
      "32.000m: train epoch *158*, loss = *2.780*, 32.000m: test epoch *158*, loss = *4.684*, best loss = *2.732*\n",
      "32.000m: train epoch *159*, loss = *2.744*, 32.000m: test epoch *159*, loss = *4.683*, best loss = *2.732*\n",
      "32.000m: train epoch *160*, loss = *2.777*, 32.000m: test epoch *160*, loss = *4.682*, best loss = *2.732*\n",
      "32.000m: train epoch *161*, loss = *2.749*, 32.000m: test epoch *161*, loss = *4.683*, best loss = *2.732*\n",
      "Epoch   162: reducing learning rate of group 0 to 5.6391e-06.\n",
      "32.000m: train epoch *162*, loss = *2.759*, 32.000m: test epoch *162*, loss = *4.682*, best loss = *2.732*\n",
      "33.000m: train epoch *163*, loss = *2.768*, 33.000m: test epoch *163*, loss = *4.683*, best loss = *2.732*\n",
      "33.000m: train epoch *164*, loss = *2.748*, 33.000m: test epoch *164*, loss = *4.683*, best loss = *2.732*\n",
      "33.000m: train epoch *165*, loss = *2.754*, 33.000m: test epoch *165*, loss = *4.684*, best loss = *2.732*\n",
      "Epoch   166: reducing learning rate of group 0 to 5.0752e-06.\n",
      "33.000m: train epoch *166*, loss = *2.753*, 33.000m: test epoch *166*, loss = *4.684*, best loss = *2.732*\n",
      "33.000m: train epoch *167*, loss = *2.766*, 33.000m: test epoch *167*, loss = *4.682*, best loss = *2.732*\n",
      "34.000m: train epoch *168*, loss = *2.761*, 34.000m: test epoch *168*, loss = *4.682*, best loss = *2.732*\n",
      "34.000m: train epoch *169*, loss = *2.752*, 34.000m: test epoch *169*, loss = *4.682*, best loss = *2.732*\n",
      "Epoch   170: reducing learning rate of group 0 to 4.5677e-06.\n",
      "34.000m: train epoch *170*, loss = *2.762*, 34.000m: test epoch *170*, loss = *4.682*, best loss = *2.732*\n",
      "34.000m: train epoch *171*, loss = *2.746*, 34.000m: test epoch *171*, loss = *4.683*, best loss = *2.732*\n",
      "34.000m: train epoch *172*, loss = *2.756*, 34.000m: test epoch *172*, loss = *4.684*, best loss = *2.732*\n",
      "35.000m: train epoch *173*, loss = *2.757*, 35.000m: test epoch *173*, loss = *4.683*, best loss = *2.732*\n",
      "Epoch   174: reducing learning rate of group 0 to 4.1109e-06.\n",
      "35.000m: train epoch *174*, loss = *2.754*, 35.000m: test epoch *174*, loss = *4.683*, best loss = *2.732*\n",
      "35.000m: train epoch *175*, loss = *2.745*, 35.000m: test epoch *175*, loss = *4.683*, best loss = *2.732*\n",
      "35.000m: train epoch *176*, loss = *2.740*, 35.000m: test epoch *176*, loss = *4.683*, best loss = *2.732*\n",
      "35.000m: train epoch *177*, loss = *2.760*, 35.000m: test epoch *177*, loss = *4.683*, best loss = *2.732*\n",
      "Epoch   178: reducing learning rate of group 0 to 3.6998e-06.\n",
      "36.000m: train epoch *178*, loss = *2.764*, 36.000m: test epoch *178*, loss = *4.682*, best loss = *2.732*\n",
      "36.000m: train epoch *179*, loss = *2.762*, 36.000m: test epoch *179*, loss = *4.683*, best loss = *2.732*\n",
      "36.000m: train epoch *180*, loss = *2.740*, 36.000m: test epoch *180*, loss = *4.683*, best loss = *2.732*\n",
      "36.000m: train epoch *181*, loss = *2.763*, 36.000m: test epoch *181*, loss = *4.683*, best loss = *2.732*\n",
      "Epoch   182: reducing learning rate of group 0 to 3.3298e-06.\n",
      "36.000m: train epoch *182*, loss = *2.761*, 36.000m: test epoch *182*, loss = *4.683*, best loss = *2.732*\n",
      "37.000m: train epoch *183*, loss = *2.759*, 37.000m: test epoch *183*, loss = *4.682*, best loss = *2.732*\n",
      "37.000m: train epoch *184*, loss = *2.764*, 37.000m: test epoch *184*, loss = *4.682*, best loss = *2.732*\n",
      "37.000m: train epoch *185*, loss = *2.769*, 37.000m: test epoch *185*, loss = *4.682*, best loss = *2.732*\n",
      "Epoch   186: reducing learning rate of group 0 to 2.9968e-06.\n",
      "37.000m: train epoch *186*, loss = *2.755*, 37.000m: test epoch *186*, loss = *4.683*, best loss = *2.732*\n",
      "38.000m: train epoch *187*, loss = *2.736*, 38.000m: test epoch *187*, loss = *4.684*, best loss = *2.732*\n",
      "38.000m: train epoch *188*, loss = *2.752*, 38.000m: test epoch *188*, loss = *4.684*, best loss = *2.732*\n",
      "38.000m: train epoch *189*, loss = *2.738*, 38.000m: test epoch *189*, loss = *4.684*, best loss = *2.732*\n",
      "Epoch   190: reducing learning rate of group 0 to 2.6972e-06.\n",
      "38.000m: train epoch *190*, loss = *2.759*, 38.000m: test epoch *190*, loss = *4.684*, best loss = *2.732*\n",
      "38.000m: train epoch *191*, loss = *2.758*, 38.000m: test epoch *191*, loss = *4.683*, best loss = *2.732*\n",
      "39.000m: train epoch *192*, loss = *2.754*, 39.000m: test epoch *192*, loss = *4.683*, best loss = *2.732*\n",
      "39.000m: train epoch *193*, loss = *2.749*, 39.000m: test epoch *193*, loss = *4.683*, best loss = *2.732*\n",
      "Epoch   194: reducing learning rate of group 0 to 2.4274e-06.\n",
      "39.000m: train epoch *194*, loss = *2.742*, 39.000m: test epoch *194*, loss = *4.683*, best loss = *2.732*\n",
      "39.000m: train epoch *195*, loss = *2.749*, 39.000m: test epoch *195*, loss = *4.684*, best loss = *2.732*\n",
      "39.000m: train epoch *196*, loss = *2.763*, 39.000m: test epoch *196*, loss = *4.684*, best loss = *2.732*\n",
      "40.000m: train epoch *197*, loss = *2.757*, 40.000m: test epoch *197*, loss = *4.684*, best loss = *2.732*\n",
      "Epoch   198: reducing learning rate of group 0 to 2.1847e-06.\n",
      "40.000m: train epoch *198*, loss = *2.748*, 40.000m: test epoch *198*, loss = *4.684*, best loss = *2.732*\n",
      "40.000m: train epoch *199*, loss = *2.762*, 40.000m: test epoch *199*, loss = *4.684*, best loss = *2.732*\n",
      "40.000m: train epoch *200*, loss = *2.766*, 40.000m: test epoch *200*, loss = *4.684*, best loss = *2.732*\n"
     ]
    }
   ],
   "source": [
    "#plateau - 1 layer\n",
    "linformer_trainer(l_model, train_data_iter, train_opt, test_data_iter, test_opt, l_optimizer, l_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> a GPU was detected, model will be trained on GPU\n",
      "1.000m: train epoch *1*, loss = *3.648*, 1.000m: test epoch *1*, loss = *3.421*, best loss = *3.648*\n",
      "2.000m: train epoch *2*, loss = *4.024*, 2.000m: test epoch *2*, loss = *3.824*, best loss = *3.648*\n",
      "3.000m: train epoch *3*, loss = *4.051*, 3.000m: test epoch *3*, loss = *3.827*, best loss = *3.648*\n",
      "4.000m: train epoch *4*, loss = *4.040*, 4.000m: test epoch *4*, loss = *3.811*, best loss = *3.648*\n",
      "Epoch     5: reducing learning rate of group 0 to 9.0000e-04.\n",
      "5.000m: train epoch *5*, loss = *4.032*, 5.000m: test epoch *5*, loss = *3.816*, best loss = *3.648*\n",
      "6.000m: train epoch *6*, loss = *4.024*, 6.000m: test epoch *6*, loss = *3.810*, best loss = *3.648*\n",
      "7.000m: train epoch *7*, loss = *4.017*, 7.000m: test epoch *7*, loss = *3.824*, best loss = *3.648*\n",
      "8.000m: train epoch *8*, loss = *4.015*, 8.000m: test epoch *8*, loss = *3.846*, best loss = *3.648*\n",
      "Epoch     9: reducing learning rate of group 0 to 8.1000e-04.\n",
      "9.000m: train epoch *9*, loss = *4.013*, 9.000m: test epoch *9*, loss = *3.815*, best loss = *3.648*\n",
      "10.000m: train epoch *10*, loss = *4.009*, 10.000m: test epoch *10*, loss = *3.802*, best loss = *3.648*\n",
      "11.000m: train epoch *11*, loss = *4.009*, 11.000m: test epoch *11*, loss = *3.822*, best loss = *3.648*\n",
      "12.000m: train epoch *12*, loss = *4.006*, 12.000m: test epoch *12*, loss = *3.828*, best loss = *3.648*\n",
      "Epoch    13: reducing learning rate of group 0 to 7.2900e-04.\n",
      "13.000m: train epoch *13*, loss = *4.001*, 13.000m: test epoch *13*, loss = *3.828*, best loss = *3.648*\n",
      "14.000m: train epoch *14*, loss = *4.000*, 14.000m: test epoch *14*, loss = *3.803*, best loss = *3.648*\n",
      "15.000m: train epoch *15*, loss = *3.996*, 15.000m: test epoch *15*, loss = *3.818*, best loss = *3.648*\n",
      "16.000m: train epoch *16*, loss = *3.999*, 16.000m: test epoch *16*, loss = *3.829*, best loss = *3.648*\n",
      "Epoch    17: reducing learning rate of group 0 to 6.5610e-04.\n",
      "17.000m: train epoch *17*, loss = *3.996*, 17.000m: test epoch *17*, loss = *3.804*, best loss = *3.648*\n",
      "18.000m: train epoch *18*, loss = *3.992*, 18.000m: test epoch *18*, loss = *3.820*, best loss = *3.648*\n",
      "19.000m: train epoch *19*, loss = *3.992*, 19.000m: test epoch *19*, loss = *3.798*, best loss = *3.648*\n",
      "20.000m: train epoch *20*, loss = *3.991*, 20.000m: test epoch *20*, loss = *3.800*, best loss = *3.648*\n",
      "Epoch    21: reducing learning rate of group 0 to 5.9049e-04.\n",
      "21.000m: train epoch *21*, loss = *3.989*, 21.000m: test epoch *21*, loss = *3.816*, best loss = *3.648*\n",
      "22.000m: train epoch *22*, loss = *3.987*, 22.000m: test epoch *22*, loss = *3.800*, best loss = *3.648*\n",
      "23.000m: train epoch *23*, loss = *3.987*, 23.000m: test epoch *23*, loss = *3.801*, best loss = *3.648*\n",
      "24.000m: train epoch *24*, loss = *3.986*, 24.000m: test epoch *24*, loss = *3.810*, best loss = *3.648*\n",
      "Epoch    25: reducing learning rate of group 0 to 5.3144e-04.\n",
      "25.000m: train epoch *25*, loss = *3.985*, 25.000m: test epoch *25*, loss = *3.808*, best loss = *3.648*\n",
      "26.000m: train epoch *26*, loss = *3.984*, 26.000m: test epoch *26*, loss = *3.800*, best loss = *3.648*\n",
      "27.000m: train epoch *27*, loss = *3.984*, 27.000m: test epoch *27*, loss = *3.799*, best loss = *3.648*\n",
      "28.000m: train epoch *28*, loss = *3.982*, 28.000m: test epoch *28*, loss = *3.800*, best loss = *3.648*\n",
      "Epoch    29: reducing learning rate of group 0 to 4.7830e-04.\n",
      "29.000m: train epoch *29*, loss = *3.982*, 29.000m: test epoch *29*, loss = *3.800*, best loss = *3.648*\n",
      "30.000m: train epoch *30*, loss = *3.980*, 30.000m: test epoch *30*, loss = *3.803*, best loss = *3.648*\n",
      "31.000m: train epoch *31*, loss = *3.979*, 31.000m: test epoch *31*, loss = *3.803*, best loss = *3.648*\n",
      "32.000m: train epoch *32*, loss = *3.979*, 32.000m: test epoch *32*, loss = *3.801*, best loss = *3.648*\n",
      "Epoch    33: reducing learning rate of group 0 to 4.3047e-04.\n",
      "33.000m: train epoch *33*, loss = *3.980*, 33.000m: test epoch *33*, loss = *3.800*, best loss = *3.648*\n",
      "34.000m: train epoch *34*, loss = *3.979*, 34.000m: test epoch *34*, loss = *3.800*, best loss = *3.648*\n",
      "35.000m: train epoch *35*, loss = *3.980*, 35.000m: test epoch *35*, loss = *3.799*, best loss = *3.648*\n",
      "36.000m: train epoch *36*, loss = *3.978*, 36.000m: test epoch *36*, loss = *3.809*, best loss = *3.648*\n",
      "Epoch    37: reducing learning rate of group 0 to 3.8742e-04.\n",
      "37.000m: train epoch *37*, loss = *3.977*, 37.000m: test epoch *37*, loss = *3.800*, best loss = *3.648*\n",
      "39.000m: train epoch *38*, loss = *3.974*, 39.000m: test epoch *38*, loss = *3.801*, best loss = *3.648*\n",
      "40.000m: train epoch *39*, loss = *3.976*, 40.000m: test epoch *39*, loss = *3.799*, best loss = *3.648*\n",
      "41.000m: train epoch *40*, loss = *3.979*, 41.000m: test epoch *40*, loss = *3.801*, best loss = *3.648*\n",
      "Epoch    41: reducing learning rate of group 0 to 3.4868e-04.\n",
      "42.000m: train epoch *41*, loss = *3.977*, 42.000m: test epoch *41*, loss = *3.800*, best loss = *3.648*\n",
      "43.000m: train epoch *42*, loss = *3.974*, 43.000m: test epoch *42*, loss = *3.799*, best loss = *3.648*\n",
      "44.000m: train epoch *43*, loss = *3.975*, 44.000m: test epoch *43*, loss = *3.799*, best loss = *3.648*\n",
      "45.000m: train epoch *44*, loss = *3.977*, 45.000m: test epoch *44*, loss = *3.800*, best loss = *3.648*\n",
      "Epoch    45: reducing learning rate of group 0 to 3.1381e-04.\n",
      "46.000m: train epoch *45*, loss = *3.977*, 46.000m: test epoch *45*, loss = *3.805*, best loss = *3.648*\n",
      "47.000m: train epoch *46*, loss = *3.976*, 47.000m: test epoch *46*, loss = *3.800*, best loss = *3.648*\n",
      "48.000m: train epoch *47*, loss = *3.978*, 48.000m: test epoch *47*, loss = *3.801*, best loss = *3.648*\n",
      "49.000m: train epoch *48*, loss = *3.975*, 49.000m: test epoch *48*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch    49: reducing learning rate of group 0 to 2.8243e-04.\n",
      "50.000m: train epoch *49*, loss = *3.974*, 50.000m: test epoch *49*, loss = *3.803*, best loss = *3.648*\n",
      "51.000m: train epoch *50*, loss = *3.976*, 51.000m: test epoch *50*, loss = *3.799*, best loss = *3.648*\n",
      "52.000m: train epoch *51*, loss = *3.973*, 52.000m: test epoch *51*, loss = *3.799*, best loss = *3.648*\n",
      "53.000m: train epoch *52*, loss = *3.976*, 53.000m: test epoch *52*, loss = *3.800*, best loss = *3.648*\n",
      "Epoch    53: reducing learning rate of group 0 to 2.5419e-04.\n",
      "54.000m: train epoch *53*, loss = *3.975*, 54.000m: test epoch *53*, loss = *3.799*, best loss = *3.648*\n",
      "56.000m: train epoch *54*, loss = *3.972*, 56.000m: test epoch *54*, loss = *3.799*, best loss = *3.648*\n",
      "57.000m: train epoch *55*, loss = *3.976*, 57.000m: test epoch *55*, loss = *3.799*, best loss = *3.648*\n",
      "58.000m: train epoch *56*, loss = *3.974*, 58.000m: test epoch *56*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch    57: reducing learning rate of group 0 to 2.2877e-04.\n",
      "59.000m: train epoch *57*, loss = *3.973*, 59.000m: test epoch *57*, loss = *3.799*, best loss = *3.648*\n",
      "60.000m: train epoch *58*, loss = *3.974*, 60.000m: test epoch *58*, loss = *3.799*, best loss = *3.648*\n",
      "61.000m: train epoch *59*, loss = *3.973*, 61.000m: test epoch *59*, loss = *3.799*, best loss = *3.648*\n",
      "62.000m: train epoch *60*, loss = *3.973*, 62.000m: test epoch *60*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch    61: reducing learning rate of group 0 to 2.0589e-04.\n",
      "63.000m: train epoch *61*, loss = *3.974*, 63.000m: test epoch *61*, loss = *3.799*, best loss = *3.648*\n",
      "64.000m: train epoch *62*, loss = *3.973*, 64.000m: test epoch *62*, loss = *3.798*, best loss = *3.648*\n",
      "65.000m: train epoch *63*, loss = *3.972*, 65.000m: test epoch *63*, loss = *3.798*, best loss = *3.648*\n",
      "66.000m: train epoch *64*, loss = *3.970*, 66.000m: test epoch *64*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch    65: reducing learning rate of group 0 to 1.8530e-04.\n",
      "67.000m: train epoch *65*, loss = *3.976*, 67.000m: test epoch *65*, loss = *3.800*, best loss = *3.648*\n",
      "68.000m: train epoch *66*, loss = *3.972*, 68.000m: test epoch *66*, loss = *3.798*, best loss = *3.648*\n",
      "69.000m: train epoch *67*, loss = *3.972*, 69.000m: test epoch *67*, loss = *3.799*, best loss = *3.648*\n",
      "71.000m: train epoch *68*, loss = *3.973*, 71.000m: test epoch *68*, loss = *3.798*, best loss = *3.648*\n",
      "Epoch    69: reducing learning rate of group 0 to 1.6677e-04.\n",
      "72.000m: train epoch *69*, loss = *3.973*, 72.000m: test epoch *69*, loss = *3.798*, best loss = *3.648*\n",
      "73.000m: train epoch *70*, loss = *3.971*, 73.000m: test epoch *70*, loss = *3.798*, best loss = *3.648*\n",
      "74.000m: train epoch *71*, loss = *3.972*, 74.000m: test epoch *71*, loss = *3.798*, best loss = *3.648*\n",
      "75.000m: train epoch *72*, loss = *3.972*, 75.000m: test epoch *72*, loss = *3.798*, best loss = *3.648*\n",
      "Epoch    73: reducing learning rate of group 0 to 1.5009e-04.\n",
      "76.000m: train epoch *73*, loss = *3.972*, 76.000m: test epoch *73*, loss = *3.799*, best loss = *3.648*\n",
      "77.000m: train epoch *74*, loss = *3.974*, 77.000m: test epoch *74*, loss = *3.798*, best loss = *3.648*\n",
      "78.000m: train epoch *75*, loss = *3.972*, 78.000m: test epoch *75*, loss = *3.799*, best loss = *3.648*\n",
      "79.000m: train epoch *76*, loss = *3.973*, 79.000m: test epoch *76*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch    77: reducing learning rate of group 0 to 1.3509e-04.\n",
      "80.000m: train epoch *77*, loss = *3.972*, 80.000m: test epoch *77*, loss = *3.799*, best loss = *3.648*\n",
      "81.000m: train epoch *78*, loss = *3.972*, 81.000m: test epoch *78*, loss = *3.798*, best loss = *3.648*\n",
      "82.000m: train epoch *79*, loss = *3.972*, 82.000m: test epoch *79*, loss = *3.799*, best loss = *3.648*\n",
      "83.000m: train epoch *80*, loss = *3.972*, 83.000m: test epoch *80*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch    81: reducing learning rate of group 0 to 1.2158e-04.\n",
      "84.000m: train epoch *81*, loss = *3.973*, 84.000m: test epoch *81*, loss = *3.799*, best loss = *3.648*\n",
      "86.000m: train epoch *82*, loss = *3.973*, 86.000m: test epoch *82*, loss = *3.799*, best loss = *3.648*\n",
      "87.000m: train epoch *83*, loss = *3.969*, 87.000m: test epoch *83*, loss = *3.799*, best loss = *3.648*\n",
      "88.000m: train epoch *84*, loss = *3.971*, 88.000m: test epoch *84*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0942e-04.\n",
      "89.000m: train epoch *85*, loss = *3.971*, 89.000m: test epoch *85*, loss = *3.799*, best loss = *3.648*\n",
      "90.000m: train epoch *86*, loss = *3.971*, 90.000m: test epoch *86*, loss = *3.799*, best loss = *3.648*\n",
      "91.000m: train epoch *87*, loss = *3.971*, 91.000m: test epoch *87*, loss = *3.799*, best loss = *3.648*\n",
      "92.000m: train epoch *88*, loss = *3.970*, 92.000m: test epoch *88*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch    89: reducing learning rate of group 0 to 9.8477e-05.\n",
      "93.000m: train epoch *89*, loss = *3.970*, 93.000m: test epoch *89*, loss = *3.799*, best loss = *3.648*\n",
      "94.000m: train epoch *90*, loss = *3.971*, 94.000m: test epoch *90*, loss = *3.799*, best loss = *3.648*\n",
      "95.000m: train epoch *91*, loss = *3.972*, 95.000m: test epoch *91*, loss = *3.799*, best loss = *3.648*\n",
      "96.000m: train epoch *92*, loss = *3.969*, 96.000m: test epoch *92*, loss = *3.800*, best loss = *3.648*\n",
      "Epoch    93: reducing learning rate of group 0 to 8.8629e-05.\n",
      "97.000m: train epoch *93*, loss = *3.971*, 97.000m: test epoch *93*, loss = *3.799*, best loss = *3.648*\n",
      "98.000m: train epoch *94*, loss = *3.970*, 98.000m: test epoch *94*, loss = *3.799*, best loss = *3.648*\n",
      "99.000m: train epoch *95*, loss = *3.971*, 99.000m: test epoch *95*, loss = *3.799*, best loss = *3.648*\n",
      "101.000m: train epoch *96*, loss = *3.971*, 101.000m: test epoch *96*, loss = *3.800*, best loss = *3.648*\n",
      "Epoch    97: reducing learning rate of group 0 to 7.9766e-05.\n",
      "102.000m: train epoch *97*, loss = *3.972*, 102.000m: test epoch *97*, loss = *3.799*, best loss = *3.648*\n",
      "103.000m: train epoch *98*, loss = *3.971*, 103.000m: test epoch *98*, loss = *3.799*, best loss = *3.648*\n",
      "104.000m: train epoch *99*, loss = *3.971*, 104.000m: test epoch *99*, loss = *3.799*, best loss = *3.648*\n",
      "105.000m: train epoch *100*, loss = *3.971*, 105.000m: test epoch *100*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   101: reducing learning rate of group 0 to 7.1790e-05.\n",
      "106.000m: train epoch *101*, loss = *3.971*, 106.000m: test epoch *101*, loss = *3.799*, best loss = *3.648*\n",
      "107.000m: train epoch *102*, loss = *3.970*, 107.000m: test epoch *102*, loss = *3.799*, best loss = *3.648*\n",
      "108.000m: train epoch *103*, loss = *3.970*, 108.000m: test epoch *103*, loss = *3.799*, best loss = *3.648*\n",
      "109.000m: train epoch *104*, loss = *3.968*, 109.000m: test epoch *104*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   105: reducing learning rate of group 0 to 6.4611e-05.\n",
      "110.000m: train epoch *105*, loss = *3.971*, 110.000m: test epoch *105*, loss = *3.799*, best loss = *3.648*\n",
      "111.000m: train epoch *106*, loss = *3.970*, 111.000m: test epoch *106*, loss = *3.799*, best loss = *3.648*\n",
      "112.000m: train epoch *107*, loss = *3.969*, 112.000m: test epoch *107*, loss = *3.799*, best loss = *3.648*\n",
      "113.000m: train epoch *108*, loss = *3.969*, 113.000m: test epoch *108*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   109: reducing learning rate of group 0 to 5.8150e-05.\n",
      "114.000m: train epoch *109*, loss = *3.969*, 114.000m: test epoch *109*, loss = *3.799*, best loss = *3.648*\n",
      "116.000m: train epoch *110*, loss = *3.970*, 116.000m: test epoch *110*, loss = *3.799*, best loss = *3.648*\n",
      "117.000m: train epoch *111*, loss = *3.970*, 117.000m: test epoch *111*, loss = *3.799*, best loss = *3.648*\n",
      "118.000m: train epoch *112*, loss = *3.971*, 118.000m: test epoch *112*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   113: reducing learning rate of group 0 to 5.2335e-05.\n",
      "119.000m: train epoch *113*, loss = *3.969*, 119.000m: test epoch *113*, loss = *3.799*, best loss = *3.648*\n",
      "120.000m: train epoch *114*, loss = *3.972*, 120.000m: test epoch *114*, loss = *3.799*, best loss = *3.648*\n",
      "121.000m: train epoch *115*, loss = *3.969*, 121.000m: test epoch *115*, loss = *3.799*, best loss = *3.648*\n",
      "122.000m: train epoch *116*, loss = *3.969*, 122.000m: test epoch *116*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   117: reducing learning rate of group 0 to 4.7101e-05.\n",
      "123.000m: train epoch *117*, loss = *3.969*, 123.000m: test epoch *117*, loss = *3.799*, best loss = *3.648*\n",
      "124.000m: train epoch *118*, loss = *3.970*, 124.000m: test epoch *118*, loss = *3.799*, best loss = *3.648*\n",
      "125.000m: train epoch *119*, loss = *3.971*, 125.000m: test epoch *119*, loss = *3.799*, best loss = *3.648*\n",
      "126.000m: train epoch *120*, loss = *3.970*, 126.000m: test epoch *120*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   121: reducing learning rate of group 0 to 4.2391e-05.\n",
      "127.000m: train epoch *121*, loss = *3.969*, 127.000m: test epoch *121*, loss = *3.799*, best loss = *3.648*\n",
      "128.000m: train epoch *122*, loss = *3.970*, 128.000m: test epoch *122*, loss = *3.799*, best loss = *3.648*\n",
      "129.000m: train epoch *123*, loss = *3.968*, 129.000m: test epoch *123*, loss = *3.799*, best loss = *3.648*\n",
      "131.000m: train epoch *124*, loss = *3.970*, 131.000m: test epoch *124*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   125: reducing learning rate of group 0 to 3.8152e-05.\n",
      "132.000m: train epoch *125*, loss = *3.969*, 132.000m: test epoch *125*, loss = *3.799*, best loss = *3.648*\n",
      "133.000m: train epoch *126*, loss = *3.967*, 133.000m: test epoch *126*, loss = *3.799*, best loss = *3.648*\n",
      "134.000m: train epoch *127*, loss = *3.969*, 134.000m: test epoch *127*, loss = *3.799*, best loss = *3.648*\n",
      "135.000m: train epoch *128*, loss = *3.969*, 135.000m: test epoch *128*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   129: reducing learning rate of group 0 to 3.4337e-05.\n",
      "136.000m: train epoch *129*, loss = *3.970*, 136.000m: test epoch *129*, loss = *3.799*, best loss = *3.648*\n",
      "137.000m: train epoch *130*, loss = *3.970*, 137.000m: test epoch *130*, loss = *3.799*, best loss = *3.648*\n",
      "138.000m: train epoch *131*, loss = *3.969*, 138.000m: test epoch *131*, loss = *3.799*, best loss = *3.648*\n",
      "139.000m: train epoch *132*, loss = *3.971*, 139.000m: test epoch *132*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   133: reducing learning rate of group 0 to 3.0903e-05.\n",
      "140.000m: train epoch *133*, loss = *3.968*, 140.000m: test epoch *133*, loss = *3.799*, best loss = *3.648*\n",
      "141.000m: train epoch *134*, loss = *3.970*, 141.000m: test epoch *134*, loss = *3.799*, best loss = *3.648*\n",
      "142.000m: train epoch *135*, loss = *3.969*, 142.000m: test epoch *135*, loss = *3.799*, best loss = *3.648*\n",
      "143.000m: train epoch *136*, loss = *3.967*, 143.000m: test epoch *136*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   137: reducing learning rate of group 0 to 2.7813e-05.\n",
      "144.000m: train epoch *137*, loss = *3.969*, 144.000m: test epoch *137*, loss = *3.799*, best loss = *3.648*\n",
      "145.000m: train epoch *138*, loss = *3.970*, 145.000m: test epoch *138*, loss = *3.799*, best loss = *3.648*\n",
      "147.000m: train epoch *139*, loss = *3.968*, 147.000m: test epoch *139*, loss = *3.799*, best loss = *3.648*\n",
      "148.000m: train epoch *140*, loss = *3.971*, 148.000m: test epoch *140*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   141: reducing learning rate of group 0 to 2.5032e-05.\n",
      "149.000m: train epoch *141*, loss = *3.967*, 149.000m: test epoch *141*, loss = *3.799*, best loss = *3.648*\n",
      "150.000m: train epoch *142*, loss = *3.970*, 150.000m: test epoch *142*, loss = *3.799*, best loss = *3.648*\n",
      "151.000m: train epoch *143*, loss = *3.967*, 151.000m: test epoch *143*, loss = *3.799*, best loss = *3.648*\n",
      "152.000m: train epoch *144*, loss = *3.967*, 152.000m: test epoch *144*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   145: reducing learning rate of group 0 to 2.2528e-05.\n",
      "153.000m: train epoch *145*, loss = *3.969*, 153.000m: test epoch *145*, loss = *3.799*, best loss = *3.648*\n",
      "154.000m: train epoch *146*, loss = *3.969*, 154.000m: test epoch *146*, loss = *3.799*, best loss = *3.648*\n",
      "155.000m: train epoch *147*, loss = *3.968*, 155.000m: test epoch *147*, loss = *3.799*, best loss = *3.648*\n",
      "156.000m: train epoch *148*, loss = *3.967*, 156.000m: test epoch *148*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   149: reducing learning rate of group 0 to 2.0276e-05.\n",
      "157.000m: train epoch *149*, loss = *3.969*, 157.000m: test epoch *149*, loss = *3.799*, best loss = *3.648*\n",
      "158.000m: train epoch *150*, loss = *3.968*, 158.000m: test epoch *150*, loss = *3.799*, best loss = *3.648*\n",
      "159.000m: train epoch *151*, loss = *3.970*, 159.000m: test epoch *151*, loss = *3.799*, best loss = *3.648*\n",
      "160.000m: train epoch *152*, loss = *3.968*, 160.000m: test epoch *152*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   153: reducing learning rate of group 0 to 1.8248e-05.\n",
      "162.000m: train epoch *153*, loss = *3.971*, 162.000m: test epoch *153*, loss = *3.799*, best loss = *3.648*\n",
      "163.000m: train epoch *154*, loss = *3.969*, 163.000m: test epoch *154*, loss = *3.799*, best loss = *3.648*\n",
      "164.000m: train epoch *155*, loss = *3.968*, 164.000m: test epoch *155*, loss = *3.799*, best loss = *3.648*\n",
      "165.000m: train epoch *156*, loss = *3.967*, 165.000m: test epoch *156*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   157: reducing learning rate of group 0 to 1.6423e-05.\n",
      "166.000m: train epoch *157*, loss = *3.968*, 166.000m: test epoch *157*, loss = *3.799*, best loss = *3.648*\n",
      "167.000m: train epoch *158*, loss = *3.968*, 167.000m: test epoch *158*, loss = *3.799*, best loss = *3.648*\n",
      "168.000m: train epoch *159*, loss = *3.971*, 168.000m: test epoch *159*, loss = *3.799*, best loss = *3.648*\n",
      "169.000m: train epoch *160*, loss = *3.968*, 169.000m: test epoch *160*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   161: reducing learning rate of group 0 to 1.4781e-05.\n",
      "170.000m: train epoch *161*, loss = *3.968*, 170.000m: test epoch *161*, loss = *3.799*, best loss = *3.648*\n",
      "171.000m: train epoch *162*, loss = *3.969*, 171.000m: test epoch *162*, loss = *3.799*, best loss = *3.648*\n",
      "172.000m: train epoch *163*, loss = *3.969*, 172.000m: test epoch *163*, loss = *3.799*, best loss = *3.648*\n",
      "173.000m: train epoch *164*, loss = *3.969*, 173.000m: test epoch *164*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   165: reducing learning rate of group 0 to 1.3303e-05.\n",
      "174.000m: train epoch *165*, loss = *3.969*, 174.000m: test epoch *165*, loss = *3.799*, best loss = *3.648*\n",
      "175.000m: train epoch *166*, loss = *3.969*, 175.000m: test epoch *166*, loss = *3.799*, best loss = *3.648*\n",
      "177.000m: train epoch *167*, loss = *3.970*, 177.000m: test epoch *167*, loss = *3.799*, best loss = *3.648*\n",
      "178.000m: train epoch *168*, loss = *3.970*, 178.000m: test epoch *168*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   169: reducing learning rate of group 0 to 1.1973e-05.\n",
      "179.000m: train epoch *169*, loss = *3.969*, 179.000m: test epoch *169*, loss = *3.799*, best loss = *3.648*\n",
      "180.000m: train epoch *170*, loss = *3.968*, 180.000m: test epoch *170*, loss = *3.799*, best loss = *3.648*\n",
      "181.000m: train epoch *171*, loss = *3.969*, 181.000m: test epoch *171*, loss = *3.799*, best loss = *3.648*\n",
      "182.000m: train epoch *172*, loss = *3.969*, 182.000m: test epoch *172*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   173: reducing learning rate of group 0 to 1.0775e-05.\n",
      "183.000m: train epoch *173*, loss = *3.969*, 183.000m: test epoch *173*, loss = *3.799*, best loss = *3.648*\n",
      "184.000m: train epoch *174*, loss = *3.969*, 184.000m: test epoch *174*, loss = *3.799*, best loss = *3.648*\n",
      "185.000m: train epoch *175*, loss = *3.967*, 185.000m: test epoch *175*, loss = *3.799*, best loss = *3.648*\n",
      "186.000m: train epoch *176*, loss = *3.968*, 186.000m: test epoch *176*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   177: reducing learning rate of group 0 to 9.6977e-06.\n",
      "187.000m: train epoch *177*, loss = *3.969*, 187.000m: test epoch *177*, loss = *3.799*, best loss = *3.648*\n",
      "188.000m: train epoch *178*, loss = *3.969*, 188.000m: test epoch *178*, loss = *3.799*, best loss = *3.648*\n",
      "189.000m: train epoch *179*, loss = *3.968*, 189.000m: test epoch *179*, loss = *3.799*, best loss = *3.648*\n",
      "190.000m: train epoch *180*, loss = *3.969*, 190.000m: test epoch *180*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   181: reducing learning rate of group 0 to 8.7280e-06.\n",
      "191.000m: train epoch *181*, loss = *3.967*, 191.000m: test epoch *181*, loss = *3.799*, best loss = *3.648*\n",
      "193.000m: train epoch *182*, loss = *3.969*, 193.000m: test epoch *182*, loss = *3.799*, best loss = *3.648*\n",
      "194.000m: train epoch *183*, loss = *3.969*, 194.000m: test epoch *183*, loss = *3.799*, best loss = *3.648*\n",
      "195.000m: train epoch *184*, loss = *3.969*, 195.000m: test epoch *184*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   185: reducing learning rate of group 0 to 7.8552e-06.\n",
      "196.000m: train epoch *185*, loss = *3.970*, 196.000m: test epoch *185*, loss = *3.799*, best loss = *3.648*\n",
      "197.000m: train epoch *186*, loss = *3.970*, 197.000m: test epoch *186*, loss = *3.799*, best loss = *3.648*\n",
      "198.000m: train epoch *187*, loss = *3.968*, 198.000m: test epoch *187*, loss = *3.799*, best loss = *3.648*\n",
      "199.000m: train epoch *188*, loss = *3.969*, 199.000m: test epoch *188*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   189: reducing learning rate of group 0 to 7.0697e-06.\n",
      "200.000m: train epoch *189*, loss = *3.969*, 200.000m: test epoch *189*, loss = *3.799*, best loss = *3.648*\n",
      "201.000m: train epoch *190*, loss = *3.968*, 201.000m: test epoch *190*, loss = *3.799*, best loss = *3.648*\n",
      "202.000m: train epoch *191*, loss = *3.969*, 202.000m: test epoch *191*, loss = *3.799*, best loss = *3.648*\n",
      "203.000m: train epoch *192*, loss = *3.968*, 203.000m: test epoch *192*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   193: reducing learning rate of group 0 to 6.3627e-06.\n",
      "204.000m: train epoch *193*, loss = *3.967*, 204.000m: test epoch *193*, loss = *3.799*, best loss = *3.648*\n",
      "205.000m: train epoch *194*, loss = *3.970*, 205.000m: test epoch *194*, loss = *3.799*, best loss = *3.648*\n",
      "206.000m: train epoch *195*, loss = *3.969*, 206.000m: test epoch *195*, loss = *3.799*, best loss = *3.648*\n",
      "207.000m: train epoch *196*, loss = *3.967*, 207.000m: test epoch *196*, loss = *3.799*, best loss = *3.648*\n",
      "Epoch   197: reducing learning rate of group 0 to 5.7264e-06.\n",
      "209.000m: train epoch *197*, loss = *3.968*, 209.000m: test epoch *197*, loss = *3.799*, best loss = *3.648*\n",
      "210.000m: train epoch *198*, loss = *3.967*, 210.000m: test epoch *198*, loss = *3.799*, best loss = *3.648*\n",
      "211.000m: train epoch *199*, loss = *3.968*, 211.000m: test epoch *199*, loss = *3.799*, best loss = *3.648*\n",
      "212.000m: train epoch *200*, loss = *3.969*, 212.000m: test epoch *200*, loss = *3.799*, best loss = *3.648*\n"
     ]
    }
   ],
   "source": [
    "#plateau - 6 layers -0.001\n",
    "linformer_trainer(l_model, train_data_iter, train_opt, test_data_iter, test_opt, l_optimizer, l_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> a GPU was detected, model will be trained on GPU\n",
      "0.000m: train epoch *1*, loss = *2.780*, 0.000m: test epoch *1*, loss = *4.643*, best loss = *2.780*\n",
      "0.000m: train epoch *2*, loss = *2.770*, 0.000m: test epoch *2*, loss = *4.711*, best loss = *2.770*\n",
      "0.000m: train epoch *3*, loss = *2.776*, 0.000m: test epoch *3*, loss = *4.743*, best loss = *2.770*\n",
      "0.000m: train epoch *4*, loss = *2.799*, 0.000m: test epoch *4*, loss = *4.686*, best loss = *2.770*\n",
      "1.000m: train epoch *5*, loss = *2.786*, 1.000m: test epoch *5*, loss = *4.719*, best loss = *2.770*\n",
      "1.000m: train epoch *6*, loss = *2.775*, 1.000m: test epoch *6*, loss = *4.717*, best loss = *2.770*\n",
      "1.000m: train epoch *7*, loss = *2.771*, 1.000m: test epoch *7*, loss = *4.689*, best loss = *2.770*\n",
      "1.000m: train epoch *8*, loss = *2.793*, 1.000m: test epoch *8*, loss = *4.678*, best loss = *2.770*\n",
      "1.000m: train epoch *9*, loss = *2.787*, 1.000m: test epoch *9*, loss = *4.691*, best loss = *2.770*\n",
      "2.000m: train epoch *10*, loss = *2.782*, 2.000m: test epoch *10*, loss = *4.710*, best loss = *2.770*\n",
      "2.000m: train epoch *11*, loss = *2.787*, 2.000m: test epoch *11*, loss = *4.689*, best loss = *2.770*\n",
      "2.000m: train epoch *12*, loss = *2.783*, 2.000m: test epoch *12*, loss = *4.678*, best loss = *2.770*\n",
      "2.000m: train epoch *13*, loss = *2.779*, 2.000m: test epoch *13*, loss = *4.682*, best loss = *2.770*\n",
      "2.000m: train epoch *14*, loss = *2.761*, 2.000m: test epoch *14*, loss = *4.708*, best loss = *2.761*\n",
      "3.000m: train epoch *15*, loss = *2.787*, 3.000m: test epoch *15*, loss = *4.688*, best loss = *2.761*\n",
      "3.000m: train epoch *16*, loss = *2.789*, 3.000m: test epoch *16*, loss = *4.667*, best loss = *2.761*\n",
      "3.000m: train epoch *17*, loss = *2.787*, 3.000m: test epoch *17*, loss = *4.665*, best loss = *2.761*\n",
      "3.000m: train epoch *18*, loss = *2.773*, 3.000m: test epoch *18*, loss = *4.682*, best loss = *2.761*\n",
      "3.000m: train epoch *19*, loss = *2.772*, 3.000m: test epoch *19*, loss = *4.738*, best loss = *2.761*\n",
      "4.000m: train epoch *20*, loss = *2.778*, 4.000m: test epoch *20*, loss = *4.683*, best loss = *2.761*\n",
      "4.000m: train epoch *21*, loss = *2.781*, 4.000m: test epoch *21*, loss = *4.736*, best loss = *2.761*\n",
      "4.000m: train epoch *22*, loss = *2.775*, 4.000m: test epoch *22*, loss = *4.671*, best loss = *2.761*\n",
      "4.000m: train epoch *23*, loss = *2.758*, 4.000m: test epoch *23*, loss = *4.697*, best loss = *2.758*\n",
      "4.000m: train epoch *24*, loss = *2.782*, 4.000m: test epoch *24*, loss = *4.708*, best loss = *2.758*\n",
      "5.000m: train epoch *25*, loss = *2.777*, 5.000m: test epoch *25*, loss = *4.667*, best loss = *2.758*\n",
      "5.000m: train epoch *26*, loss = *2.772*, 5.000m: test epoch *26*, loss = *4.697*, best loss = *2.758*\n",
      "5.000m: train epoch *27*, loss = *2.792*, 5.000m: test epoch *27*, loss = *4.697*, best loss = *2.758*\n",
      "5.000m: train epoch *28*, loss = *2.770*, 5.000m: test epoch *28*, loss = *4.688*, best loss = *2.758*\n",
      "5.000m: train epoch *29*, loss = *2.795*, 5.000m: test epoch *29*, loss = *4.684*, best loss = *2.758*\n",
      "6.000m: train epoch *30*, loss = *2.777*, 6.000m: test epoch *30*, loss = *4.679*, best loss = *2.758*\n",
      "6.000m: train epoch *31*, loss = *2.786*, 6.000m: test epoch *31*, loss = *4.740*, best loss = *2.758*\n",
      "6.000m: train epoch *32*, loss = *2.782*, 6.000m: test epoch *32*, loss = *4.727*, best loss = *2.758*\n",
      "6.000m: train epoch *33*, loss = *2.777*, 6.000m: test epoch *33*, loss = *4.651*, best loss = *2.758*\n",
      "6.000m: train epoch *34*, loss = *2.787*, 6.000m: test epoch *34*, loss = *4.677*, best loss = *2.758*\n",
      "7.000m: train epoch *35*, loss = *2.777*, 7.000m: test epoch *35*, loss = *4.662*, best loss = *2.758*\n",
      "7.000m: train epoch *36*, loss = *2.761*, 7.000m: test epoch *36*, loss = *4.707*, best loss = *2.758*\n",
      "7.000m: train epoch *37*, loss = *2.766*, 7.000m: test epoch *37*, loss = *4.721*, best loss = *2.758*\n",
      "7.000m: train epoch *38*, loss = *2.783*, 7.000m: test epoch *38*, loss = *4.733*, best loss = *2.758*\n",
      "7.000m: train epoch *39*, loss = *2.773*, 7.000m: test epoch *39*, loss = *4.713*, best loss = *2.758*\n",
      "8.000m: train epoch *40*, loss = *2.799*, 8.000m: test epoch *40*, loss = *4.723*, best loss = *2.758*\n",
      "8.000m: train epoch *41*, loss = *2.781*, 8.000m: test epoch *41*, loss = *4.690*, best loss = *2.758*\n",
      "8.000m: train epoch *42*, loss = *2.764*, 8.000m: test epoch *42*, loss = *4.680*, best loss = *2.758*\n",
      "8.000m: train epoch *43*, loss = *2.772*, 8.000m: test epoch *43*, loss = *4.683*, best loss = *2.758*\n",
      "8.000m: train epoch *44*, loss = *2.767*, 8.000m: test epoch *44*, loss = *4.724*, best loss = *2.758*\n",
      "9.000m: train epoch *45*, loss = *2.773*, 9.000m: test epoch *45*, loss = *4.750*, best loss = *2.758*\n",
      "9.000m: train epoch *46*, loss = *2.778*, 9.000m: test epoch *46*, loss = *4.708*, best loss = *2.758*\n",
      "9.000m: train epoch *47*, loss = *2.761*, 9.000m: test epoch *47*, loss = *4.736*, best loss = *2.758*\n",
      "9.000m: train epoch *48*, loss = *2.757*, 9.000m: test epoch *48*, loss = *4.724*, best loss = *2.757*\n",
      "9.000m: train epoch *49*, loss = *2.782*, 9.000m: test epoch *49*, loss = *4.720*, best loss = *2.757*\n",
      "10.000m: train epoch *50*, loss = *2.784*, 10.000m: test epoch *50*, loss = *4.673*, best loss = *2.757*\n",
      "10.000m: train epoch *51*, loss = *2.777*, 10.000m: test epoch *51*, loss = *4.699*, best loss = *2.757*\n",
      "10.000m: train epoch *52*, loss = *2.759*, 10.000m: test epoch *52*, loss = *4.725*, best loss = *2.757*\n",
      "10.000m: train epoch *53*, loss = *2.764*, 10.000m: test epoch *53*, loss = *4.770*, best loss = *2.757*\n",
      "10.000m: train epoch *54*, loss = *2.769*, 10.000m: test epoch *54*, loss = *4.786*, best loss = *2.757*\n",
      "11.000m: train epoch *55*, loss = *2.771*, 11.000m: test epoch *55*, loss = *4.780*, best loss = *2.757*\n",
      "11.000m: train epoch *56*, loss = *2.781*, 11.000m: test epoch *56*, loss = *4.753*, best loss = *2.757*\n",
      "11.000m: train epoch *57*, loss = *2.778*, 11.000m: test epoch *57*, loss = *4.736*, best loss = *2.757*\n",
      "11.000m: train epoch *58*, loss = *2.763*, 11.000m: test epoch *58*, loss = *4.754*, best loss = *2.757*\n",
      "11.000m: train epoch *59*, loss = *2.773*, 11.000m: test epoch *59*, loss = *4.719*, best loss = *2.757*\n",
      "12.000m: train epoch *60*, loss = *2.780*, 12.000m: test epoch *60*, loss = *4.709*, best loss = *2.757*\n",
      "12.000m: train epoch *61*, loss = *2.782*, 12.000m: test epoch *61*, loss = *4.728*, best loss = *2.757*\n",
      "12.000m: train epoch *62*, loss = *2.784*, 12.000m: test epoch *62*, loss = *4.720*, best loss = *2.757*\n",
      "12.000m: train epoch *63*, loss = *2.756*, 12.000m: test epoch *63*, loss = *4.743*, best loss = *2.756*\n",
      "12.000m: train epoch *64*, loss = *2.782*, 12.000m: test epoch *64*, loss = *4.730*, best loss = *2.756*\n",
      "13.000m: train epoch *65*, loss = *2.769*, 13.000m: test epoch *65*, loss = *4.729*, best loss = *2.756*\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-946a346e0575>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#cosine 0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlinformer_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/deeplearning/project2/scripts/TalkTrain.py\u001b[0m in \u001b[0;36mlinformer_trainer\u001b[0;34m(model, train_data_iterator, train_options, test_data_iterator, test_options, optimizer, scheduler, scheduler_name)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mtrain_batch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mtrain_batch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mtrain_total_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_batch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#cosine 0.001\n",
    "linformer_trainer(l_model, train_data_iter, train_opt, test_data_iter, test_opt, l_optimizer, l_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> a GPU was detected, model will be trained on GPU\n",
      "28.000m: train epoch *1*, loss = *0.359*, 28.000m: test epoch *1*, loss = *0.162*, best loss = *0.359*\n",
      "57.000m: train epoch *2*, loss = *0.276*, 57.000m: test epoch *2*, loss = *0.183*, best loss = *0.276*\n",
      "85.000m: train epoch *3*, loss = *0.275*, 85.000m: test epoch *3*, loss = *0.172*, best loss = *0.275*\n",
      "114.000m: train epoch *4*, loss = *0.272*, 114.000m: test epoch *4*, loss = *0.165*, best loss = *0.272*\n",
      "143.000m: train epoch *5*, loss = *0.273*, 143.000m: test epoch *5*, loss = *0.166*, best loss = *0.272*\n",
      "172.000m: train epoch *6*, loss = *0.277*, 172.000m: test epoch *6*, loss = *0.174*, best loss = *0.272*\n"
     ]
    }
   ],
   "source": [
    "#plateau - 6 layers -0.0003 smalldata\n",
    "linformer_trainer(l_model, train_data_iter, train_opt, test_data_iter, test_opt, l_optimizer, l_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> a GPU was detected, model will be trained on GPU\n",
      "618.403m: train epoch *1*, loss = *0.376*, 618.404m: test epoch *1*, loss = *0.165*, best loss = *0.376*\n",
      "1236.832m: train epoch *2*, loss = *0.277*, 1236.833m: test epoch *2*, loss = *0.181*, best loss = *0.277*\n",
      "1854.888m: train epoch *3*, loss = *0.275*, 1854.889m: test epoch *3*, loss = *0.173*, best loss = *0.275*\n",
      "2472.388m: train epoch *4*, loss = *0.275*, 2472.388m: test epoch *4*, loss = *0.168*, best loss = *0.275*\n",
      "3090.095m: train epoch *5*, loss = *0.277*, 3090.095m: test epoch *5*, loss = *0.172*, best loss = *0.275*\n",
      "3708.625m: train epoch *6*, loss = *0.280*, 3708.626m: test epoch *6*, loss = *0.170*, best loss = *0.275*\n",
      "Epoch     7: reducing learning rate of group 0 to 2.7000e-04.\n",
      "4325.161m: train epoch *7*, loss = *0.281*, 4325.162m: test epoch *7*, loss = *0.167*, best loss = *0.275*\n",
      "4942.032m: train epoch *8*, loss = *0.284*, 4942.032m: test epoch *8*, loss = *0.172*, best loss = *0.275*\n",
      "5559.489m: train epoch *9*, loss = *0.285*, 5559.489m: test epoch *9*, loss = *0.174*, best loss = *0.275*\n"
     ]
    }
   ],
   "source": [
    "#plateau - 2 layers -0.0003 data\n",
    "linformer_trainer(l_model, train_data_iter, train_opt, test_data_iter, test_opt, l_optimizer, l_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plateau - 6 layers -0.0003 data\n",
    "linformer_trainer(l_model, train_data_iter, train_opt, test_data_iter, test_opt, l_optimizer, l_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> a GPU was detected, model will be trained on GPU\n",
      "29.610m: train epoch *1*, loss = *0.355*, 29.610m: test epoch *1*, loss = *0.164*, best loss = *0.355*\n",
      "58.754m: train epoch *2*, loss = *0.276*, 58.754m: test epoch *2*, loss = *0.179*, best loss = *0.276*\n",
      "88.426m: train epoch *3*, loss = *0.274*, 88.426m: test epoch *3*, loss = *0.169*, best loss = *0.274*\n",
      "117.689m: train epoch *4*, loss = *0.271*, 117.689m: test epoch *4*, loss = *0.169*, best loss = *0.271*\n",
      "147.713m: train epoch *5*, loss = *0.274*, 147.713m: test epoch *5*, loss = *0.170*, best loss = *0.271*\n",
      "176.758m: train epoch *6*, loss = *0.277*, 176.758m: test epoch *6*, loss = *0.169*, best loss = *0.271*\n",
      "206.233m: train epoch *7*, loss = *0.280*, 206.233m: test epoch *7*, loss = *0.171*, best loss = *0.271*\n",
      "Epoch     8: reducing learning rate of group 0 to 2.7000e-04.\n",
      "236.419m: train epoch *8*, loss = *0.286*, 236.419m: test epoch *8*, loss = *0.178*, best loss = *0.271*\n"
     ]
    }
   ],
   "source": [
    "#plateau - 6 layers -0.0003 data\n",
    "linformer_trainer(l_model, train_data_iter, train_opt, test_data_iter, test_opt, l_optimizer, l_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> a GPU was detected, model will be trained on GPU\n",
      "0.079236secs: train iter *1*, train size 1 *22*, train size 2 *113*\n",
      "0.026605secs: train iter *2*, train size 1 *10*, train size 2 *13*\n",
      "0.025490secs: train iter *3*, train size 1 *10*, train size 2 *10*\n",
      "0.062034secs: train iter *4*, train size 1 *19*, train size 2 *91*\n",
      "0.026649secs: train iter *5*, train size 1 *11*, train size 2 *16*\n",
      "0.039931secs: train iter *6*, train size 1 *32*, train size 2 *38*\n",
      "0.025368secs: train iter *7*, train size 1 *5*, train size 2 *6*\n",
      "0.074836secs: train iter *8*, train size 1 *7*, train size 2 *121*\n",
      "0.054698secs: train iter *9*, train size 1 *56*, train size 2 *62*\n",
      "0.025429secs: train iter *10*, train size 1 *9*, train size 2 *7*\n",
      "0.025245secs: train iter *11*, train size 1 *8*, train size 2 *7*\n",
      "0.071464secs: train iter *12*, train size 1 *14*, train size 2 *112*\n",
      "0.025408secs: train iter *13*, train size 1 *9*, train size 2 *12*\n",
      "0.054296secs: train iter *14*, train size 1 *18*, train size 2 *77*\n",
      "0.043122secs: train iter *15*, train size 1 *44*, train size 2 *44*\n",
      "0.025292secs: train iter *16*, train size 1 *4*, train size 2 *14*\n",
      "0.069785secs: train iter *17*, train size 1 *8*, train size 2 *108*\n",
      "0.025309secs: train iter *18*, train size 1 *13*, train size 2 *11*\n",
      "0.067565secs: train iter *19*, train size 1 *5*, train size 2 *103*\n",
      "0.053414secs: train iter *20*, train size 1 *10*, train size 2 *74*\n",
      "0.028087secs: train iter *21*, train size 1 *14*, train size 2 *19*\n",
      "0.026759secs: train iter *22*, train size 1 *5*, train size 2 *18*\n",
      "0.027443secs: train iter *23*, train size 1 *10*, train size 2 *18*\n",
      "0.063602secs: train iter *24*, train size 1 *6*, train size 2 *96*\n",
      "0.082727secs: train iter *25*, train size 1 *12*, train size 2 *135*\n",
      "0.034883secs: train iter *26*, train size 1 *11*, train size 2 *32*\n",
      "0.039088secs: train iter *27*, train size 1 *9*, train size 2 *43*\n",
      "0.029452secs: train iter *28*, train size 1 *12*, train size 2 *22*\n",
      "0.040862secs: train iter *29*, train size 1 *24*, train size 2 *46*\n",
      "0.119337secs: train iter *30*, train size 1 *9*, train size 2 *209*\n",
      "0.025268secs: train iter *31*, train size 1 *7*, train size 2 *14*\n",
      "0.025308secs: train iter *32*, train size 1 *12*, train size 2 *13*\n",
      "0.025307secs: train iter *33*, train size 1 *9*, train size 2 *6*\n",
      "0.025281secs: train iter *34*, train size 1 *15*, train size 2 *10*\n",
      "0.032938secs: train iter *35*, train size 1 *6*, train size 2 *28*\n",
      "0.025336secs: train iter *36*, train size 1 *7*, train size 2 *6*\n",
      "0.037456secs: train iter *37*, train size 1 *5*, train size 2 *38*\n",
      "0.026700secs: train iter *38*, train size 1 *16*, train size 2 *16*\n",
      "0.025170secs: train iter *39*, train size 1 *8*, train size 2 *8*\n",
      "0.050169secs: train iter *40*, train size 1 *21*, train size 2 *64*\n",
      "0.025393secs: train iter *41*, train size 1 *11*, train size 2 *7*\n",
      "0.027917secs: train iter *42*, train size 1 *15*, train size 2 *18*\n",
      "0.049983secs: train iter *43*, train size 1 *15*, train size 2 *64*\n",
      "0.047526secs: train iter *44*, train size 1 *28*, train size 2 *55*\n",
      "0.026933secs: train iter *45*, train size 1 *6*, train size 2 *18*\n",
      "0.025307secs: train iter *46*, train size 1 *10*, train size 2 *8*\n",
      "0.035226secs: train iter *47*, train size 1 *40*, train size 2 *27*\n",
      "0.025252secs: train iter *48*, train size 1 *4*, train size 2 *8*\n",
      "0.031318secs: train iter *49*, train size 1 *22*, train size 2 *24*\n",
      "0.040926secs: train iter *50*, train size 1 *23*, train size 2 *46*\n",
      "0.025245secs: train iter *51*, train size 1 *4*, train size 2 *11*\n",
      "0.025343secs: train iter *52*, train size 1 *6*, train size 2 *12*\n",
      "0.032943secs: train iter *53*, train size 1 *18*, train size 2 *28*\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-5ccb4388b3af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformer_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/deeplearning/project2/scripts/TalkTrain.py\u001b[0m in \u001b[0;36mtransformer_trainer\u001b[0;34m(model, train_data_iterator, train_options, test_data_iterator, test_options, optimizer, scheduler, scheduler_name)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mtrain_batch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mtrain_total_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_batch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mscheduler_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "transformer_trainer(t_model, train_data_iter, train_opt, test_data_iter, test_opt, t_optimizer, t_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> a GPU was detected, model will be trained on GPU\n",
      "0.307968secs: train iter *1*, train size 1 *19*, train size 2 *18*\n",
      "0.272683secs: train iter *2*, train size 1 *18*, train size 2 *19*\n",
      "0.248318secs: train iter *3*, train size 1 *6*, train size 2 *22*\n",
      "0.236382secs: train iter *4*, train size 1 *8*, train size 2 *20*\n",
      "0.237868secs: train iter *5*, train size 1 *6*, train size 2 *7*\n",
      "0.242065secs: train iter *6*, train size 1 *35*, train size 2 *34*\n",
      "0.236939secs: train iter *7*, train size 1 *10*, train size 2 *10*\n",
      "0.256302secs: train iter *8*, train size 1 *24*, train size 2 *43*\n",
      "0.238596secs: train iter *9*, train size 1 *9*, train size 2 *8*\n",
      "0.238395secs: train iter *10*, train size 1 *12*, train size 2 *13*\n",
      "0.302706secs: train iter *11*, train size 1 *20*, train size 2 *64*\n",
      "0.271135secs: train iter *12*, train size 1 *26*, train size 2 *63*\n",
      "0.238793secs: train iter *13*, train size 1 *7*, train size 2 *6*\n",
      "0.240233secs: train iter *14*, train size 1 *4*, train size 2 *14*\n",
      "0.237905secs: train iter *15*, train size 1 *15*, train size 2 *15*\n",
      "0.264152secs: train iter *16*, train size 1 *23*, train size 2 *60*\n",
      "0.263024secs: train iter *17*, train size 1 *4*, train size 2 *59*\n",
      "0.308931secs: train iter *18*, train size 1 *7*, train size 2 *84*\n",
      "0.239497secs: train iter *19*, train size 1 *21*, train size 2 *26*\n",
      "0.237061secs: train iter *20*, train size 1 *7*, train size 2 *7*\n",
      "0.237085secs: train iter *21*, train size 1 *8*, train size 2 *8*\n",
      "0.242560secs: train iter *22*, train size 1 *4*, train size 2 *22*\n",
      "0.393208secs: train iter *23*, train size 1 *42*, train size 2 *145*\n",
      "0.237165secs: train iter *24*, train size 1 *5*, train size 2 *12*\n",
      "0.237495secs: train iter *25*, train size 1 *4*, train size 2 *11*\n",
      "0.284252secs: train iter *26*, train size 1 *10*, train size 2 *76*\n",
      "0.269237secs: train iter *27*, train size 1 *22*, train size 2 *64*\n",
      "0.239077secs: train iter *28*, train size 1 *11*, train size 2 *22*\n",
      "0.244221secs: train iter *29*, train size 1 *30*, train size 2 *38*\n",
      "0.305588secs: train iter *30*, train size 1 *29*, train size 2 *83*\n",
      "0.237705secs: train iter *31*, train size 1 *6*, train size 2 *8*\n",
      "0.237508secs: train iter *32*, train size 1 *8*, train size 2 *14*\n",
      "0.247570secs: train iter *33*, train size 1 *4*, train size 2 *44*\n",
      "0.278255secs: train iter *34*, train size 1 *13*, train size 2 *70*\n",
      "0.275097secs: train iter *35*, train size 1 *11*, train size 2 *68*\n",
      "0.241370secs: train iter *36*, train size 1 *7*, train size 2 *27*\n",
      "0.242078secs: train iter *37*, train size 1 *6*, train size 2 *37*\n",
      "0.246786secs: train iter *38*, train size 1 *27*, train size 2 *44*\n",
      "0.491737secs: train iter *39*, train size 1 *9*, train size 2 *209*\n",
      "0.239332secs: train iter *40*, train size 1 *17*, train size 2 *23*\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-31964c638420>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plateau - 6 layers -0.0003 data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlinformer_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/deeplearning/project2/scripts/TalkTrain.py\u001b[0m in \u001b[0;36mlinformer_trainer\u001b[0;34m(model, train_data_iterator, train_options, test_data_iterator, test_options, optimizer, scheduler, scheduler_name)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;31m# print(\"src shape\", src.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/project2/linformer_pytorch/linformer_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/project2/linformer_pytorch/linformer_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/project2/linformer_pytorch/linformer_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/project2/linformer_pytorch/linformer_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/project2/linformer_pytorch/linformer_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_k\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_mode\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_k\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embeddings\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_mode\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embeddings\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_level\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"C2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0mhead_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#plateau - 6 layers -0.0003 data\n",
    "linformer_trainer(l_model, train_data_iter, train_opt, test_data_iter, test_opt, l_optimizer, l_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> a GPU was detected, model will be trained on GPU\n",
      "0.109582secs: train iter *1*, train size 1 *19*, train size 2 *74*\n",
      "0.068334secs: train iter *2*, train size 1 *6*, train size 2 *17*\n",
      "0.068595secs: train iter *3*, train size 1 *6*, train size 2 *10*\n",
      "0.068183secs: train iter *4*, train size 1 *11*, train size 2 *9*\n",
      "0.104889secs: train iter *5*, train size 1 *4*, train size 2 *71*\n",
      "0.143829secs: train iter *6*, train size 1 *11*, train size 2 *119*\n",
      "0.067831secs: train iter *7*, train size 1 *4*, train size 2 *15*\n",
      "0.068091secs: train iter *8*, train size 1 *9*, train size 2 *15*\n",
      "0.068008secs: train iter *9*, train size 1 *24*, train size 2 *25*\n",
      "0.068451secs: train iter *10*, train size 1 *9*, train size 2 *24*\n",
      "0.109009secs: train iter *11*, train size 1 *12*, train size 2 *76*\n",
      "0.088395secs: train iter *12*, train size 1 *42*, train size 2 *37*\n",
      "0.068309secs: train iter *13*, train size 1 *10*, train size 2 *23*\n",
      "0.123246secs: train iter *14*, train size 1 *18*, train size 2 *92*\n",
      "0.067446secs: train iter *15*, train size 1 *13*, train size 2 *19*\n",
      "0.068063secs: train iter *16*, train size 1 *7*, train size 2 *6*\n",
      "0.068048secs: train iter *17*, train size 1 *4*, train size 2 *8*\n",
      "0.086798secs: train iter *18*, train size 1 *23*, train size 2 *46*\n",
      "0.067490secs: train iter *19*, train size 1 *11*, train size 2 *20*\n",
      "0.096656secs: train iter *20*, train size 1 *50*, train size 2 *44*\n",
      "0.125916secs: train iter *21*, train size 1 *15*, train size 2 *95*\n",
      "0.068008secs: train iter *22*, train size 1 *4*, train size 2 *24*\n",
      "0.068417secs: train iter *23*, train size 1 *10*, train size 2 *10*\n",
      "0.068318secs: train iter *24*, train size 1 *10*, train size 2 *14*\n",
      "0.067213secs: train iter *25*, train size 1 *5*, train size 2 *20*\n",
      "0.068061secs: train iter *26*, train size 1 *14*, train size 2 *24*\n",
      "0.067849secs: train iter *27*, train size 1 *18*, train size 2 *10*\n",
      "0.094635secs: train iter *28*, train size 1 *36*, train size 2 *51*\n",
      "0.102086secs: train iter *29*, train size 1 *13*, train size 2 *66*\n",
      "0.186143secs: train iter *30*, train size 1 *198*, train size 2 *52*\n",
      "0.068039secs: train iter *31*, train size 1 *7*, train size 2 *16*\n",
      "0.068333secs: train iter *32*, train size 1 *7*, train size 2 *10*\n",
      "0.067238secs: train iter *33*, train size 1 *8*, train size 2 *18*\n",
      "0.084368secs: train iter *34*, train size 1 *24*, train size 2 *42*\n",
      "0.079553secs: train iter *35*, train size 1 *5*, train size 2 *36*\n",
      "0.102386secs: train iter *36*, train size 1 *20*, train size 2 *67*\n",
      "0.068497secs: train iter *37*, train size 1 *9*, train size 2 *10*\n",
      "0.068139secs: train iter *38*, train size 1 *7*, train size 2 *12*\n",
      "0.157494secs: train iter *39*, train size 1 *9*, train size 2 *130*\n",
      "0.067799secs: train iter *40*, train size 1 *12*, train size 2 *16*\n",
      "0.116502secs: train iter *41*, train size 1 *7*, train size 2 *83*\n",
      "0.068439secs: train iter *42*, train size 1 *10*, train size 2 *8*\n",
      "0.068248secs: train iter *43*, train size 1 *7*, train size 2 *8*\n",
      "0.068742secs: train iter *44*, train size 1 *8*, train size 2 *13*\n",
      "0.090822secs: train iter *45*, train size 1 *30*, train size 2 *46*\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5ccb4388b3af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformer_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/deeplearning/project2/scripts/TalkTrain.py\u001b[0m in \u001b[0;36mtransformer_trainer\u001b[0;34m(model, train_data_iterator, train_options, test_data_iterator, test_options, optimizer, scheduler, scheduler_name)\u001b[0m\n\u001b[1;32m    291\u001b[0m             train_batch_loss = F.cross_entropy(preds.view(-1, preds.size(-1)), \n\u001b[1;32m    292\u001b[0m                                          ys, ignore_index = train_options.trg_pad)\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0mtrain_batch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mtrain_total_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_batch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "transformer_trainer(t_model, train_data_iter, train_opt, test_data_iter, test_opt, t_optimizer, t_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, train_data_iterator, train_options, test_data_iterator, test_options, optimizer, scheduler, scheduler_name):\n",
    "\n",
    "    if torch.cuda.is_available() and train_options.device == torch.device(\"cuda\"):\n",
    "        print(\"a GPU was detected, model will be trained on GPU\")\n",
    "        model = model.cuda()\n",
    "    else:\n",
    "        print(\"training on cpu\")\n",
    "\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    best_loss = 100\n",
    "    \n",
    "    iters = len(train_data_iterator)\n",
    "    for epoch in range(train_options.epochs):\n",
    "        train_total_loss = 0\n",
    "        for i, batch in enumerate(train_data_iterator): \n",
    "            if i > 0: raise KeyboardInterrupt\n",
    "            src = batch.listen.transpose(0,1)\n",
    "#             print(\"src\", src)\n",
    "            trg = batch.reply.transpose(0,1)\n",
    "#             print(\"src\", src.shape)\n",
    "#             print(\"trg\", trg)\n",
    "#             print(\"trg\", trg.shape)\n",
    "            trg_input = trg[:, :-1]\n",
    "#             print(\"trg_input shape\", trg_input.shape)\n",
    "#             print(\"trg_input\", trg_input)\n",
    "        \n",
    "            src_mask, trg_mask = create_masks(src, trg_input, train_options)\n",
    "#             print(\"src_mask\", src_mask)\n",
    "#             print(\"trg_mask\", trg_mask)\n",
    "            preds = model(src, src_mask, trg_input, trg_mask)\n",
    "            print(preds)\n",
    "            print(\"pred :\",preds)\n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            print(\"ys shape\", ys.shape)\n",
    "            print(\"preds shape1\", preds.shape)\n",
    "#             print(\"preds view:\", preds.view(-1, preds.size(-1)))\n",
    "            print(\"preds shape2\", preds.view(-1, preds.size(-1)).shape)\n",
    "            optimizer.zero_grad()\n",
    "            train_batch_loss = F.cross_entropy(preds.view(-1, preds.size(-1)), \n",
    "                                         ys, ignore_index = train_options.trg_pad)\n",
    "            train_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_total_loss += train_batch_loss.item()\n",
    "            if scheduler_name == \"cosine\":\n",
    "                #print(\"cosine\")\n",
    "                scheduler.step(epoch + i / iters)\n",
    "            if scheduler_name == \"warmup\": \n",
    "                scheduler.step()\n",
    "            print(\"batch loss\", train_batch_loss)\n",
    "            \n",
    "            \n",
    "\n",
    "        train_epoch_loss = train_total_loss/(num_batches(train_data_iterator)+1)\n",
    "\n",
    "        model.eval()\n",
    "        test_total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(test_data_iterator): \n",
    "                src = batch.listen.transpose(0,1)\n",
    "                trg = batch.reply.transpose(0,1)\n",
    "                trg_input = trg[:, :-1]\n",
    "                src_mask, trg_mask = create_masks(src, trg_input, test_options)\n",
    "                preds = model(src, src_mask, trg_input, trg_mask)\n",
    "                ys = trg[:, 1:].contiguous().view(-1)\n",
    "                test_batch_loss = F.cross_entropy(preds.view(-1, preds.size(-1)), \n",
    "                                             ys, ignore_index = test_options.trg_pad)\n",
    "                test_total_loss += test_batch_loss.item()\n",
    "\n",
    "            test_epoch_loss = test_total_loss/(num_batches(test_data_iterator)+1)\n",
    "        \n",
    "        model.train()\n",
    "        if scheduler_name == \"plateau\":\n",
    "            scheduler.step(train_epoch_loss)\n",
    "        # scheduler.step(test_epoch_loss)\n",
    "        \n",
    "\n",
    "        # if train_epoch_loss < best_loss:\n",
    "        #     best_loss = train_epoch_loss\n",
    "        #     torch.save(model.state_dict(), train_options.save_path)\n",
    "        if test_epoch_loss < best_loss:\n",
    "            best_loss = test_epoch_loss\n",
    "            torch.save(model.state_dict(), train_options.save_path)\n",
    "        print(\"%.3fm: train epoch *%d*, loss = *%.3f*\" %((time.time() - start)//60, epoch, train_epoch_loss), end=\", \")\n",
    "        print(\"%.3fm: test epoch *%d*, loss = *%.3f*, best loss = *%.3f*\" %((time.time() - start)//60, epoch, test_epoch_loss, best_loss) , flush=True)\n",
    "        train_total_loss = 0\n",
    "        test_total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a GPU was detected, model will be trained on GPU\n",
      "tensor([[[-0.1844, -0.5469, -0.0845,  ...,  1.2630, -0.0519,  0.7461],\n",
      "         [-0.3253, -0.6089,  0.7874,  ..., -0.8720, -0.6310,  0.5250],\n",
      "         [-0.2574, -0.3602,  0.4945,  ...,  0.7115,  0.6852, -0.5288],\n",
      "         ...,\n",
      "         [ 0.4672, -0.7800, -0.1505,  ...,  0.1785,  0.6682, -1.2653],\n",
      "         [-0.0376, -1.2615,  0.3215,  ...,  0.3061,  1.0878, -1.2897],\n",
      "         [ 0.1071, -1.1693,  0.0674,  ...,  0.6054,  0.9830, -1.5651]],\n",
      "\n",
      "        [[-0.2895, -0.4937,  0.2230,  ...,  1.2742, -0.3471,  0.7206],\n",
      "         [-0.1099, -0.4696,  0.2041,  ..., -0.5249, -0.6263,  0.4733],\n",
      "         [-0.2040, -1.0922,  0.5240,  ...,  0.9412, -0.4860,  0.1810],\n",
      "         ...,\n",
      "         [ 0.0442, -1.0805,  0.2585,  ...,  0.6251,  0.6942, -1.3362],\n",
      "         [ 0.1210, -1.5012,  0.4226,  ...,  0.5150,  1.1843, -1.6243],\n",
      "         [ 0.4514, -1.1460,  0.3091,  ...,  0.4535,  1.0736, -1.5753]],\n",
      "\n",
      "        [[-0.5722, -0.4208, -0.3433,  ...,  1.3380, -0.2040,  0.6336],\n",
      "         [ 0.1324,  0.3182,  0.1105,  ...,  0.0544, -0.5431,  0.2941],\n",
      "         [ 0.3642, -0.6918, -0.1763,  ..., -0.0784, -0.6222,  0.6757],\n",
      "         ...,\n",
      "         [-0.0199, -1.0707,  0.0729,  ...,  0.4723,  1.1299, -1.4219],\n",
      "         [ 0.2494, -0.9698,  0.1629,  ...,  0.3623,  1.0892, -1.6201],\n",
      "         [ 0.2482, -1.0747,  0.3113,  ...,  0.1292,  1.0257, -1.3177]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3197, -0.2275, -0.3832,  ...,  1.1723, -0.0918,  0.7932],\n",
      "         [ 0.0380, -0.1658, -0.5725,  ..., -0.3140,  0.0423,  0.2159],\n",
      "         [ 1.3310, -0.3201,  0.1802,  ...,  0.1808,  0.5447,  0.0991],\n",
      "         ...,\n",
      "         [ 0.1388, -1.0532, -0.1021,  ...,  0.2405,  0.8538, -1.2312],\n",
      "         [ 0.2002, -1.3598,  0.0372,  ...,  0.4625,  0.9690, -1.4265],\n",
      "         [ 0.0590, -1.1432,  0.0455,  ...,  0.3023,  1.0253, -1.4029]],\n",
      "\n",
      "        [[-0.3781, -0.1796, -0.3367,  ...,  1.4314, -0.3575,  0.3540],\n",
      "         [ 1.0346, -0.6394, -0.0686,  ..., -0.0944,  0.4653,  0.3459],\n",
      "         [-0.0408, -0.6340, -0.0998,  ...,  1.1079,  0.4737, -0.0055],\n",
      "         ...,\n",
      "         [ 0.2840, -0.7151,  0.0342,  ...,  0.6290,  0.8020, -1.2119],\n",
      "         [ 0.1870, -1.0298,  0.1665,  ...,  0.0680,  0.8768, -1.1457],\n",
      "         [ 0.0926, -1.1404, -0.1185,  ...,  0.2683,  0.8976, -1.0574]],\n",
      "\n",
      "        [[-0.1499, -0.7774,  0.2527,  ...,  1.2262, -0.2868,  0.7801],\n",
      "         [-0.1553,  0.8737, -0.8148,  ...,  0.4697,  1.0714,  0.1205],\n",
      "         [ 0.3391,  0.0116, -0.2966,  ..., -0.1615,  0.0592, -1.1918],\n",
      "         ...,\n",
      "         [ 0.0735, -1.3960,  0.1441,  ...,  0.4814,  1.1720, -1.2887],\n",
      "         [ 0.2993, -1.0479,  0.1884,  ...,  0.4714,  1.0894, -1.3018],\n",
      "         [ 0.0877, -1.0695, -0.2930,  ...,  0.4051,  1.0394, -1.2845]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "pred : tensor([[[-0.1844, -0.5469, -0.0845,  ...,  1.2630, -0.0519,  0.7461],\n",
      "         [-0.3253, -0.6089,  0.7874,  ..., -0.8720, -0.6310,  0.5250],\n",
      "         [-0.2574, -0.3602,  0.4945,  ...,  0.7115,  0.6852, -0.5288],\n",
      "         ...,\n",
      "         [ 0.4672, -0.7800, -0.1505,  ...,  0.1785,  0.6682, -1.2653],\n",
      "         [-0.0376, -1.2615,  0.3215,  ...,  0.3061,  1.0878, -1.2897],\n",
      "         [ 0.1071, -1.1693,  0.0674,  ...,  0.6054,  0.9830, -1.5651]],\n",
      "\n",
      "        [[-0.2895, -0.4937,  0.2230,  ...,  1.2742, -0.3471,  0.7206],\n",
      "         [-0.1099, -0.4696,  0.2041,  ..., -0.5249, -0.6263,  0.4733],\n",
      "         [-0.2040, -1.0922,  0.5240,  ...,  0.9412, -0.4860,  0.1810],\n",
      "         ...,\n",
      "         [ 0.0442, -1.0805,  0.2585,  ...,  0.6251,  0.6942, -1.3362],\n",
      "         [ 0.1210, -1.5012,  0.4226,  ...,  0.5150,  1.1843, -1.6243],\n",
      "         [ 0.4514, -1.1460,  0.3091,  ...,  0.4535,  1.0736, -1.5753]],\n",
      "\n",
      "        [[-0.5722, -0.4208, -0.3433,  ...,  1.3380, -0.2040,  0.6336],\n",
      "         [ 0.1324,  0.3182,  0.1105,  ...,  0.0544, -0.5431,  0.2941],\n",
      "         [ 0.3642, -0.6918, -0.1763,  ..., -0.0784, -0.6222,  0.6757],\n",
      "         ...,\n",
      "         [-0.0199, -1.0707,  0.0729,  ...,  0.4723,  1.1299, -1.4219],\n",
      "         [ 0.2494, -0.9698,  0.1629,  ...,  0.3623,  1.0892, -1.6201],\n",
      "         [ 0.2482, -1.0747,  0.3113,  ...,  0.1292,  1.0257, -1.3177]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.3197, -0.2275, -0.3832,  ...,  1.1723, -0.0918,  0.7932],\n",
      "         [ 0.0380, -0.1658, -0.5725,  ..., -0.3140,  0.0423,  0.2159],\n",
      "         [ 1.3310, -0.3201,  0.1802,  ...,  0.1808,  0.5447,  0.0991],\n",
      "         ...,\n",
      "         [ 0.1388, -1.0532, -0.1021,  ...,  0.2405,  0.8538, -1.2312],\n",
      "         [ 0.2002, -1.3598,  0.0372,  ...,  0.4625,  0.9690, -1.4265],\n",
      "         [ 0.0590, -1.1432,  0.0455,  ...,  0.3023,  1.0253, -1.4029]],\n",
      "\n",
      "        [[-0.3781, -0.1796, -0.3367,  ...,  1.4314, -0.3575,  0.3540],\n",
      "         [ 1.0346, -0.6394, -0.0686,  ..., -0.0944,  0.4653,  0.3459],\n",
      "         [-0.0408, -0.6340, -0.0998,  ...,  1.1079,  0.4737, -0.0055],\n",
      "         ...,\n",
      "         [ 0.2840, -0.7151,  0.0342,  ...,  0.6290,  0.8020, -1.2119],\n",
      "         [ 0.1870, -1.0298,  0.1665,  ...,  0.0680,  0.8768, -1.1457],\n",
      "         [ 0.0926, -1.1404, -0.1185,  ...,  0.2683,  0.8976, -1.0574]],\n",
      "\n",
      "        [[-0.1499, -0.7774,  0.2527,  ...,  1.2262, -0.2868,  0.7801],\n",
      "         [-0.1553,  0.8737, -0.8148,  ...,  0.4697,  1.0714,  0.1205],\n",
      "         [ 0.3391,  0.0116, -0.2966,  ..., -0.1615,  0.0592, -1.1918],\n",
      "         ...,\n",
      "         [ 0.0735, -1.3960,  0.1441,  ...,  0.4814,  1.1720, -1.2887],\n",
      "         [ 0.2993, -1.0479,  0.1884,  ...,  0.4714,  1.0894, -1.3018],\n",
      "         [ 0.0877, -1.0695, -0.2930,  ...,  0.4051,  1.0394, -1.2845]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "ys shape torch.Size([21376])\n",
      "preds shape1 torch.Size([128, 167, 30876])\n",
      "preds shape2 torch.Size([21376, 30876])\n",
      "batch loss tensor(10.5180, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-15c1d5ce4b89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-107672da262c>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(model, train_data_iterator, train_options, test_data_iterator, test_options, optimizer, scheduler, scheduler_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtrain_total_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#             print(\"src\", src)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer(t_model, train_data_iter, train_opt, test_data_iter, test_opt, t_optimizer, t_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_opt.src_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer2(model, train_data_iterator, train_options, test_data_iterator, test_options, optimizer, scheduler, scheduler_name):\n",
    "\n",
    "    if torch.cuda.is_available() and train_options.device == torch.device(\"cuda\"):\n",
    "        print(\"==> a GPU was detected, model will be trained on GPU\")\n",
    "        model = model.cuda()\n",
    "    else:\n",
    "        print(\"==> training on cpu\")\n",
    "\n",
    "    model.train()\n",
    "    start = time.monotonic()\n",
    "\n",
    "    best_loss = 100\n",
    "    iters = len(train_data_iterator)\n",
    "    for epoch in range(train_options.epochs):\n",
    "        train_total_loss = 0\n",
    "        for i, batch in enumerate(train_data_iterator):\n",
    "            print(f\" Batch {i} of {len(train_data_iterator)}\", end=\"\")\n",
    "            each_iter = time.monotonic()\n",
    "            src = batch.listen.transpose(0,1)\n",
    "            trg = batch.reply.transpose(0,1)\n",
    "#             print(\"trg \", trg)\n",
    "#             trg = trg[:, :-1]\n",
    "#             print(\"trg\", trg)\n",
    "#             print(\"normal src\", src)\n",
    "#             print(\"normal trg\", trg)\n",
    "#             print(\"src shape\", src.shape)\n",
    "#             print(\"trg shape\", trg.shape)\n",
    "#             print(\"src len: \", len(src))\n",
    "#             print(\"trg len: \", len(trg))\n",
    "            _, src_seq_len = src.shape\n",
    "            if src_seq_len < train_options.max_len:\n",
    "                src_diff = abs(src_seq_len - train_options.max_len)\n",
    "                src = torch.nn.functional.pad(input=src, pad=(0,src_diff,0,0), mode='constant', value=train_options.trg_pad)\n",
    "            elif src_seq_len > train_options.max_len:\n",
    "#                 src = src.narrow(1,0,train_options.max_len)\n",
    "#                 print(\"narrow src shape\", src.shape)\n",
    "#                 print(\"removed batch size too long\")\n",
    "                continue\n",
    "            _, trg_seq_len = trg.shape\n",
    "            if trg_seq_len < train_options.max_len:\n",
    "                trg_diff = abs(trg_seq_len - train_options.max_len)\n",
    "                trg = torch.nn.functional.pad(input=trg, pad=(0,trg_diff,0,0), mode='constant', value=train_options.trg_pad)\n",
    "            elif trg_seq_len > train_options.max_len:\n",
    "#                 trg = trg.narrow(1,0,train_options.max_len)\n",
    "#                 print(\"narrow trg shape\", trg.shape) \n",
    "#                 print(\"removed batch size too long\")\n",
    "                continue\n",
    "#             print(\"normal src\", src)\n",
    "#             print(\"padded trg\", trg)\n",
    "#             print(\"src shape\", src.shape)\n",
    "#             print(\"trg shape\", trg.shape)            \n",
    "#             diff = abs(len(batch.listen) - len(batch.reply))\n",
    "#             if len(batch.listen) > len(batch.reply):\n",
    "#                 trg = torch.nn.functional.pad(input=trg, pad=(0,0,0,diff), mode='constant', value=train_options.trg_pad)\n",
    "#             elif len(batch.listen) < len(batch.reply):\n",
    "#                 src = torch.nn.functional.pad(input=src, pad=(0,0,0,diff), mode='constant', value=train_options.src_pad)\n",
    "                \n",
    "            # print(\"src shape\", src.shape)\n",
    "            # if(diff>0):  \n",
    "            #     print(\"src pad\", src)\n",
    "            #     print(\"trg pad\", trg)\n",
    "            #     new_src = batch.listen.transpose(0,1)\n",
    "            #     new_trg = batch.reply.transpose(0,1)\n",
    "            #     new_trg_input = trg[:, :-1]\n",
    "            #     src_mark, trg_mask = create_masks(new_src,new_trg_input,train_options)\n",
    "            #     print(\"src mark\", src_mark)\n",
    "            #     print(\"trg_mask\", trg_mark)\n",
    "            \n",
    "#             diff = abs(train_options.batchsize - src.shape[1])\n",
    "#             if train_options.batchsize > src.shape[1]:\n",
    "#                 src = torch.nn.functional.pad(input=src, pad=(0,diff,0,0), mode='constant', value=train_options.src_pad)\n",
    "#             if train_options.batchsize > trg.shape[1]:\n",
    "#                 trg = torch.nn.functional.pad(input=trg, pad=(0,diff,0,0), mode='constant', value=train_options.trg_pad)\n",
    "                \n",
    "#             print(\"src shape\", src.shape)\n",
    "\n",
    "            preds = model(src,trg)\n",
    "            \n",
    "#             print(\"preds :\", preds)\n",
    "#             print(\"trg[:, 1;]\", trg[:,1:].shape)\n",
    "            ys = trg.contiguous().view(-1)\n",
    "#             print(\"ys1 shape:\", ys.shape)\n",
    "#             print(\"ys1:\", ys)\n",
    "#             ys = trg[:, 1:].contiguous().view(-1)\n",
    "#             print(\"ys shape2:\", ys.shape)\n",
    "#             print(\"ys2:\", ys)\n",
    "#             print(\"yes :\", ys)\n",
    "#             preds = preds.contiguous().view(-1, preds.size(-1))\n",
    "                           \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "#             print(\"preds size -1 :\", preds.size(-1))\n",
    "#             print(\"preds size [:, 1:] :\", preds[:,1:].shape)\n",
    "            preds = preds.view(-1, preds.size(-1))\n",
    "#             print(\"preds shape1 :\", preds.shape)\n",
    "#             print(\"preds :\", preds)\n",
    "#             preds = preds[:, 1:].view(-1, preds.size(-1))\n",
    "#             print(\"preds shape2 :\", preds.shape)\n",
    "    \n",
    "            \n",
    "            train_batch_loss = F.cross_entropy(preds, ys, ignore_index = train_options.trg_pad)\n",
    "            train_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_total_loss += train_batch_loss.item()\n",
    "            if scheduler_name == \"cosine\":\n",
    "                scheduler.step(epoch + i / iters)\n",
    "            if scheduler_name == \"warmup\": \n",
    "                scheduler.step()\n",
    "#             print(\"batch loss\", train_batch_loss)\n",
    "#             print(\"%.6fsecs: train iter *%d*, train size 1 *%d*, train size 2 *%d*\"  %(time.monotonic() - each_iter, i+1,len(batch.listen),len(batch.reply)))\n",
    "            \n",
    "        if scheduler_name == \"warmup\":\n",
    "            scheduler.print_lr(epoch+i)\n",
    "        train_epoch_loss = train_total_loss/(num_batches(train_data_iterator)+1)\n",
    "\n",
    "        if scheduler_name == \"plateau\": \n",
    "            scheduler.step(train_epoch_loss) \n",
    "\n",
    "        model.eval()\n",
    "        test_total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(test_data_iterator): \n",
    "                src = batch.listen.transpose(0,1)\n",
    "                trg = batch.reply.transpose(0,1)\n",
    "                trg = trg[:, :-1]\n",
    "                _, src_seq_len = src.shape\n",
    "                if src_seq_len < train_options.max_len:\n",
    "                    src_diff = abs(src_seq_len - test_options.max_len)\n",
    "                    src = torch.nn.functional.pad(input=src, pad=(0,src_diff,0,0), mode='constant', value=test_options.trg_pad)\n",
    "                elif src_seq_len > train_options.max_len:\n",
    "#                     src = src.narrow(1,0,train_options.max_len)\n",
    "                  continue\n",
    "                  \n",
    "                _, trg_seq_len = trg.shape\n",
    "                if trg_seq_len < train_options.max_len:\n",
    "                    trg_diff = abs(trg_seq_len - test_options.max_len)\n",
    "                    trg = torch.nn.functional.pad(input=trg, pad=(0,trg_diff,0,0), mode='constant', value=test_options.trg_pad)\n",
    "                elif trg_seq_len > train_options.max_len:\n",
    "#                     trg = trg.narrow(1,0,train_options.max_len)\n",
    "                  continue\n",
    "#                 diff = abs(len(batch.listen) - len(batch.reply))\n",
    "#                 if len(batch.listen) > len(batch.reply):\n",
    "#                     trg = torch.nn.functional.pad(input=trg, pad=(0,0,0,diff), mode='constant', value=test_options.trg_pad)\n",
    "#                 elif len(batch.listen) < len(batch.reply):\n",
    "#                     src = torch.nn.functional.pad(input=src, pad=(0,0,0,diff), mode='constant', value=test_options.src_pad)\n",
    "#                 # print(\"src shape\", src.shape) \n",
    "#                 diff = abs(test_options.batchsize - src.shape[1])\n",
    "#                 if test_options.batchsize > src.shape[1]:\n",
    "#                     src = torch.nn.functional.pad(input=src, pad=(0,diff,0,0), mode='constant', value=test_options.src_pad)\n",
    "#                 if test_options.batchsize > trg.shape[1]:\n",
    "#                     trg = torch.nn.functional.pad(input=trg, pad=(0,diff,0,0), mode='constant', value=test_options.trg_pad)\n",
    "                # print(\"src shape\", src.shape)   \n",
    "                         \n",
    "                preds = model(src,trg)\n",
    "#                 ys = trg[:, 1:].contiguous().view(-1)\n",
    "#                 print(\"ys1 : \", ys)\n",
    "                ys = trg.contiguous().view(-1)\n",
    "#                 print(\"ys2 : \", ys)               \n",
    "                preds = preds.view(-1, preds.size(-1))\n",
    "#                 preds = preds.contiguous().view(-1, preds.size(-1))\n",
    "                test_batch_loss = F.cross_entropy(preds, ys, ignore_index = test_options.trg_pad)\n",
    "                test_total_loss += test_batch_loss.item()\n",
    "\n",
    "            test_epoch_loss = test_total_loss/(num_batches(test_data_iterator)+1)\n",
    "\n",
    "        # if scheduler_name == \"plateau\": \n",
    "        #     scheduler.step(test_epoch_loss) \n",
    "\n",
    "        model.train()\n",
    "\n",
    "        # scheduler.step()\n",
    "\n",
    "        if train_epoch_loss < best_loss:\n",
    "            best_loss = train_epoch_loss\n",
    "            torch.save(model.state_dict(), train_options.save_path)\n",
    "        # if test_epoch_loss < best_loss:\n",
    "        #     best_loss = test_epoch_loss\n",
    "        #     torch.save(model.state_dict(), train_options.save_path)\n",
    "        print(\"%.3fm: train epoch *%d*, loss = *%.3f*\" %((time.monotonic() - start)/60, epoch+1, train_epoch_loss), end=\", \")\n",
    "        print(\"%.3fm: test epoch *%d*, loss = *%.3f*, best loss = *%.3f*\" %((time.monotonic() - start)/60, epoch+1, test_epoch_loss, best_loss) , flush=True)\n",
    "        train_total_loss = 0\n",
    "        test_total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> a GPU was detected, model will be trained on GPU\n",
      " Batch 0 of 1556 Batch 1 of 1556 Batch 2 of 1556 Batch 3 of 1556 Batch 4 of 1556 Batch 5 of 1556 Batch 6 of 1556 Batch 7 of 1556 Batch 8 of 1556 Batch 9 of 1556 Batch 10 of 1556 Batch 11 of 1556 Batch 12 of 1556 Batch 13 of 1556 Batch 14 of 1556 Batch 15 of 1556 Batch 16 of 1556 Batch 17 of 1556 Batch 18 of 1556 Batch 19 of 1556 Batch 20 of 1556 Batch 21 of 1556 Batch 22 of 1556 Batch 23 of 1556 Batch 24 of 1556 Batch 25 of 1556 Batch 26 of 1556 Batch 27 of 1556 Batch 28 of 1556 Batch 29 of 1556 Batch 30 of 1556 Batch 31 of 1556 Batch 32 of 1556 Batch 33 of 1556 Batch 34 of 1556 Batch 35 of 1556 Batch 36 of 1556 Batch 37 of 1556 Batch 38 of 1556 Batch 39 of 1556 Batch 40 of 1556 Batch 41 of 1556 Batch 42 of 1556 Batch 43 of 1556 Batch 44 of 1556 Batch 45 of 1556 Batch 46 of 1556 Batch 47 of 1556 Batch 48 of 1556 Batch 49 of 1556 Batch 50 of 1556 Batch 51 of 1556 Batch 52 of 1556 Batch 53 of 1556 Batch 54 of 1556 Batch 55 of 1556 Batch 56 of 1556 Batch 57 of 1556 Batch 58 of 1556 Batch 59 of 1556 Batch 60 of 1556 Batch 61 of 1556 Batch 62 of 1556 Batch 63 of 1556 Batch 64 of 1556 Batch 65 of 1556 Batch 66 of 1556 Batch 67 of 1556 Batch 68 of 1556 Batch 69 of 1556 Batch 70 of 1556 Batch 71 of 1556 Batch 72 of 1556 Batch 73 of 1556 Batch 74 of 1556 Batch 75 of 1556 Batch 76 of 1556 Batch 77 of 1556 Batch 78 of 1556 Batch 79 of 1556 Batch 80 of 1556 Batch 81 of 1556 Batch 82 of 1556 Batch 83 of 1556 Batch 84 of 1556 Batch 85 of 1556 Batch 86 of 1556 Batch 87 of 1556 Batch 88 of 1556 Batch 89 of 1556 Batch 90 of 1556 Batch 91 of 1556 Batch 92 of 1556 Batch 93 of 1556 Batch 94 of 1556 Batch 95 of 1556 Batch 96 of 1556 Batch 97 of 1556 Batch 98 of 1556 Batch 99 of 1556 Batch 100 of 1556 Batch 101 of 1556 Batch 102 of 1556 Batch 103 of 1556 Batch 104 of 1556 Batch 105 of 1556 Batch 106 of 1556 Batch 107 of 1556 Batch 108 of 1556 Batch 109 of 1556 Batch 110 of 1556 Batch 111 of 1556 Batch 112 of 1556 Batch 113 of 1556 Batch 114 of 1556 Batch 115 of 1556 Batch 116 of 1556 Batch 117 of 1556 Batch 118 of 1556 Batch 119 of 1556 Batch 120 of 1556 Batch 121 of 1556 Batch 122 of 1556 Batch 123 of 1556 Batch 124 of 1556 Batch 125 of 1556 Batch 126 of 1556 Batch 127 of 1556 Batch 128 of 1556 Batch 129 of 1556 Batch 130 of 1556 Batch 131 of 1556 Batch 132 of 1556 Batch 133 of 1556 Batch 134 of 1556 Batch 135 of 1556 Batch 136 of 1556 Batch 137 of 1556 Batch 138 of 1556 Batch 139 of 1556 Batch 140 of 1556 Batch 141 of 1556 Batch 142 of 1556 Batch 143 of 1556 Batch 144 of 1556 Batch 145 of 1556 Batch 146 of 1556 Batch 147 of 1556 Batch 148 of 1556 Batch 149 of 1556 Batch 150 of 1556 Batch 151 of 1556 Batch 152 of 1556 Batch 153 of 1556 Batch 154 of 1556 Batch 155 of 1556 Batch 156 of 1556 Batch 157 of 1556 Batch 158 of 1556 Batch 159 of 1556 Batch 160 of 1556 Batch 161 of 1556 Batch 162 of 1556 Batch 163 of 1556 Batch 164 of 1556 Batch 165 of 1556 Batch 166 of 1556 Batch 167 of 1556 Batch 168 of 1556 Batch 169 of 1556 Batch 170 of 1556 Batch 171 of 1556 Batch 172 of 1556 Batch 173 of 1556 Batch 174 of 1556 Batch 175 of 1556 Batch 176 of 1556 Batch 177 of 1556 Batch 178 of 1556 Batch 179 of 1556 Batch 180 of 1556 Batch 181 of 1556 Batch 182 of 1556 Batch 183 of 1556 Batch 184 of 1556 Batch 185 of 1556 Batch 186 of 1556 Batch 187 of 1556 Batch 188 of 1556 Batch 189 of 1556 Batch 190 of 1556 Batch 191 of 1556 Batch 192 of 1556 Batch 193 of 1556 Batch 194 of 1556 Batch 195 of 1556 Batch 196 of 1556 Batch 197 of 1556 Batch 198 of 1556 Batch 199 of 1556 Batch 200 of 1556 Batch 201 of 1556 Batch 202 of 1556 Batch 203 of 1556 Batch 204 of 1556 Batch 205 of 1556 Batch 206 of 1556 Batch 207 of 1556 Batch 208 of 1556 Batch 209 of 1556 Batch 210 of 1556 Batch 211 of 1556 Batch 212 of 1556 Batch 213 of 1556 Batch 214 of 1556 Batch 215 of 1556 Batch 216 of 1556 Batch 217 of 1556 Batch 218 of 1556 Batch 219 of 1556 Batch 220 of 1556 Batch 221 of 1556 Batch 222 of 1556 Batch 223 of 1556 Batch 224 of 1556 Batch 225 of 1556 Batch 226 of 1556 Batch 227 of 1556 Batch 228 of 1556 Batch 229 of 1556 Batch 230 of 1556 Batch 231 of 1556 Batch 232 of 1556 Batch 233 of 1556 Batch 234 of 1556 Batch 235 of 1556 Batch 236 of 1556 Batch 237 of 1556 Batch 238 of 1556 Batch 239 of 1556 Batch 240 of 1556 Batch 241 of 1556 Batch 242 of 1556 Batch 243 of 1556 Batch 244 of 1556 Batch 245 of 1556 Batch 246 of 1556 Batch 247 of 1556 Batch 248 of 1556 Batch 249 of 1556 Batch 250 of 1556 Batch 251 of 1556 Batch 252 of 1556 Batch 253 of 1556 Batch 254 of 1556 Batch 255 of 1556 Batch 256 of 1556 Batch 257 of 1556 Batch 258 of 1556 Batch 259 of 1556 Batch 260 of 1556 Batch 261 of 1556 Batch 262 of 1556 Batch 263 of 1556 Batch 264 of 1556 Batch 265 of 1556 Batch 266 of 1556 Batch 267 of 1556 Batch 268 of 1556 Batch 269 of 1556 Batch 270 of 1556 Batch 271 of 1556 Batch 272 of 1556 Batch 273 of 1556 Batch 274 of 1556 Batch 275 of 1556 Batch 276 of 1556 Batch 277 of 1556 Batch 278 of 1556 Batch 279 of 1556 Batch 280 of 1556 Batch 281 of 1556 Batch 282 of 1556 Batch 283 of 1556 Batch 284 of 1556 Batch 285 of 1556 Batch 286 of 1556 Batch 287 of 1556 Batch 288 of 1556 Batch 289 of 1556 Batch 290 of 1556 Batch 291 of 1556 Batch 292 of 1556 Batch 293 of 1556 Batch 294 of 1556 Batch 295 of 1556 Batch 296 of 1556 Batch 297 of 1556 Batch 298 of 1556 Batch 299 of 1556 Batch 300 of 1556 Batch 301 of 1556 Batch 302 of 1556 Batch 303 of 1556 Batch 304 of 1556 Batch 305 of 1556 Batch 306 of 1556 Batch 307 of 1556 Batch 308 of 1556 Batch 309 of 1556 Batch 310 of 1556 Batch 311 of 1556 Batch 312 of 1556 Batch 313 of 1556 Batch 314 of 1556 Batch 315 of 1556 Batch 316 of 1556 Batch 317 of 1556 Batch 318 of 1556 Batch 319 of 1556 Batch 320 of 1556 Batch 321 of 1556 Batch 322 of 1556 Batch 323 of 1556 Batch 324 of 1556 Batch 325 of 1556 Batch 326 of 1556 Batch 327 of 1556 Batch 328 of 1556 Batch 329 of 1556 Batch 330 of 1556 Batch 331 of 1556 Batch 332 of 1556 Batch 333 of 1556 Batch 334 of 1556 Batch 335 of 1556 Batch 336 of 1556 Batch 337 of 1556 Batch 338 of 1556 Batch 339 of 1556 Batch 340 of 1556 Batch 341 of 1556 Batch 342 of 1556 Batch 343 of 1556 Batch 344 of 1556 Batch 345 of 1556 Batch 346 of 1556 Batch 347 of 1556 Batch 348 of 1556 Batch 349 of 1556 Batch 350 of 1556 Batch 351 of 1556 Batch 352 of 1556 Batch 353 of 1556 Batch 354 of 1556 Batch 355 of 1556 Batch 356 of 1556 Batch 357 of 1556 Batch 358 of 1556 Batch 359 of 1556 Batch 360 of 1556 Batch 361 of 1556 Batch 362 of 1556 Batch 363 of 1556 Batch 364 of 1556 Batch 365 of 1556 Batch 366 of 1556 Batch 367 of 1556 Batch 368 of 1556 Batch 369 of 1556 Batch 370 of 1556 Batch 371 of 1556 Batch 372 of 1556 Batch 373 of 1556 Batch 374 of 1556 Batch 375 of 1556 Batch 376 of 1556 Batch 377 of 1556 Batch 378 of 1556 Batch 379 of 1556 Batch 380 of 1556 Batch 381 of 1556 Batch 382 of 1556 Batch 383 of 1556 Batch 384 of 1556 Batch 385 of 1556 Batch 386 of 1556 Batch 387 of 1556 Batch 388 of 1556 Batch 389 of 1556 Batch 390 of 1556 Batch 391 of 1556 Batch 392 of 1556 Batch 393 of 1556 Batch 394 of 1556 Batch 395 of 1556 Batch 396 of 1556 Batch 397 of 1556 Batch 398 of 1556 Batch 399 of 1556 Batch 400 of 1556 Batch 401 of 1556 Batch 402 of 1556 Batch 403 of 1556 Batch 404 of 1556 Batch 405 of 1556 Batch 406 of 1556 Batch 407 of 1556 Batch 408 of 1556 Batch 409 of 1556 Batch 410 of 1556 Batch 411 of 1556 Batch 412 of 1556 Batch 413 of 1556 Batch 414 of 1556 Batch 415 of 1556 Batch 416 of 1556 Batch 417 of 1556 Batch 418 of 1556 Batch 419 of 1556 Batch 420 of 1556 Batch 421 of 1556 Batch 422 of 1556 Batch 423 of 1556 Batch 424 of 1556 Batch 425 of 1556 Batch 426 of 1556 Batch 427 of 1556 Batch 428 of 1556 Batch 429 of 1556 Batch 430 of 1556 Batch 431 of 1556 Batch 432 of 1556 Batch 433 of 1556 Batch 434 of 1556 Batch 435 of 1556 Batch 436 of 1556 Batch 437 of 1556 Batch 438 of 1556 Batch 439 of 1556 Batch 440 of 1556 Batch 441 of 1556 Batch 442 of 1556 Batch 443 of 1556 Batch 444 of 1556 Batch 445 of 1556 Batch 446 of 1556 Batch 447 of 1556 Batch 448 of 1556 Batch 449 of 1556 Batch 450 of 1556 Batch 451 of 1556 Batch 452 of 1556 Batch 453 of 1556 Batch 454 of 1556 Batch 455 of 1556 Batch 456 of 1556 Batch 457 of 1556 Batch 458 of 1556 Batch 459 of 1556 Batch 460 of 1556 Batch 461 of 1556 Batch 462 of 1556 Batch 463 of 1556 Batch 464 of 1556 Batch 465 of 1556 Batch 466 of 1556 Batch 467 of 1556 Batch 468 of 1556 Batch 469 of 1556 Batch 470 of 1556 Batch 471 of 1556 Batch 472 of 1556 Batch 473 of 1556 Batch 474 of 1556 Batch 475 of 1556 Batch 476 of 1556 Batch 477 of 1556 Batch 478 of 1556 Batch 479 of 1556 Batch 480 of 1556 Batch 481 of 1556 Batch 482 of 1556 Batch 483 of 1556 Batch 484 of 1556 Batch 485 of 1556 Batch 486 of 1556 Batch 487 of 1556 Batch 488 of 1556 Batch 489 of 1556 Batch 490 of 1556 Batch 491 of 1556 Batch 492 of 1556 Batch 493 of 1556 Batch 494 of 1556 Batch 495 of 1556 Batch 496 of 1556 Batch 497 of 1556 Batch 498 of 1556 Batch 499 of 1556 Batch 500 of 1556 Batch 501 of 1556 Batch 502 of 1556 Batch 503 of 1556 Batch 504 of 1556 Batch 505 of 1556 Batch 506 of 1556 Batch 507 of 1556 Batch 508 of 1556 Batch 509 of 1556 Batch 510 of 1556 Batch 511 of 1556 Batch 512 of 1556 Batch 513 of 1556 Batch 514 of 1556 Batch 515 of 1556 Batch 516 of 1556 Batch 517 of 1556 Batch 518 of 1556 Batch 519 of 1556 Batch 520 of 1556 Batch 521 of 1556 Batch 522 of 1556 Batch 523 of 1556 Batch 524 of 1556 Batch 525 of 1556 Batch 526 of 1556 Batch 527 of 1556 Batch 528 of 1556 Batch 529 of 1556 Batch 530 of 1556 Batch 531 of 1556 Batch 532 of 1556 Batch 533 of 1556 Batch 534 of 1556 Batch 535 of 1556 Batch 536 of 1556 Batch 537 of 1556 Batch 538 of 1556 Batch 539 of 1556 Batch 540 of 1556 Batch 541 of 1556 Batch 542 of 1556 Batch 543 of 1556 Batch 544 of 1556 Batch 545 of 1556 Batch 546 of 1556 Batch 547 of 1556 Batch 548 of 1556 Batch 549 of 1556 Batch 550 of 1556 Batch 551 of 1556 Batch 552 of 1556 Batch 553 of 1556 Batch 554 of 1556 Batch 555 of 1556 Batch 556 of 1556 Batch 557 of 1556 Batch 558 of 1556 Batch 559 of 1556 Batch 560 of 1556 Batch 561 of 1556 Batch 562 of 1556 Batch 563 of 1556 Batch 564 of 1556 Batch 565 of 1556 Batch 566 of 1556 Batch 567 of 1556 Batch 568 of 1556 Batch 569 of 1556 Batch 570 of 1556 Batch 571 of 1556 Batch 572 of 1556 Batch 573 of 1556 Batch 574 of 1556 Batch 575 of 1556 Batch 576 of 1556 Batch 577 of 1556 Batch 578 of 1556 Batch 579 of 1556 Batch 580 of 1556 Batch 581 of 1556 Batch 582 of 1556 Batch 583 of 1556 Batch 584 of 1556 Batch 585 of 1556 Batch 586 of 1556 Batch 587 of 1556 Batch 588 of 1556 Batch 589 of 1556 Batch 590 of 1556 Batch 591 of 1556 Batch 592 of 1556 Batch 593 of 1556 Batch 594 of 1556 Batch 595 of 1556 Batch 596 of 1556 Batch 597 of 1556 Batch 598 of 1556 Batch 599 of 1556 Batch 600 of 1556 Batch 601 of 1556 Batch 602 of 1556 Batch 603 of 1556 Batch 604 of 1556 Batch 605 of 1556 Batch 606 of 1556 Batch 607 of 1556 Batch 608 of 1556 Batch 609 of 1556 Batch 610 of 1556 Batch 611 of 1556 Batch 612 of 1556 Batch 613 of 1556 Batch 614 of 1556 Batch 615 of 1556 Batch 616 of 1556 Batch 617 of 1556 Batch 618 of 1556 Batch 619 of 1556 Batch 620 of 1556 Batch 621 of 1556 Batch 622 of 1556 Batch 623 of 1556 Batch 624 of 1556 Batch 625 of 1556 Batch 626 of 1556 Batch 627 of 1556 Batch 628 of 1556 Batch 629 of 1556 Batch 630 of 1556 Batch 631 of 1556 Batch 632 of 1556 Batch 633 of 1556 Batch 634 of 1556 Batch 635 of 1556 Batch 636 of 1556 Batch 637 of 1556 Batch 638 of 1556 Batch 639 of 1556 Batch 640 of 1556 Batch 641 of 1556 Batch 642 of 1556 Batch 643 of 1556 Batch 644 of 1556 Batch 645 of 1556 Batch 646 of 1556 Batch 647 of 1556 Batch 648 of 1556 Batch 649 of 1556 Batch 650 of 1556 Batch 651 of 1556 Batch 652 of 1556 Batch 653 of 1556 Batch 654 of 1556 Batch 655 of 1556 Batch 656 of 1556 Batch 657 of 1556 Batch 658 of 1556 Batch 659 of 1556 Batch 660 of 1556 Batch 661 of 1556 Batch 662 of 1556 Batch 663 of 1556 Batch 664 of 1556 Batch 665 of 1556 Batch 666 of 1556 Batch 667 of 1556 Batch 668 of 1556 Batch 669 of 1556 Batch 670 of 1556 Batch 671 of 1556 Batch 672 of 1556 Batch 673 of 1556 Batch 674 of 1556 Batch 675 of 1556 Batch 676 of 1556 Batch 677 of 1556 Batch 678 of 1556 Batch 679 of 1556 Batch 680 of 1556 Batch 681 of 1556 Batch 682 of 1556 Batch 683 of 1556 Batch 684 of 1556 Batch 685 of 1556"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b635a092be96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plateau - 6 layers -0.0003 data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-7a9777a01d65>\u001b[0m in \u001b[0;36mtrainer2\u001b[0;34m(model, train_data_iterator, train_options, test_data_iterator, test_options, optimizer, scheduler, scheduler_name)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mtrain_batch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mtrain_batch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mtrain_total_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_batch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#plateau - 6 layers -0.0003 data\n",
    "trainer2(l_model, train_data_iter, train_opt, test_data_iter, test_opt, l_optimizer, l_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plateau - 6 layers -0.0003 data\n",
    "linformer_trainer(l_model, train_data_iter, train_opt, test_data_iter, test_opt, l_optimizer, l_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a GPU was detected, model will be trained on GPU\n",
      "k shape torch.Size([128, 40, 512])\n",
      "e shape torch.Size([128, 40, 256])\n",
      "k view shape torch.Size([128, 40, 8, 64])\n",
      "e view shape torch.Size([128, 40, 8, 32])\n",
      "k transpose shape torch.Size([128, 8, 40, 64])\n",
      "e transpose shape torch.Size([128, 8, 40, 32])\n",
      "q, k torch.Size([128, 8, 40, 64]) torch.Size([128, 8, 64, 40])\n",
      "torch.Size([128, 8, 40, 40])\n",
      "scores.shape torch.Size([128, 8, 40, 32])\n",
      "mask.shape torch.Size([128, 1, 1, 40])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (40) must match the size of tensor b (32) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-715bd1b8c1b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plateau - 6 layers -0.0003 data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-107672da262c>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(model, train_data_iterator, train_options, test_data_iterator, test_options, optimizer, scheduler, scheduler_name)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#             print(\"src_mask\", src_mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#             print(\"trg_mask\", trg_mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pred :\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/project2/scripts/Linformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src_seq, src_mask, trg_seq, trg_mask)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_seq\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/project2/scripts/Linformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source_sequence, source_mask)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         '''\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mvector_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mvector_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/project2/scripts/Linformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, vector_sequence, mask)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0msource_mask\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0mover\u001b[0m \u001b[0minput\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msequence\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvectors\u001b[0m \u001b[0mafter\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostional\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         '''\n\u001b[1;32m    132\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/project2/scripts/Linformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v, e, f, mask, explain)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"k transpose shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"e transpose shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexplain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"(batch_size,num_heads,seq_length,dim_k)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# calculate attention using function we will define next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/project2/scripts/Linformer.py\u001b[0m in \u001b[0;36mattention\u001b[0;34m(self, q, k, v, e, f, dim_k, mask, dropout, explain)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexplain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scores.shape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexplain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mask.shape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (40) must match the size of tensor b (32) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "#plateau - 6 layers -0.0003 data\n",
    "trainer(l_model, train_data_iter, train_opt, test_data_iter, test_opt, l_optimizer, l_scheduler, scheduler_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
